{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d08f94",
   "metadata": {},
   "source": [
    "# 1. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8d4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc,accuracy_score,auc,accuracy_score,f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from transformers import AdamW\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import Trainer\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import Linear, Module\n",
    "\n",
    "import wandb\n",
    "import optuna\n",
    "#from prettytable import PrettyTable\n",
    "import copy\n",
    "from datasets import Dataset,DatasetDict\n",
    "from datasets import load_dataset, load_metric ,concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26599ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "tb_dir = 'tbs_HW2/'\n",
    "results = 'results/'\n",
    "models_trained_params = 'models_trained_params/'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "home_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4787b8ba",
   "metadata": {},
   "source": [
    "# 2. Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d20bb4",
   "metadata": {},
   "source": [
    "Fetching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7633ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(name):\n",
    "    data = pd.read_csv(Path(home_dir, name)).iloc[:,1:].rename({'filtered_text_2':'filtered_text'},axis=1)\n",
    "#     clean_data=clean_data(data) #replace with the real funcion name\n",
    "    return data\n",
    "\n",
    "\n",
    "data=get_clean_data('preprocessed_df.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79ea44",
   "metadata": {},
   "source": [
    "Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f2d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=data['filtered_text']\n",
    "labels=data['main_category'].values\n",
    "\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the labels and transform the labels\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Replace the labels column in the dataframe with the encoded labels\n",
    "data['encoded_main_category'] = encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24630ef7",
   "metadata": {},
   "source": [
    "Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf0de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, test_ratio=0.2, random_state=42 ,batch_size=32):\n",
    "    train_size=int((1-test_ratio)*(x.shape[0]))\n",
    "    test_size=int((x.shape[0])-train_size)\n",
    "\n",
    "    train_end_idx = train_size\n",
    "    test_end_idx = train_size + test_size\n",
    "    # Make one list for all the reviews\n",
    "    headlines = x.tolist()\n",
    "\n",
    "    # Mini sample of reviews for train and test\n",
    "    train_data = headlines[:train_end_idx]\n",
    "    test_data = headlines[train_end_idx:test_end_idx]\n",
    "    # Take mini sample of the labels and preprocess them such that we can use them in the model training loop\n",
    "    labels = encoded_labels.tolist()\n",
    "    train_labels = labels[:train_end_idx]\n",
    "    test_labels = labels[train_end_idx:test_end_idx]\n",
    "\n",
    "    # Use data set class in order to build train and test datasets\n",
    "    train_df = pd.DataFrame({'text':train_data, 'labels':train_labels})#.dropna().drop_duplicates()\n",
    "    test_df = pd.DataFrame({'text':test_data, 'labels':test_labels})#.dropna().drop_duplicates()\n",
    "    train_df.to_csv('train_df.csv', index = False)\n",
    "    test_df.to_csv('test_df.csv', index = False)\n",
    "    \n",
    "    data_files = {\n",
    "    'train':'train_df.csv',\n",
    "    'test':'test_df.csv'\n",
    "    }\n",
    "    \n",
    "    raw_datasets = load_dataset(\"csv\", data_files=data_files)\n",
    "    \n",
    "\n",
    "    return raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f09b65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/liyag/.cache/huggingface/datasets/csv/default-2e78edd390e366fa/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce1e172d64a49129734c04eb2dffb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b6317065e84aad84aeb711d8711860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/liyag/.cache/huggingface/datasets/csv/default-2e78edd390e366fa/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5877641c51534d2293d10f2ba140409c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = split_data(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27166135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 388761\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 97191\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd66b78f",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c739840",
   "metadata": {},
   "source": [
    "# Knowladge distilation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5a1befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowladge_distilation_training(student_model, teacher_model, train_dataset):\n",
    "    epochs = 10\n",
    "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-5)\n",
    "    softmax = torch.nn.Softmax()\n",
    "    relu = torch.nn.ReLU()\n",
    "    # Knowledge distillation training loop\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_dataset:\n",
    "            # Forward pass with the teacher model to generate soft targets\n",
    "            input_ids = batch['input_ids']\n",
    "            input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)  # Add batch dimension\n",
    "            attention_mask = batch['attention_mask']\n",
    "            attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(input_ids, attention_mask=attention_mask)\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_outputs = student_model(input_ids, attention_mask=attention_mask)\n",
    "            student_logits = student_outputs.logits\n",
    "\n",
    "            # Compute the distillation loss\n",
    "            temperature = 2  # Temperature parameter for softening the logits\n",
    "            soft_teacher_logits = softmax(teacher_logits / temperature)\n",
    "            soft_student_logits = softmax(student_logits / temperature)\n",
    "            distillation_loss = torch.nn.KLDivLoss()(torch.log_softmax(student_logits, dim=-1),\n",
    "                                                     soft_teacher_logits.detach())\n",
    "\n",
    "            # Optionally, add other losses such as cross-entropy or task-specific losses\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = distillation_loss  # You can add other losses here\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Optionally, you can fine-tune the student model on labeled data using traditional supervised learning techniques\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938820a",
   "metadata": {},
   "source": [
    "# 3. Creating the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba2e59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurAwesomeModel():\n",
    "    def __init__(self,model_name,dataset):\n",
    "        self.model_name=model_name\n",
    "        self.model=AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=6,return_dict=True).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.dataset=dataset\n",
    "        self.token_args={\"max_length\": 32, \"truncation\": True, \"padding\": \"max_length\"}\n",
    "        \n",
    "        self.tokenized_datasets = self.tokenization()\n",
    "        \n",
    "    # function that tokenize the data and returns train and test tokenized dataset in torch format\n",
    "    def tokenization(self):\n",
    "        tokenized_dataset=self.dataset.map(self.tokenizer, input_columns='text', fn_kwargs=self.token_args)\n",
    "        tokenized_dataset.set_format('torch')\n",
    "        return tokenized_dataset\n",
    "    \n",
    "    # function that defines the Trainer object and train the model\n",
    "    def train(self,train_args):\n",
    "        train_args = TrainingArguments(**train_args)\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=train_args,\n",
    "            train_dataset=self.tokenized_datasets['train'],\n",
    "            eval_dataset=self.tokenized_datasets['test'],\n",
    "            compute_metrics=self.metric_fn)\n",
    "        trainer.train()\n",
    "\n",
    "    \n",
    "    # definition of the metric we want to optimize\n",
    "    def metric_fn(self, predictions):\n",
    "        preds = predictions.predictions.argmax(axis=1)\n",
    "        labels = predictions.label_ids\n",
    "        return {'f1': f1_score(labels, preds, average='weighted')\n",
    "               ,'accuracy':accuracy_score(labels,preds)}\n",
    "    \n",
    "    # help function for hyperparameter finetuning\n",
    "    def model_init(self):\n",
    "        return self.model\n",
    "    \n",
    "    # function that prefurms hyperparameter finetuning with optuna as backend. \n",
    "    # The function gets the train arguments from the function 'my_hp_space'.\n",
    "    def hpm_search(self,train_args_dict):\n",
    "        wandb.init(project=\"model1\", name = self.model_name)\n",
    "        train_args = TrainingArguments(**train_args_dict)\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=train_args,\n",
    "            train_dataset=self.tokenized_datasets['train'],\n",
    "            eval_dataset=self.tokenized_datasets['test'],\n",
    "            model_init=self.model_init,\n",
    "            compute_metrics=self.metric_fn)\n",
    "        \n",
    "        best_run=trainer.hyperparameter_search(direction=\"maximize\", hp_space=self.my_hp_space,n_trials=10)\n",
    "        best_hyperparameters = best_run.hyperparameters\n",
    "        print(best_hyperparameters)\n",
    "        train_args_dict.update(best_hyperparameters)\n",
    "        print(train_args_dict)\n",
    "        self.train(TrainingArguments(**train_args_dict))\n",
    "        self.save_model()\n",
    "        wandb.finish()\n",
    "    \n",
    "    # function that is called in every optuna trial, and return a combination of hyperparameters to try. \n",
    "    def my_hp_space(self,trial):\n",
    "        return {\"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 5e-5, log=True),\n",
    "                \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [10]),\n",
    "                \"seed\": trial.suggest_categorical(\"seed\", [0]),\n",
    "                \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16,32]),\n",
    "                \"gradient_accumulation_steps\":trial.suggest_int(\"gradient_accumulation_steps\",1,6),\n",
    "                \"warmup_steps\":trial.suggest_int(\"warmup_steps\",0,500),\n",
    "                \"weight_decay\":trial.suggest_float(\"weight_decay\",1e-4,1e-2),\n",
    "                \"per_device_eval_batch_size\":trial.suggest_categorical(\"per_device_eval_batch_size\",[16,32])}\n",
    "    \n",
    "    \n",
    "    def evaluate(self, train_args_dict):\n",
    "        train_args = TrainingArguments(**train_args_dict)\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=train_args,\n",
    "            train_dataset=self.tokenized_datasets['train'],\n",
    "            eval_dataset=self.tokenized_datasets['test'],\n",
    "            compute_metrics=self.metric_fn)\n",
    "        \n",
    "        predictions = trainer.predict(self.tokenized_datasets['test'])\n",
    "        result_dict = self.metric_fn(predictions)\n",
    "        for k,v in result_dict.items():\n",
    "            print(f'{k} value: {v}')\n",
    "    \n",
    "    # First contraction function\n",
    "    # pruns the amount% of the smallest weights\n",
    "    def pruning(self,amount):           \n",
    "        for module_name, module in self.model.named_modules():\n",
    "            if isinstance(module, Linear):# and 'ff' in module_name:\n",
    "                print(f'\\n{module_name}:\\nold_total_weights = {module.weight.sum()}')\n",
    "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "                prune.remove(module, name='weight')\n",
    "                print(f'new_total_weights = {module.weight.sum()}')\n",
    "\n",
    "    # second contraction function\n",
    "    # saves the weights in half of the space \n",
    "    def half(self):\n",
    "        self.model = self.model.half()\n",
    "    \n",
    "    # function that prints the models' weights. \n",
    "    # The weights are presented in their absolute value such that negative and positive weights wont offset\n",
    "    def show_parameters(self):\n",
    "        table = PrettyTable([\"Modules\", \"Parameters\",\"Sum of Tensor\"])\n",
    "        total_params = 0\n",
    "        total_sum = 0\n",
    "        for name, parameter in self.model.named_parameters():\n",
    "            if not parameter.requires_grad: \n",
    "                continue\n",
    "            params = parameter.numel()\n",
    "            total_sum += parameter.abs().sum()\n",
    "            total=float(str(parameter.abs().sum()).split(',')[0][7:])\n",
    "            table.add_row([name, params,total])\n",
    "            total_params+=params\n",
    "        table.add_row([\"Total Trainable Params\",str(total_params),str(total_sum.item())])\n",
    "        print(table)\n",
    "        return total_params\n",
    "\n",
    "    def save_model(self, model_type = None):\n",
    "        if model_type == None:\n",
    "            torch.save(self.model.state_dict(), f'{self.model_name}.pt')\n",
    "        else:\n",
    "            torch.save(self.model.state_dict(), f'{model_type}_{self.model_name}.pt')\n",
    "        print('model was saved')\n",
    "        \n",
    "    def load_trained_model(self, model_type = None):\n",
    "        if model_type == None:\n",
    "            self.model.load_state_dict(torch.load(f'{self.model_name}.pt'))\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load(f'{model_type}_{self.model_name}.pt'))\n",
    "        print('model was loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec083c",
   "metadata": {},
   "source": [
    "# 4 Model initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a39f2",
   "metadata": {},
   "source": [
    "Intializing params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48af28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6afb75a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliyag\u001b[0m (\u001b[33mdelta_lxr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_154459-2fxfm3ur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/model1/runs/2fxfm3ur' target=\"_blank\">fragrant-frost-18</a></strong> to <a href='https://wandb.ai/delta_lxr/model1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/model1' target=\"_blank\">https://wandb.ai/delta_lxr/model1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/model1/runs/2fxfm3ur' target=\"_blank\">https://wandb.ai/delta_lxr/model1/runs/2fxfm3ur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"model1\")\n",
    "\n",
    "OUT_PATH=Path(home_dir,'results')\n",
    "\n",
    "args = {'output_dir':OUT_PATH,\n",
    " 'overwrite_output_dir':True,\n",
    " 'greater_is_better':True,\n",
    " 'evaluation_strategy':'steps',\n",
    " 'do_train':True,\n",
    " 'logging_strategy':'epoch',\n",
    " 'save_strategy':'epoch',\n",
    " 'report_to':'wandb'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2a6b12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================\n",
      "============================== xlnet-base-cased ==============================\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/388761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+------------+---------------+\n",
      "|                     Modules                     | Parameters | Sum of Tensor |\n",
      "+-------------------------------------------------+------------+---------------+\n",
      "|               transformer.mask_emb              |    768     |     7.254     |\n",
      "|        transformer.word_embedding.weight        |  24576000  |   1074791.5   |\n",
      "|          transformer.layer.0.rel_attn.q         |   589824   |   44567.0234  |\n",
      "|          transformer.layer.0.rel_attn.k         |   589824   |   94692.3594  |\n",
      "|          transformer.layer.0.rel_attn.v         |   589824   |   15121.5391  |\n",
      "|          transformer.layer.0.rel_attn.o         |   589824   |   16153.6406  |\n",
      "|          transformer.layer.0.rel_attn.r         |   589824   |   10870.3428  |\n",
      "|      transformer.layer.0.rel_attn.r_r_bias      |    768     |    119.4765   |\n",
      "|      transformer.layer.0.rel_attn.r_s_bias      |    768     |    135.992    |\n",
      "|      transformer.layer.0.rel_attn.r_w_bias      |    768     |    159.3317   |\n",
      "|      transformer.layer.0.rel_attn.seg_embed     |    1536    |    355.6453   |\n",
      "|  transformer.layer.0.rel_attn.layer_norm.weight |    768     |    739.4017   |\n",
      "|   transformer.layer.0.rel_attn.layer_norm.bias  |    768     |    93.4022    |\n",
      "|     transformer.layer.0.ff.layer_norm.weight    |    768     |    817.935    |\n",
      "|      transformer.layer.0.ff.layer_norm.bias     |    768     |    89.1101    |\n",
      "|      transformer.layer.0.ff.layer_1.weight      |  2359296   |   65524.4609  |\n",
      "|       transformer.layer.0.ff.layer_1.bias       |    3072    |    360.5238   |\n",
      "|      transformer.layer.0.ff.layer_2.weight      |  2359296   |   57152.4336  |\n",
      "|       transformer.layer.0.ff.layer_2.bias       |    768     |    30.9189    |\n",
      "|          transformer.layer.1.rel_attn.q         |   589824   |   16615.4531  |\n",
      "|          transformer.layer.1.rel_attn.k         |   589824   |   20354.8672  |\n",
      "|          transformer.layer.1.rel_attn.v         |   589824   |   15458.2891  |\n",
      "|          transformer.layer.1.rel_attn.o         |   589824   |   15927.417   |\n",
      "|          transformer.layer.1.rel_attn.r         |   589824   |   5295.8662   |\n",
      "|      transformer.layer.1.rel_attn.r_r_bias      |    768     |    212.3733   |\n",
      "|      transformer.layer.1.rel_attn.r_s_bias      |    768     |    209.8347   |\n",
      "|      transformer.layer.1.rel_attn.r_w_bias      |    768     |    128.4038   |\n",
      "|      transformer.layer.1.rel_attn.seg_embed     |    1536    |    211.6133   |\n",
      "|  transformer.layer.1.rel_attn.layer_norm.weight |    768     |    723.806    |\n",
      "|   transformer.layer.1.rel_attn.layer_norm.bias  |    768     |    111.3541   |\n",
      "|     transformer.layer.1.ff.layer_norm.weight    |    768     |    713.6425   |\n",
      "|      transformer.layer.1.ff.layer_norm.bias     |    768     |    115.101    |\n",
      "|      transformer.layer.1.ff.layer_1.weight      |  2359296   |   70932.1172  |\n",
      "|       transformer.layer.1.ff.layer_1.bias       |    3072    |    339.4705   |\n",
      "|      transformer.layer.1.ff.layer_2.weight      |  2359296   |   61284.8203  |\n",
      "|       transformer.layer.1.ff.layer_2.bias       |    768     |    35.9411    |\n",
      "|          transformer.layer.2.rel_attn.q         |   589824   |   16478.043   |\n",
      "|          transformer.layer.2.rel_attn.k         |   589824   |   19956.5039  |\n",
      "|          transformer.layer.2.rel_attn.v         |   589824   |   15817.7686  |\n",
      "|          transformer.layer.2.rel_attn.o         |   589824   |   16036.9727  |\n",
      "|          transformer.layer.2.rel_attn.r         |   589824   |   5256.8975   |\n",
      "|      transformer.layer.2.rel_attn.r_r_bias      |    768     |    248.2291   |\n",
      "|      transformer.layer.2.rel_attn.r_s_bias      |    768     |    197.4681   |\n",
      "|      transformer.layer.2.rel_attn.r_w_bias      |    768     |    112.4888   |\n",
      "|      transformer.layer.2.rel_attn.seg_embed     |    1536    |    236.724    |\n",
      "|  transformer.layer.2.rel_attn.layer_norm.weight |    768     |    707.1062   |\n",
      "|   transformer.layer.2.rel_attn.layer_norm.bias  |    768     |    123.2349   |\n",
      "|     transformer.layer.2.ff.layer_norm.weight    |    768     |    831.9916   |\n",
      "|      transformer.layer.2.ff.layer_norm.bias     |    768     |    95.8557    |\n",
      "|      transformer.layer.2.ff.layer_1.weight      |  2359296   |   71155.9219  |\n",
      "|       transformer.layer.2.ff.layer_1.bias       |    3072    |    327.7538   |\n",
      "|      transformer.layer.2.ff.layer_2.weight      |  2359296   |   60508.1641  |\n",
      "|       transformer.layer.2.ff.layer_2.bias       |    768     |    46.5968    |\n",
      "|          transformer.layer.3.rel_attn.q         |   589824   |   14184.9844  |\n",
      "|          transformer.layer.3.rel_attn.k         |   589824   |   18439.582   |\n",
      "|          transformer.layer.3.rel_attn.v         |   589824   |   16358.1836  |\n",
      "|          transformer.layer.3.rel_attn.o         |   589824   |   16112.7686  |\n",
      "|          transformer.layer.3.rel_attn.r         |   589824   |   5651.2095   |\n",
      "|      transformer.layer.3.rel_attn.r_r_bias      |    768     |    300.2114   |\n",
      "|      transformer.layer.3.rel_attn.r_s_bias      |    768     |    241.937    |\n",
      "|      transformer.layer.3.rel_attn.r_w_bias      |    768     |    248.2556   |\n",
      "|      transformer.layer.3.rel_attn.seg_embed     |    1536    |    227.3521   |\n",
      "|  transformer.layer.3.rel_attn.layer_norm.weight |    768     |    694.8253   |\n",
      "|   transformer.layer.3.rel_attn.layer_norm.bias  |    768     |    110.3689   |\n",
      "|     transformer.layer.3.ff.layer_norm.weight    |    768     |    858.9975   |\n",
      "|      transformer.layer.3.ff.layer_norm.bias     |    768     |    142.7631   |\n",
      "|      transformer.layer.3.ff.layer_1.weight      |  2359296   |   71147.6562  |\n",
      "|       transformer.layer.3.ff.layer_1.bias       |    3072    |    338.0177   |\n",
      "|      transformer.layer.3.ff.layer_2.weight      |  2359296   |   59865.1172  |\n",
      "|       transformer.layer.3.ff.layer_2.bias       |    768     |    46.5564    |\n",
      "|          transformer.layer.4.rel_attn.q         |   589824   |   16036.4863  |\n",
      "|          transformer.layer.4.rel_attn.k         |   589824   |   18271.5703  |\n",
      "|          transformer.layer.4.rel_attn.v         |   589824   |   16346.7188  |\n",
      "|          transformer.layer.4.rel_attn.o         |   589824   |   16681.373   |\n",
      "|          transformer.layer.4.rel_attn.r         |   589824   |    4920.417   |\n",
      "|      transformer.layer.4.rel_attn.r_r_bias      |    768     |    148.9918   |\n",
      "|      transformer.layer.4.rel_attn.r_s_bias      |    768     |    159.7744   |\n",
      "|      transformer.layer.4.rel_attn.r_w_bias      |    768     |    85.7614    |\n",
      "|      transformer.layer.4.rel_attn.seg_embed     |    1536    |    193.4296   |\n",
      "|  transformer.layer.4.rel_attn.layer_norm.weight |    768     |    696.5731   |\n",
      "|   transformer.layer.4.rel_attn.layer_norm.bias  |    768     |    119.2452   |\n",
      "|     transformer.layer.4.ff.layer_norm.weight    |    768     |    842.5712   |\n",
      "|      transformer.layer.4.ff.layer_norm.bias     |    768     |    74.4084    |\n",
      "|      transformer.layer.4.ff.layer_1.weight      |  2359296   |   70385.1797  |\n",
      "|       transformer.layer.4.ff.layer_1.bias       |    3072    |    289.2992   |\n",
      "|      transformer.layer.4.ff.layer_2.weight      |  2359296   |   59396.2969  |\n",
      "|       transformer.layer.4.ff.layer_2.bias       |    768     |    52.9156    |\n",
      "|          transformer.layer.5.rel_attn.q         |   589824   |   20059.2539  |\n",
      "|          transformer.layer.5.rel_attn.k         |   589824   |   22960.4961  |\n",
      "|          transformer.layer.5.rel_attn.v         |   589824   |   16006.8086  |\n",
      "|          transformer.layer.5.rel_attn.o         |   589824   |   15621.1445  |\n",
      "|          transformer.layer.5.rel_attn.r         |   589824   |   5805.9966   |\n",
      "|      transformer.layer.5.rel_attn.r_r_bias      |    768     |    134.9497   |\n",
      "|      transformer.layer.5.rel_attn.r_s_bias      |    768     |    144.4765   |\n",
      "|      transformer.layer.5.rel_attn.r_w_bias      |    768     |    91.5404    |\n",
      "|      transformer.layer.5.rel_attn.seg_embed     |    1536    |    192.1495   |\n",
      "|  transformer.layer.5.rel_attn.layer_norm.weight |    768     |    709.6529   |\n",
      "|   transformer.layer.5.rel_attn.layer_norm.bias  |    768     |    67.5633    |\n",
      "|     transformer.layer.5.ff.layer_norm.weight    |    768     |   1048.4541   |\n",
      "|      transformer.layer.5.ff.layer_norm.bias     |    768     |    112.0544   |\n",
      "|      transformer.layer.5.ff.layer_1.weight      |  2359296   |   67658.6953  |\n",
      "|       transformer.layer.5.ff.layer_1.bias       |    3072    |    309.1635   |\n",
      "|      transformer.layer.5.ff.layer_2.weight      |  2359296   |   57182.9062  |\n",
      "|       transformer.layer.5.ff.layer_2.bias       |    768     |    48.5465    |\n",
      "|          transformer.layer.6.rel_attn.q         |   589824   |   17451.0078  |\n",
      "|          transformer.layer.6.rel_attn.k         |   589824   |   20020.543   |\n",
      "|          transformer.layer.6.rel_attn.v         |   589824   |   16421.8594  |\n",
      "|          transformer.layer.6.rel_attn.o         |   589824   |   15961.7676  |\n",
      "|          transformer.layer.6.rel_attn.r         |   589824   |   5262.4746   |\n",
      "|      transformer.layer.6.rel_attn.r_r_bias      |    768     |    79.5347    |\n",
      "|      transformer.layer.6.rel_attn.r_s_bias      |    768     |    142.9248   |\n",
      "|      transformer.layer.6.rel_attn.r_w_bias      |    768     |    74.9769    |\n",
      "|      transformer.layer.6.rel_attn.seg_embed     |    1536    |    123.6726   |\n",
      "|  transformer.layer.6.rel_attn.layer_norm.weight |    768     |    824.401    |\n",
      "|   transformer.layer.6.rel_attn.layer_norm.bias  |    768     |    100.4799   |\n",
      "|     transformer.layer.6.ff.layer_norm.weight    |    768     |   1044.6274   |\n",
      "|      transformer.layer.6.ff.layer_norm.bias     |    768     |    77.4704    |\n",
      "|      transformer.layer.6.ff.layer_1.weight      |  2359296   |   70994.5156  |\n",
      "|       transformer.layer.6.ff.layer_1.bias       |    3072    |    286.0999   |\n",
      "|      transformer.layer.6.ff.layer_2.weight      |  2359296   |   55176.3672  |\n",
      "|       transformer.layer.6.ff.layer_2.bias       |    768     |     54.336    |\n",
      "|          transformer.layer.7.rel_attn.q         |   589824   |   17514.5215  |\n",
      "|          transformer.layer.7.rel_attn.k         |   589824   |   20377.6895  |\n",
      "|          transformer.layer.7.rel_attn.v         |   589824   |   16732.2969  |\n",
      "|          transformer.layer.7.rel_attn.o         |   589824   |   15741.9297  |\n",
      "|          transformer.layer.7.rel_attn.r         |   589824   |   5071.5571   |\n",
      "|      transformer.layer.7.rel_attn.r_r_bias      |    768     |    86.8166    |\n",
      "|      transformer.layer.7.rel_attn.r_s_bias      |    768     |    141.3773   |\n",
      "|      transformer.layer.7.rel_attn.r_w_bias      |    768     |    53.4513    |\n",
      "|      transformer.layer.7.rel_attn.seg_embed     |    1536    |    147.1174   |\n",
      "|  transformer.layer.7.rel_attn.layer_norm.weight |    768     |    858.785    |\n",
      "|   transformer.layer.7.rel_attn.layer_norm.bias  |    768     |    75.5416    |\n",
      "|     transformer.layer.7.ff.layer_norm.weight    |    768     |   1134.3446   |\n",
      "|      transformer.layer.7.ff.layer_norm.bias     |    768     |    86.6803    |\n",
      "|      transformer.layer.7.ff.layer_1.weight      |  2359296   |   70554.7891  |\n",
      "|       transformer.layer.7.ff.layer_1.bias       |    3072    |    301.2746   |\n",
      "|      transformer.layer.7.ff.layer_2.weight      |  2359296   |   53420.7891  |\n",
      "|       transformer.layer.7.ff.layer_2.bias       |    768     |    58.8535    |\n",
      "|          transformer.layer.8.rel_attn.q         |   589824   |   17296.2832  |\n",
      "|          transformer.layer.8.rel_attn.k         |   589824   |   20111.6289  |\n",
      "|          transformer.layer.8.rel_attn.v         |   589824   |   17486.2109  |\n",
      "|          transformer.layer.8.rel_attn.o         |   589824   |   16635.3867  |\n",
      "|          transformer.layer.8.rel_attn.r         |   589824   |   4848.1641   |\n",
      "|      transformer.layer.8.rel_attn.r_r_bias      |    768     |    76.1078    |\n",
      "|      transformer.layer.8.rel_attn.r_s_bias      |    768     |    104.5954   |\n",
      "|      transformer.layer.8.rel_attn.r_w_bias      |    768     |    50.7893    |\n",
      "|      transformer.layer.8.rel_attn.seg_embed     |    1536    |    132.1038   |\n",
      "|  transformer.layer.8.rel_attn.layer_norm.weight |    768     |   1080.6792   |\n",
      "|   transformer.layer.8.rel_attn.layer_norm.bias  |    768     |    68.7948    |\n",
      "|     transformer.layer.8.ff.layer_norm.weight    |    768     |   1076.7401   |\n",
      "|      transformer.layer.8.ff.layer_norm.bias     |    768     |    70.7123    |\n",
      "|      transformer.layer.8.ff.layer_1.weight      |  2359296   |   80616.2969  |\n",
      "|       transformer.layer.8.ff.layer_1.bias       |    3072    |    257.8336   |\n",
      "|      transformer.layer.8.ff.layer_2.weight      |  2359296   |   49840.0938  |\n",
      "|       transformer.layer.8.ff.layer_2.bias       |    768     |    48.5688    |\n",
      "|          transformer.layer.9.rel_attn.q         |   589824   |   19109.6836  |\n",
      "|          transformer.layer.9.rel_attn.k         |   589824   |   22514.7949  |\n",
      "|          transformer.layer.9.rel_attn.v         |   589824   |   17788.0781  |\n",
      "|          transformer.layer.9.rel_attn.o         |   589824   |   16198.5771  |\n",
      "|          transformer.layer.9.rel_attn.r         |   589824   |   4738.6074   |\n",
      "|      transformer.layer.9.rel_attn.r_r_bias      |    768     |    71.5137    |\n",
      "|      transformer.layer.9.rel_attn.r_s_bias      |    768     |    115.5869   |\n",
      "|      transformer.layer.9.rel_attn.r_w_bias      |    768     |    66.0999    |\n",
      "|      transformer.layer.9.rel_attn.seg_embed     |    1536    |    114.4852   |\n",
      "|  transformer.layer.9.rel_attn.layer_norm.weight |    768     |   1043.1191   |\n",
      "|   transformer.layer.9.rel_attn.layer_norm.bias  |    768     |    53.5533    |\n",
      "|     transformer.layer.9.ff.layer_norm.weight    |    768     |   1179.2814   |\n",
      "|      transformer.layer.9.ff.layer_norm.bias     |    768     |    80.6646    |\n",
      "|      transformer.layer.9.ff.layer_1.weight      |  2359296   |   78603.2812  |\n",
      "|       transformer.layer.9.ff.layer_1.bias       |    3072    |    292.0465   |\n",
      "|      transformer.layer.9.ff.layer_2.weight      |  2359296   |   52302.3594  |\n",
      "|       transformer.layer.9.ff.layer_2.bias       |    768     |    45.4677    |\n",
      "|         transformer.layer.10.rel_attn.q         |   589824   |   15579.9766  |\n",
      "|         transformer.layer.10.rel_attn.k         |   589824   |   18639.8672  |\n",
      "|         transformer.layer.10.rel_attn.v         |   589824   |   19212.7129  |\n",
      "|         transformer.layer.10.rel_attn.o         |   589824   |   17647.1055  |\n",
      "|         transformer.layer.10.rel_attn.r         |   589824   |   4220.3481   |\n",
      "|      transformer.layer.10.rel_attn.r_r_bias     |    768     |    93.9036    |\n",
      "|      transformer.layer.10.rel_attn.r_s_bias     |    768     |    110.1193   |\n",
      "|      transformer.layer.10.rel_attn.r_w_bias     |    768     |     93.364    |\n",
      "|     transformer.layer.10.rel_attn.seg_embed     |    1536    |    111.9805   |\n",
      "| transformer.layer.10.rel_attn.layer_norm.weight |    768     |   1053.8857   |\n",
      "|  transformer.layer.10.rel_attn.layer_norm.bias  |    768     |    61.7783    |\n",
      "|    transformer.layer.10.ff.layer_norm.weight    |    768     |   1021.0245   |\n",
      "|     transformer.layer.10.ff.layer_norm.bias     |    768     |    63.4507    |\n",
      "|      transformer.layer.10.ff.layer_1.weight     |  2359296   |    78399.5    |\n",
      "|       transformer.layer.10.ff.layer_1.bias      |    3072    |    258.4749   |\n",
      "|      transformer.layer.10.ff.layer_2.weight     |  2359296   |   50104.3516  |\n",
      "|       transformer.layer.10.ff.layer_2.bias      |    768     |    41.6371    |\n",
      "|         transformer.layer.11.rel_attn.q         |   589824   |   18394.8711  |\n",
      "|         transformer.layer.11.rel_attn.k         |   589824   |   23459.6387  |\n",
      "|         transformer.layer.11.rel_attn.v         |   589824   |   24145.3242  |\n",
      "|         transformer.layer.11.rel_attn.o         |   589824   |   20718.0371  |\n",
      "|         transformer.layer.11.rel_attn.r         |   589824   |   3526.5483   |\n",
      "|      transformer.layer.11.rel_attn.r_r_bias     |    768     |    93.5896    |\n",
      "|      transformer.layer.11.rel_attn.r_s_bias     |    768     |    77.3294    |\n",
      "|      transformer.layer.11.rel_attn.r_w_bias     |    768     |    95.3318    |\n",
      "|     transformer.layer.11.rel_attn.seg_embed     |    1536    |    84.8501    |\n",
      "| transformer.layer.11.rel_attn.layer_norm.weight |    768     |   1216.1434   |\n",
      "|  transformer.layer.11.rel_attn.layer_norm.bias  |    768     |    76.8259    |\n",
      "|    transformer.layer.11.ff.layer_norm.weight    |    768     |   1856.5117   |\n",
      "|     transformer.layer.11.ff.layer_norm.bias     |    768     |    135.5617   |\n",
      "|      transformer.layer.11.ff.layer_1.weight     |  2359296   |   82175.3516  |\n",
      "|       transformer.layer.11.ff.layer_1.bias      |    3072    |    300.9164   |\n",
      "|      transformer.layer.11.ff.layer_2.weight     |  2359296   |   45175.8945  |\n",
      "|       transformer.layer.11.ff.layer_2.bias      |    768     |    36.1983    |\n",
      "|         sequence_summary.summary.weight         |   589824   |   9401.6318   |\n",
      "|          sequence_summary.summary.bias          |    768     |      0.0      |\n",
      "|                logits_proj.weight               |    4608    |    71.9156    |\n",
      "|                 logits_proj.bias                |     6      |      0.0      |\n",
      "|              Total Trainable Params             | 117313542  |   3684741.75  |\n",
      "+-------------------------------------------------+------------+---------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 value: 0.07812441269462927\n",
      "accuracy value: 0.15471597164346493\n",
      "------------------------------ regular xlnet-base-cased ------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2fxfm3ur) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-frost-18</strong> at: <a href='https://wandb.ai/delta_lxr/model1/runs/2fxfm3ur' target=\"_blank\">https://wandb.ai/delta_lxr/model1/runs/2fxfm3ur</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_154459-2fxfm3ur\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2fxfm3ur). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbf3c1cf3174ffe8c5b660498a82e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_155200-f2u3zoth</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/model1/runs/f2u3zoth' target=\"_blank\">xlnet-base-cased</a></strong> to <a href='https://wandb.ai/delta_lxr/model1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/model1' target=\"_blank\">https://wandb.ai/delta_lxr/model1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/model1/runs/f2u3zoth' target=\"_blank\">https://wandb.ai/delta_lxr/model1/runs/f2u3zoth</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:359: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-06-09 15:52:15,815]\u001b[0m A new study created in memory with name: no-name-18028011-b32e-452c-89b7-c0134b0d2149\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xlnet-base-cased</strong> at: <a href='https://wandb.ai/delta_lxr/model1/runs/f2u3zoth' target=\"_blank\">https://wandb.ai/delta_lxr/model1/runs/f2u3zoth</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_155200-f2u3zoth\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_155221-84823slb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/84823slb' target=\"_blank\">crisp-wind-65</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/84823slb' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/84823slb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4049' max='4049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4049/4049 37:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.120128</td>\n",
       "      <td>0.598907</td>\n",
       "      <td>0.602031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.006096</td>\n",
       "      <td>0.637887</td>\n",
       "      <td>0.639679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.968242</td>\n",
       "      <td>0.652609</td>\n",
       "      <td>0.654268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.940844</td>\n",
       "      <td>0.661977</td>\n",
       "      <td>0.664249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.921573</td>\n",
       "      <td>0.665650</td>\n",
       "      <td>0.669290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.906607</td>\n",
       "      <td>0.674113</td>\n",
       "      <td>0.675371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.890034</td>\n",
       "      <td>0.678620</td>\n",
       "      <td>0.680773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.891456</td>\n",
       "      <td>0.678572</td>\n",
       "      <td>0.680763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 16:30:01,035]\u001b[0m Trial 0 finished with value: 1.3593341776112122 and parameters: {'learning_rate': 1.373489595987342e-05, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 3, 'warmup_steps': 43, 'weight_decay': 0.0074106703716325575, 'per_device_eval_batch_size': 16}. Best is trial 0 with value: 1.3593341776112122.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34af5af62f7a4c46abc22a601c6e6011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▆▇▇███</td></tr><tr><td>eval/f1</td><td>▁▄▆▇▇███</td></tr><tr><td>eval/loss</td><td>█▅▃▃▂▂▁▁</td></tr><tr><td>eval/runtime</td><td>▆▃▂▁█▄▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▃▆▇█▁▅▄█</td></tr><tr><td>eval/steps_per_second</td><td>▃▆▇█▁▅▄█</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.68076</td></tr><tr><td>eval/f1</td><td>0.67857</td></tr><tr><td>eval/loss</td><td>0.89146</td></tr><tr><td>eval/runtime</td><td>157.0817</td></tr><tr><td>eval/samples_per_second</td><td>618.729</td></tr><tr><td>eval/steps_per_second</td><td>38.674</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>4049</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.0345</td></tr><tr><td>train/total_flos</td><td>6921111076909056.0</td></tr><tr><td>train/train_loss</td><td>1.03446</td></tr><tr><td>train/train_runtime</td><td>2265.22</td></tr><tr><td>train/train_samples_per_second</td><td>171.622</td></tr><tr><td>train/train_steps_per_second</td><td>1.787</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crisp-wind-65</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/84823slb' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/84823slb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_155221-84823slb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_163007-q1laf4n7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/q1laf4n7' target=\"_blank\">warm-dew-66</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/q1laf4n7' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/q1laf4n7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6074' max='6074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6074/6074 34:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.904572</td>\n",
       "      <td>0.676291</td>\n",
       "      <td>0.678108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.927829</td>\n",
       "      <td>0.676025</td>\n",
       "      <td>0.678417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.943130</td>\n",
       "      <td>0.681107</td>\n",
       "      <td>0.681966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.959677</td>\n",
       "      <td>0.676780</td>\n",
       "      <td>0.678725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.933554</td>\n",
       "      <td>0.683145</td>\n",
       "      <td>0.684415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.938400</td>\n",
       "      <td>0.681046</td>\n",
       "      <td>0.683088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.920184</td>\n",
       "      <td>0.685310</td>\n",
       "      <td>0.687183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909510</td>\n",
       "      <td>0.686306</td>\n",
       "      <td>0.687718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.896039</td>\n",
       "      <td>0.687725</td>\n",
       "      <td>0.688850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.884864</td>\n",
       "      <td>0.689374</td>\n",
       "      <td>0.691391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.869602</td>\n",
       "      <td>0.691697</td>\n",
       "      <td>0.693480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.866417</td>\n",
       "      <td>0.691236</td>\n",
       "      <td>0.693367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 17:04:31,214]\u001b[0m Trial 1 finished with value: 1.3846022626670083 and parameters: {'learning_rate': 8.0042071943296e-06, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 2, 'warmup_steps': 226, 'weight_decay': 0.009505582380919406, 'per_device_eval_batch_size': 32}. Best is trial 1 with value: 1.3846022626670083.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb4dc5cc189494b9178ce46ba9969e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▃▁▄▃▅▅▆▇██</td></tr><tr><td>eval/f1</td><td>▁▁▃▁▄▃▅▆▆▇██</td></tr><tr><td>eval/loss</td><td>▄▆▇█▆▆▅▄▃▂▁▁</td></tr><tr><td>eval/runtime</td><td>█▅▄▁▆▄▁▅▇▄▂▆</td></tr><tr><td>eval/samples_per_second</td><td>▁▄▄█▃▅█▄▂▅▇▃</td></tr><tr><td>eval/steps_per_second</td><td>▁▄▄█▃▅█▄▂▅▇▃</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▄▄▅▅▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.69337</td></tr><tr><td>eval/f1</td><td>0.69124</td></tr><tr><td>eval/loss</td><td>0.86642</td></tr><tr><td>eval/runtime</td><td>83.0871</td></tr><tr><td>eval/samples_per_second</td><td>1169.749</td></tr><tr><td>eval/steps_per_second</td><td>36.564</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>6074</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8006</td></tr><tr><td>train/total_flos</td><td>6921680856367104.0</td></tr><tr><td>train/train_loss</td><td>0.80064</td></tr><tr><td>train/train_runtime</td><td>2070.1468</td></tr><tr><td>train/train_samples_per_second</td><td>187.794</td></tr><tr><td>train/train_steps_per_second</td><td>2.934</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">warm-dew-66</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/q1laf4n7' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/q1laf4n7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_163007-q1laf4n7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_170437-bj5kyybo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/bj5kyybo' target=\"_blank\">dashing-eon-67</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/bj5kyybo' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/bj5kyybo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12149' max='12149' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12149/12149 54:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.957291</td>\n",
       "      <td>0.685401</td>\n",
       "      <td>0.686247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.979467</td>\n",
       "      <td>0.682548</td>\n",
       "      <td>0.683397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.061554</td>\n",
       "      <td>0.680159</td>\n",
       "      <td>0.680989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.078669</td>\n",
       "      <td>0.678191</td>\n",
       "      <td>0.679621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.121005</td>\n",
       "      <td>0.672390</td>\n",
       "      <td>0.675340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.145086</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.677326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.148397</td>\n",
       "      <td>0.674955</td>\n",
       "      <td>0.677779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.126086</td>\n",
       "      <td>0.676870</td>\n",
       "      <td>0.676822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.122853</td>\n",
       "      <td>0.676472</td>\n",
       "      <td>0.676997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.101280</td>\n",
       "      <td>0.680873</td>\n",
       "      <td>0.682121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.084300</td>\n",
       "      <td>0.682321</td>\n",
       "      <td>0.682923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.084388</td>\n",
       "      <td>0.678323</td>\n",
       "      <td>0.680228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.056453</td>\n",
       "      <td>0.681091</td>\n",
       "      <td>0.683283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.035598</td>\n",
       "      <td>0.684193</td>\n",
       "      <td>0.685598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.005407</td>\n",
       "      <td>0.682144</td>\n",
       "      <td>0.684642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.002883</td>\n",
       "      <td>0.684084</td>\n",
       "      <td>0.685310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.986410</td>\n",
       "      <td>0.684496</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.952885</td>\n",
       "      <td>0.688489</td>\n",
       "      <td>0.689230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.925663</td>\n",
       "      <td>0.690514</td>\n",
       "      <td>0.691751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.690069</td>\n",
       "      <td>0.692070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.892518</td>\n",
       "      <td>0.693757</td>\n",
       "      <td>0.695507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.884025</td>\n",
       "      <td>0.693684</td>\n",
       "      <td>0.695373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.875310</td>\n",
       "      <td>0.693426</td>\n",
       "      <td>0.695270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.868033</td>\n",
       "      <td>0.693639</td>\n",
       "      <td>0.695754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 17:59:00,790]\u001b[0m Trial 2 finished with value: 1.3893931968850302 and parameters: {'learning_rate': 9.72585136490878e-06, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1, 'warmup_steps': 450, 'weight_decay': 0.002903679962944256, 'per_device_eval_batch_size': 32}. Best is trial 2 with value: 1.3893931968850302.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375aaab089cc40479dcb2ab393fb1a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▅▄▃▂▁▂▂▂▂▃▄▃▄▅▄▄▅▆▇▇████</td></tr><tr><td>eval/f1</td><td>▅▄▄▃▁▃▂▂▂▄▄▃▄▅▄▅▅▆▇▇████</td></tr><tr><td>eval/loss</td><td>▃▄▆▆▇██▇▇▇▆▆▆▅▄▄▄▃▂▂▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▃▁▁▁▂▂▂▂▂▂▁▂▁▂▁▂▂▂▄▆▇█▃▆</td></tr><tr><td>eval/samples_per_second</td><td>▆███▇▇▇▇▇▇█▇█▇█▇▇▇▅▃▂▁▆▃</td></tr><tr><td>eval/steps_per_second</td><td>▆███▇▇▇▇▇▇█▇█▇█▇▇▇▅▃▂▁▆▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.69575</td></tr><tr><td>eval/f1</td><td>0.69364</td></tr><tr><td>eval/loss</td><td>0.86803</td></tr><tr><td>eval/runtime</td><td>85.3835</td></tr><tr><td>eval/samples_per_second</td><td>1138.288</td></tr><tr><td>eval/steps_per_second</td><td>35.581</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>12149</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6406</td></tr><tr><td>train/total_flos</td><td>6922125996568704.0</td></tr><tr><td>train/train_loss</td><td>0.64062</td></tr><tr><td>train/train_runtime</td><td>3269.5755</td></tr><tr><td>train/train_samples_per_second</td><td>118.903</td></tr><tr><td>train/train_steps_per_second</td><td>3.716</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-eon-67</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/bj5kyybo' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/bj5kyybo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_170437-bj5kyybo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0e7b41f51441a68e1336e7dbcba8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_175906-g2n8g6lu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/g2n8g6lu' target=\"_blank\">mild-plasma-68</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/g2n8g6lu' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/g2n8g6lu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6074' max='6074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6074/6074 48:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.873198</td>\n",
       "      <td>0.692193</td>\n",
       "      <td>0.694324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.868280</td>\n",
       "      <td>0.695727</td>\n",
       "      <td>0.697390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.855643</td>\n",
       "      <td>0.697153</td>\n",
       "      <td>0.698336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858870</td>\n",
       "      <td>0.698641</td>\n",
       "      <td>0.700240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850856</td>\n",
       "      <td>0.700345</td>\n",
       "      <td>0.701433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.851248</td>\n",
       "      <td>0.699806</td>\n",
       "      <td>0.701598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.842568</td>\n",
       "      <td>0.702038</td>\n",
       "      <td>0.703923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841271</td>\n",
       "      <td>0.701056</td>\n",
       "      <td>0.702472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.837960</td>\n",
       "      <td>0.702868</td>\n",
       "      <td>0.704108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.831500</td>\n",
       "      <td>0.703496</td>\n",
       "      <td>0.705117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.827053</td>\n",
       "      <td>0.704081</td>\n",
       "      <td>0.705621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.827638</td>\n",
       "      <td>0.703351</td>\n",
       "      <td>0.705312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 18:47:50,758]\u001b[0m Trial 3 finished with value: 1.408663555451863 and parameters: {'learning_rate': 4.716373095627379e-06, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 4, 'warmup_steps': 332, 'weight_decay': 0.00376190193606706, 'per_device_eval_batch_size': 32}. Best is trial 3 with value: 1.408663555451863.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc024e40af824852ab6baa7fb604e846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▃▅▅▆▇▆▇███</td></tr><tr><td>eval/f1</td><td>▁▃▄▅▆▅▇▆▇███</td></tr><tr><td>eval/loss</td><td>█▇▅▆▅▅▃▃▃▂▁▁</td></tr><tr><td>eval/runtime</td><td>▂▂▅▃█▁▂▃▃▃▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▇▇▄▆▁█▇▆▆▆▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▇▇▄▆▁█▇▆▆▆▆▆</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▄▄▅▅▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.70531</td></tr><tr><td>eval/f1</td><td>0.70335</td></tr><tr><td>eval/loss</td><td>0.82764</td></tr><tr><td>eval/runtime</td><td>84.0005</td></tr><tr><td>eval/samples_per_second</td><td>1157.029</td></tr><tr><td>eval/steps_per_second</td><td>36.166</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>6074</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8372</td></tr><tr><td>train/total_flos</td><td>6921680856367104.0</td></tr><tr><td>train/train_loss</td><td>0.83717</td></tr><tr><td>train/train_runtime</td><td>2929.9375</td></tr><tr><td>train/train_samples_per_second</td><td>132.686</td></tr><tr><td>train/train_steps_per_second</td><td>2.073</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-plasma-68</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/g2n8g6lu' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/g2n8g6lu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_175906-g2n8g6lu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a9e5b9ef55439ab22ae17d9e4ce03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332902596, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_184757-n7crvznb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/n7crvznb' target=\"_blank\">vocal-meadow-77</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/n7crvznb' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/n7crvznb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8099' max='8099' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8099/8099 55:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.963914</td>\n",
       "      <td>0.690410</td>\n",
       "      <td>0.691525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.978342</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>0.692317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.951713</td>\n",
       "      <td>0.690907</td>\n",
       "      <td>0.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.911814</td>\n",
       "      <td>0.694140</td>\n",
       "      <td>0.695003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.953980</td>\n",
       "      <td>0.688653</td>\n",
       "      <td>0.691319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.889982</td>\n",
       "      <td>0.697904</td>\n",
       "      <td>0.698233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.908312</td>\n",
       "      <td>0.693674</td>\n",
       "      <td>0.695486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.881859</td>\n",
       "      <td>0.699578</td>\n",
       "      <td>0.702195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.859502</td>\n",
       "      <td>0.700771</td>\n",
       "      <td>0.702616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.837281</td>\n",
       "      <td>0.705313</td>\n",
       "      <td>0.707679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.835985</td>\n",
       "      <td>0.705491</td>\n",
       "      <td>0.707627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.837412</td>\n",
       "      <td>0.708348</td>\n",
       "      <td>0.709623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.817435</td>\n",
       "      <td>0.707590</td>\n",
       "      <td>0.709644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.810078</td>\n",
       "      <td>0.711333</td>\n",
       "      <td>0.712926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.807342</td>\n",
       "      <td>0.711519</td>\n",
       "      <td>0.713060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.807551</td>\n",
       "      <td>0.712142</td>\n",
       "      <td>0.714120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 19:43:17,073]\u001b[0m Trial 4 finished with value: 1.4262618535121545 and parameters: {'learning_rate': 1.5478032577602657e-05, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 3, 'warmup_steps': 322, 'weight_decay': 0.008013760485701626, 'per_device_eval_batch_size': 32}. Best is trial 4 with value: 1.4262618535121545.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672a02f600e1485f93cb7a54e0476a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▁▂▁▃▂▄▄▆▆▇▇███</td></tr><tr><td>eval/f1</td><td>▂▂▂▃▁▄▂▄▅▆▆▇▇███</td></tr><tr><td>eval/loss</td><td>▇█▇▅▇▄▅▄▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▃▁▂▁▅▅▄▆▃▄▃█▆▄▄</td></tr><tr><td>eval/samples_per_second</td><td>▄▆▇▇█▄▄▅▃▆▅▆▁▃▅▅</td></tr><tr><td>eval/steps_per_second</td><td>▄▆█▇█▄▄▅▃▆▅▆▁▃▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.71412</td></tr><tr><td>eval/f1</td><td>0.71214</td></tr><tr><td>eval/loss</td><td>0.80755</td></tr><tr><td>eval/runtime</td><td>84.2675</td></tr><tr><td>eval/samples_per_second</td><td>1153.363</td></tr><tr><td>eval/steps_per_second</td><td>36.052</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>8099</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7543</td></tr><tr><td>train/total_flos</td><td>6921965746096128.0</td></tr><tr><td>train/train_loss</td><td>0.75432</td></tr><tr><td>train/train_runtime</td><td>3326.295</td></tr><tr><td>train/train_samples_per_second</td><td>116.875</td></tr><tr><td>train/train_steps_per_second</td><td>2.435</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-meadow-77</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/n7crvznb' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/n7crvznb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_184757-n7crvznb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193b211c89e74bc2ac5259be3a28e9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_194323-qmszq5dw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/qmszq5dw' target=\"_blank\">icy-bird-78</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/qmszq5dw' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/qmszq5dw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4049' max='4049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4049/4049 27:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.994766</td>\n",
       "      <td>0.708040</td>\n",
       "      <td>0.708944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.036872</td>\n",
       "      <td>0.704104</td>\n",
       "      <td>0.705034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.038139</td>\n",
       "      <td>0.705081</td>\n",
       "      <td>0.706598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.028348</td>\n",
       "      <td>0.705412</td>\n",
       "      <td>0.707082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.003674</td>\n",
       "      <td>0.705823</td>\n",
       "      <td>0.707205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.968645</td>\n",
       "      <td>0.705709</td>\n",
       "      <td>0.706784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.923456</td>\n",
       "      <td>0.706456</td>\n",
       "      <td>0.707679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.899002</td>\n",
       "      <td>0.706659</td>\n",
       "      <td>0.708018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 20:11:24,967]\u001b[0m Trial 5 finished with value: 1.4146772834597436 and parameters: {'learning_rate': 1.478613898874922e-06, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 3, 'warmup_steps': 171, 'weight_decay': 0.009318396304951801, 'per_device_eval_batch_size': 32}. Best is trial 4 with value: 1.4262618535121545.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbf2ae85d8f42e59a04716a0a58f666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>█▁▄▅▅▄▆▆</td></tr><tr><td>eval/f1</td><td>█▁▃▃▄▄▅▆</td></tr><tr><td>eval/loss</td><td>▆███▆▅▂▁</td></tr><tr><td>eval/runtime</td><td>▆▃▂▁▄▄█▇</td></tr><tr><td>eval/samples_per_second</td><td>▃▆▇█▅▅▁▂</td></tr><tr><td>eval/steps_per_second</td><td>▃▆▇█▅▅▁▂</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.70802</td></tr><tr><td>eval/f1</td><td>0.70666</td></tr><tr><td>eval/loss</td><td>0.899</td></tr><tr><td>eval/runtime</td><td>84.0882</td></tr><tr><td>eval/samples_per_second</td><td>1155.823</td></tr><tr><td>eval/steps_per_second</td><td>36.129</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>4049</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5153</td></tr><tr><td>train/total_flos</td><td>6921111076909056.0</td></tr><tr><td>train/train_loss</td><td>0.51526</td></tr><tr><td>train/train_runtime</td><td>1687.8738</td></tr><tr><td>train/train_samples_per_second</td><td>230.326</td></tr><tr><td>train/train_steps_per_second</td><td>2.399</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-bird-78</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/qmszq5dw' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/qmszq5dw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_194323-qmszq5dw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0baca922bfd641f3be3f8f5893aed040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332902596, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_201133-u8pipqwa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/u8pipqwa' target=\"_blank\">autumn-lake-79</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/u8pipqwa' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/u8pipqwa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='4049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 500/4049 06:27 < 46:02, 1.28 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.291916</td>\n",
       "      <td>0.682407</td>\n",
       "      <td>0.683633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 20:18:10,912]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee15d52450640dda4462eb30a95ef94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.68363</td></tr><tr><td>eval/f1</td><td>0.68241</td></tr><tr><td>eval/loss</td><td>1.29192</td></tr><tr><td>eval/runtime</td><td>160.5545</td></tr><tr><td>eval/samples_per_second</td><td>605.346</td></tr><tr><td>eval/steps_per_second</td><td>37.838</td></tr><tr><td>train/epoch</td><td>0.12</td></tr><tr><td>train/global_step</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autumn-lake-79</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/u8pipqwa' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/u8pipqwa</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_201133-u8pipqwa\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f730be513c8b4a9db6762b1da60fcddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_201817-ztzzzdnd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/ztzzzdnd' target=\"_blank\">winter-donkey-80</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/ztzzzdnd' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/ztzzzdnd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2429' max='2429' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2429/2429 21:29, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.288374</td>\n",
       "      <td>0.692460</td>\n",
       "      <td>0.693572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.241664</td>\n",
       "      <td>0.691721</td>\n",
       "      <td>0.692410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.122425</td>\n",
       "      <td>0.694665</td>\n",
       "      <td>0.696011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.944313</td>\n",
       "      <td>0.699396</td>\n",
       "      <td>0.701114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 20:39:55,841]\u001b[0m Trial 7 finished with value: 1.400510697336472 and parameters: {'learning_rate': 1.7031152456350014e-05, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 5, 'warmup_steps': 366, 'weight_decay': 0.002920701632894948, 'per_device_eval_batch_size': 32}. Best is trial 4 with value: 1.4262618535121545.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fafd26aca8474991b149b0435fbbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▂▁▄█</td></tr><tr><td>eval/f1</td><td>▂▁▄█</td></tr><tr><td>eval/loss</td><td>█▇▅▁</td></tr><tr><td>eval/runtime</td><td>▂▃█▁</td></tr><tr><td>eval/samples_per_second</td><td>▇▆▁█</td></tr><tr><td>eval/steps_per_second</td><td>▇▆▁█</td></tr><tr><td>train/epoch</td><td>▁▃▅▆██</td></tr><tr><td>train/global_step</td><td>▁▃▅▆██</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.70111</td></tr><tr><td>eval/f1</td><td>0.6994</td></tr><tr><td>eval/loss</td><td>0.94431</td></tr><tr><td>eval/runtime</td><td>83.8781</td></tr><tr><td>eval/samples_per_second</td><td>1158.718</td></tr><tr><td>eval/steps_per_second</td><td>36.219</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>2429</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4721</td></tr><tr><td>train/total_flos</td><td>7774640705064960.0</td></tr><tr><td>train/train_loss</td><td>0.47213</td></tr><tr><td>train/train_runtime</td><td>1304.915</td></tr><tr><td>train/train_samples_per_second</td><td>297.921</td></tr><tr><td>train/train_steps_per_second</td><td>1.861</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-donkey-80</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/ztzzzdnd' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/ztzzzdnd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_201817-ztzzzdnd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c95288d9c234433861c25c3fd8abe61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333333135428, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_204002-2ezu0gsg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/2ezu0gsg' target=\"_blank\">elated-frog-81</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/2ezu0gsg' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/2ezu0gsg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='6074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 500/6074 04:07 < 46:09, 2.01 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.498300</td>\n",
       "      <td>0.685156</td>\n",
       "      <td>0.686133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 20:44:18,714]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c96a0985891448bbaf8127665390e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.68613</td></tr><tr><td>eval/f1</td><td>0.68516</td></tr><tr><td>eval/loss</td><td>1.4983</td></tr><tr><td>eval/runtime</td><td>160.1391</td></tr><tr><td>eval/samples_per_second</td><td>606.916</td></tr><tr><td>eval/steps_per_second</td><td>37.936</td></tr><tr><td>train/epoch</td><td>0.08</td></tr><tr><td>train/global_step</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-frog-81</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/2ezu0gsg' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/2ezu0gsg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_204002-2ezu0gsg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea56255c21048e7ac6403ff997736cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_204424-y227tvm4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/y227tvm4' target=\"_blank\">rich-pine-82</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/y227tvm4' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/y227tvm4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='6074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 500/6074 04:05 < 45:52, 2.02 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.856807</td>\n",
       "      <td>0.688943</td>\n",
       "      <td>0.689611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 20:48:39,150]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 1.5478032577602657e-05, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 3, 'warmup_steps': 322, 'weight_decay': 0.008013760485701626, 'per_device_eval_batch_size': 32}\n",
      "{'output_dir': WindowsPath('C:/Users/liyag/OneDrive - mail.tau.ac.il/Desktop/NLP/results'), 'overwrite_output_dir': True, 'greater_is_better': True, 'evaluation_strategy': 'steps', 'do_train': True, 'logging_strategy': 'epoch', 'save_strategy': 'epoch', 'report_to': 'wandb', 'learning_rate': 1.5478032577602657e-05, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 3, 'warmup_steps': 322, 'weight_decay': 0.008013760485701626, 'per_device_eval_batch_size': 32}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8099' max='8099' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8099/8099 54:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.872986</td>\n",
       "      <td>0.689693</td>\n",
       "      <td>0.689498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.760890</td>\n",
       "      <td>0.674682</td>\n",
       "      <td>0.673962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.395288</td>\n",
       "      <td>0.677090</td>\n",
       "      <td>0.678417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.317701</td>\n",
       "      <td>0.682480</td>\n",
       "      <td>0.684271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.315684</td>\n",
       "      <td>0.685224</td>\n",
       "      <td>0.687461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.224269</td>\n",
       "      <td>0.682763</td>\n",
       "      <td>0.682625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.198329</td>\n",
       "      <td>0.685566</td>\n",
       "      <td>0.686092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.207603</td>\n",
       "      <td>0.689087</td>\n",
       "      <td>0.690753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.084959</td>\n",
       "      <td>0.691835</td>\n",
       "      <td>0.692544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.001050</td>\n",
       "      <td>0.695690</td>\n",
       "      <td>0.697441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.999224</td>\n",
       "      <td>0.696240</td>\n",
       "      <td>0.698254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.966328</td>\n",
       "      <td>0.701517</td>\n",
       "      <td>0.702699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.892981</td>\n",
       "      <td>0.701678</td>\n",
       "      <td>0.703491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.706685</td>\n",
       "      <td>0.708173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.839711</td>\n",
       "      <td>0.707959</td>\n",
       "      <td>0.709551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.831531</td>\n",
       "      <td>0.707789</td>\n",
       "      <td>0.709881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model was saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▄▄▁▂▃▄▃▃▄▅▆▆▇▇███</td></tr><tr><td>eval/f1</td><td>▄▄▁▂▃▃▃▃▄▅▅▆▇▇███</td></tr><tr><td>eval/loss</td><td>██▇▅▄▄▄▃▄▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁████████████████</td></tr><tr><td>eval/steps_per_second</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.70988</td></tr><tr><td>eval/f1</td><td>0.70779</td></tr><tr><td>eval/loss</td><td>0.83153</td></tr><tr><td>eval/runtime</td><td>83.3805</td></tr><tr><td>eval/samples_per_second</td><td>1165.632</td></tr><tr><td>eval/steps_per_second</td><td>36.435</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>8099</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4835</td></tr><tr><td>train/total_flos</td><td>6921965746096128.0</td></tr><tr><td>train/train_loss</td><td>0.48349</td></tr><tr><td>train/train_runtime</td><td>3289.4875</td></tr><tr><td>train/train_samples_per_second</td><td>118.183</td></tr><tr><td>train/train_steps_per_second</td><td>2.462</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rich-pine-82</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/y227tvm4' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/y227tvm4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_204424-y227tvm4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+------------+---------------+\n",
      "|                     Modules                     | Parameters | Sum of Tensor |\n",
      "+-------------------------------------------------+------------+---------------+\n",
      "|               transformer.mask_emb              |    768     |     7.254     |\n",
      "|        transformer.word_embedding.weight        |  24576000  |   1073323.0   |\n",
      "|          transformer.layer.0.rel_attn.q         |   589824   |   44516.6953  |\n",
      "|          transformer.layer.0.rel_attn.k         |   589824   |    94528.0    |\n",
      "|          transformer.layer.0.rel_attn.v         |   589824   |   15092.6914  |\n",
      "|          transformer.layer.0.rel_attn.o         |   589824   |   16119.0615  |\n",
      "|          transformer.layer.0.rel_attn.r         |   589824   |   10848.5449  |\n",
      "|      transformer.layer.0.rel_attn.r_r_bias      |    768     |    117.9462   |\n",
      "|      transformer.layer.0.rel_attn.r_s_bias      |    768     |    136.0362   |\n",
      "|      transformer.layer.0.rel_attn.r_w_bias      |    768     |    158.9057   |\n",
      "|      transformer.layer.0.rel_attn.seg_embed     |    1536    |    355.0389   |\n",
      "|  transformer.layer.0.rel_attn.layer_norm.weight |    768     |    739.7091   |\n",
      "|   transformer.layer.0.rel_attn.layer_norm.bias  |    768     |    93.3119    |\n",
      "|     transformer.layer.0.ff.layer_norm.weight    |    768     |    818.2251   |\n",
      "|      transformer.layer.0.ff.layer_norm.bias     |    768     |    89.1191    |\n",
      "|      transformer.layer.0.ff.layer_1.weight      |  2359296   |   65556.9531  |\n",
      "|       transformer.layer.0.ff.layer_1.bias       |    3072    |    360.2599   |\n",
      "|      transformer.layer.0.ff.layer_2.weight      |  2359296   |   57193.0273  |\n",
      "|       transformer.layer.0.ff.layer_2.bias       |    768     |    30.7977    |\n",
      "|          transformer.layer.1.rel_attn.q         |   589824   |   16621.5742  |\n",
      "|          transformer.layer.1.rel_attn.k         |   589824   |    20351.25   |\n",
      "|          transformer.layer.1.rel_attn.v         |   589824   |   15446.0039  |\n",
      "|          transformer.layer.1.rel_attn.o         |   589824   |   15908.8301  |\n",
      "|          transformer.layer.1.rel_attn.r         |   589824   |    5369.167   |\n",
      "|      transformer.layer.1.rel_attn.r_r_bias      |    768     |    211.618    |\n",
      "|      transformer.layer.1.rel_attn.r_s_bias      |    768     |    209.2944   |\n",
      "|      transformer.layer.1.rel_attn.r_w_bias      |    768     |    128.2724   |\n",
      "|      transformer.layer.1.rel_attn.seg_embed     |    1536    |    210.7794   |\n",
      "|  transformer.layer.1.rel_attn.layer_norm.weight |    768     |    724.1213   |\n",
      "|   transformer.layer.1.rel_attn.layer_norm.bias  |    768     |    111.2842   |\n",
      "|     transformer.layer.1.ff.layer_norm.weight    |    768     |    713.9308   |\n",
      "|      transformer.layer.1.ff.layer_norm.bias     |    768     |    114.9962   |\n",
      "|      transformer.layer.1.ff.layer_1.weight      |  2359296   |   70957.2891  |\n",
      "|       transformer.layer.1.ff.layer_1.bias       |    3072    |    339.3696   |\n",
      "|      transformer.layer.1.ff.layer_2.weight      |  2359296   |    61329.5    |\n",
      "|       transformer.layer.1.ff.layer_2.bias       |    768     |    35.8274    |\n",
      "|          transformer.layer.2.rel_attn.q         |   589824   |   16489.3828  |\n",
      "|          transformer.layer.2.rel_attn.k         |   589824   |   19956.7227  |\n",
      "|          transformer.layer.2.rel_attn.v         |   589824   |   15804.4395  |\n",
      "|          transformer.layer.2.rel_attn.o         |   589824   |   16022.5977  |\n",
      "|          transformer.layer.2.rel_attn.r         |   589824   |   5338.2607   |\n",
      "|      transformer.layer.2.rel_attn.r_r_bias      |    768     |    247.8882   |\n",
      "|      transformer.layer.2.rel_attn.r_s_bias      |    768     |    197.3545   |\n",
      "|      transformer.layer.2.rel_attn.r_w_bias      |    768     |    112.4253   |\n",
      "|      transformer.layer.2.rel_attn.seg_embed     |    1536    |    236.2521   |\n",
      "|  transformer.layer.2.rel_attn.layer_norm.weight |    768     |    707.5247   |\n",
      "|   transformer.layer.2.rel_attn.layer_norm.bias  |    768     |    123.1936   |\n",
      "|     transformer.layer.2.ff.layer_norm.weight    |    768     |    832.1479   |\n",
      "|      transformer.layer.2.ff.layer_norm.bias     |    768     |    95.9052    |\n",
      "|      transformer.layer.2.ff.layer_1.weight      |  2359296   |    71196.5    |\n",
      "|       transformer.layer.2.ff.layer_1.bias       |    3072    |    327.6783   |\n",
      "|      transformer.layer.2.ff.layer_2.weight      |  2359296   |   60559.1758  |\n",
      "|       transformer.layer.2.ff.layer_2.bias       |    768     |    46.5203    |\n",
      "|          transformer.layer.3.rel_attn.q         |   589824   |   14201.9092  |\n",
      "|          transformer.layer.3.rel_attn.k         |   589824   |   18440.4395  |\n",
      "|          transformer.layer.3.rel_attn.v         |   589824   |   16347.0186  |\n",
      "|          transformer.layer.3.rel_attn.o         |   589824   |   16101.8408  |\n",
      "|          transformer.layer.3.rel_attn.r         |   589824   |   5741.2773   |\n",
      "|      transformer.layer.3.rel_attn.r_r_bias      |    768     |    300.1675   |\n",
      "|      transformer.layer.3.rel_attn.r_s_bias      |    768     |    241.8109   |\n",
      "|      transformer.layer.3.rel_attn.r_w_bias      |    768     |    248.1956   |\n",
      "|      transformer.layer.3.rel_attn.seg_embed     |    1536    |    227.1258   |\n",
      "|  transformer.layer.3.rel_attn.layer_norm.weight |    768     |    695.2471   |\n",
      "|   transformer.layer.3.rel_attn.layer_norm.bias  |    768     |    110.2333   |\n",
      "|     transformer.layer.3.ff.layer_norm.weight    |    768     |    859.3008   |\n",
      "|      transformer.layer.3.ff.layer_norm.bias     |    768     |    142.6005   |\n",
      "|      transformer.layer.3.ff.layer_1.weight      |  2359296   |   71190.8281  |\n",
      "|       transformer.layer.3.ff.layer_1.bias       |    3072    |    337.9751   |\n",
      "|      transformer.layer.3.ff.layer_2.weight      |  2359296   |   59924.7969  |\n",
      "|       transformer.layer.3.ff.layer_2.bias       |    768     |    46.4886    |\n",
      "|          transformer.layer.4.rel_attn.q         |   589824   |   16041.9648  |\n",
      "|          transformer.layer.4.rel_attn.k         |   589824   |   18267.7031  |\n",
      "|          transformer.layer.4.rel_attn.v         |   589824   |   16337.3252  |\n",
      "|          transformer.layer.4.rel_attn.o         |   589824   |   16669.9023  |\n",
      "|          transformer.layer.4.rel_attn.r         |   589824   |   5013.7627   |\n",
      "|      transformer.layer.4.rel_attn.r_r_bias      |    768     |    149.1275   |\n",
      "|      transformer.layer.4.rel_attn.r_s_bias      |    768     |    159.751    |\n",
      "|      transformer.layer.4.rel_attn.r_w_bias      |    768     |    85.6629    |\n",
      "|      transformer.layer.4.rel_attn.seg_embed     |    1536    |    193.0969   |\n",
      "|  transformer.layer.4.rel_attn.layer_norm.weight |    768     |    697.0922   |\n",
      "|   transformer.layer.4.rel_attn.layer_norm.bias  |    768     |    119.1436   |\n",
      "|     transformer.layer.4.ff.layer_norm.weight    |    768     |    842.9413   |\n",
      "|      transformer.layer.4.ff.layer_norm.bias     |    768     |    74.3242    |\n",
      "|      transformer.layer.4.ff.layer_1.weight      |  2359296   |   70438.3594  |\n",
      "|       transformer.layer.4.ff.layer_1.bias       |    3072    |    289.3455   |\n",
      "|      transformer.layer.4.ff.layer_2.weight      |  2359296   |   59469.4609  |\n",
      "|       transformer.layer.4.ff.layer_2.bias       |    768     |     52.86     |\n",
      "|          transformer.layer.5.rel_attn.q         |   589824   |   20057.9277  |\n",
      "|          transformer.layer.5.rel_attn.k         |   589824   |   22951.125   |\n",
      "|          transformer.layer.5.rel_attn.v         |   589824   |   16004.6465  |\n",
      "|          transformer.layer.5.rel_attn.o         |   589824   |   15618.6191  |\n",
      "|          transformer.layer.5.rel_attn.r         |   589824   |   5893.2017   |\n",
      "|      transformer.layer.5.rel_attn.r_r_bias      |    768     |    135.0706   |\n",
      "|      transformer.layer.5.rel_attn.r_s_bias      |    768     |    144.1386   |\n",
      "|      transformer.layer.5.rel_attn.r_w_bias      |    768     |    91.5611    |\n",
      "|      transformer.layer.5.rel_attn.seg_embed     |    1536    |    191.4123   |\n",
      "|  transformer.layer.5.rel_attn.layer_norm.weight |    768     |    710.2025   |\n",
      "|   transformer.layer.5.rel_attn.layer_norm.bias  |    768     |    67.4739    |\n",
      "|     transformer.layer.5.ff.layer_norm.weight    |    768     |   1048.9409   |\n",
      "|      transformer.layer.5.ff.layer_norm.bias     |    768     |    112.0247   |\n",
      "|      transformer.layer.5.ff.layer_1.weight      |  2359296   |   67723.125   |\n",
      "|       transformer.layer.5.ff.layer_1.bias       |    3072    |    309.3526   |\n",
      "|      transformer.layer.5.ff.layer_2.weight      |  2359296   |   57270.0078  |\n",
      "|       transformer.layer.5.ff.layer_2.bias       |    768     |     48.547    |\n",
      "|          transformer.layer.6.rel_attn.q         |   589824   |   17462.4492  |\n",
      "|          transformer.layer.6.rel_attn.k         |   589824   |   20022.082   |\n",
      "|          transformer.layer.6.rel_attn.v         |   589824   |   16422.5898  |\n",
      "|          transformer.layer.6.rel_attn.o         |   589824   |   15961.0244  |\n",
      "|          transformer.layer.6.rel_attn.r         |   589824   |   5371.1924   |\n",
      "|      transformer.layer.6.rel_attn.r_r_bias      |    768     |    79.5023    |\n",
      "|      transformer.layer.6.rel_attn.r_s_bias      |    768     |    143.3034   |\n",
      "|      transformer.layer.6.rel_attn.r_w_bias      |    768     |    74.7939    |\n",
      "|      transformer.layer.6.rel_attn.seg_embed     |    1536    |    123.7623   |\n",
      "|  transformer.layer.6.rel_attn.layer_norm.weight |    768     |    824.7728   |\n",
      "|   transformer.layer.6.rel_attn.layer_norm.bias  |    768     |    100.4918   |\n",
      "|     transformer.layer.6.ff.layer_norm.weight    |    768     |   1045.1604   |\n",
      "|      transformer.layer.6.ff.layer_norm.bias     |    768     |    77.5212    |\n",
      "|      transformer.layer.6.ff.layer_1.weight      |  2359296   |   71031.4609  |\n",
      "|       transformer.layer.6.ff.layer_1.bias       |    3072    |    286.2841   |\n",
      "|      transformer.layer.6.ff.layer_2.weight      |  2359296   |   55266.4453  |\n",
      "|       transformer.layer.6.ff.layer_2.bias       |    768     |    54.1989    |\n",
      "|          transformer.layer.7.rel_attn.q         |   589824   |   17523.916   |\n",
      "|          transformer.layer.7.rel_attn.k         |   589824   |   20381.0098  |\n",
      "|          transformer.layer.7.rel_attn.v         |   589824   |   16730.4355  |\n",
      "|          transformer.layer.7.rel_attn.o         |   589824   |   15741.9336  |\n",
      "|          transformer.layer.7.rel_attn.r         |   589824   |   5178.6074   |\n",
      "|      transformer.layer.7.rel_attn.r_r_bias      |    768     |    86.9653    |\n",
      "|      transformer.layer.7.rel_attn.r_s_bias      |    768     |    141.4761   |\n",
      "|      transformer.layer.7.rel_attn.r_w_bias      |    768     |    53.3114    |\n",
      "|      transformer.layer.7.rel_attn.seg_embed     |    1536    |    146.9689   |\n",
      "|  transformer.layer.7.rel_attn.layer_norm.weight |    768     |    859.1363   |\n",
      "|   transformer.layer.7.rel_attn.layer_norm.bias  |    768     |     75.514    |\n",
      "|     transformer.layer.7.ff.layer_norm.weight    |    768     |   1135.0479   |\n",
      "|      transformer.layer.7.ff.layer_norm.bias     |    768     |    86.7028    |\n",
      "|      transformer.layer.7.ff.layer_1.weight      |  2359296   |   70600.5938  |\n",
      "|       transformer.layer.7.ff.layer_1.bias       |    3072    |    301.3833   |\n",
      "|      transformer.layer.7.ff.layer_2.weight      |  2359296   |   53523.6914  |\n",
      "|       transformer.layer.7.ff.layer_2.bias       |    768     |    58.7541    |\n",
      "|          transformer.layer.8.rel_attn.q         |   589824   |   17310.2656  |\n",
      "|          transformer.layer.8.rel_attn.k         |   589824   |   20113.9023  |\n",
      "|          transformer.layer.8.rel_attn.v         |   589824   |   17486.1016  |\n",
      "|          transformer.layer.8.rel_attn.o         |   589824   |   16634.457   |\n",
      "|          transformer.layer.8.rel_attn.r         |   589824   |   4995.8047   |\n",
      "|      transformer.layer.8.rel_attn.r_r_bias      |    768     |    76.3157    |\n",
      "|      transformer.layer.8.rel_attn.r_s_bias      |    768     |    104.5686   |\n",
      "|      transformer.layer.8.rel_attn.r_w_bias      |    768     |    50.6241    |\n",
      "|      transformer.layer.8.rel_attn.seg_embed     |    1536    |    131.8607   |\n",
      "|  transformer.layer.8.rel_attn.layer_norm.weight |    768     |   1081.1025   |\n",
      "|   transformer.layer.8.rel_attn.layer_norm.bias  |    768     |    68.8813    |\n",
      "|     transformer.layer.8.ff.layer_norm.weight    |    768     |   1077.2712   |\n",
      "|      transformer.layer.8.ff.layer_norm.bias     |    768     |    70.7522    |\n",
      "|      transformer.layer.8.ff.layer_1.weight      |  2359296   |   80630.8281  |\n",
      "|       transformer.layer.8.ff.layer_1.bias       |    3072    |    258.1456   |\n",
      "|      transformer.layer.8.ff.layer_2.weight      |  2359296   |   49929.6367  |\n",
      "|       transformer.layer.8.ff.layer_2.bias       |    768     |    48.4146    |\n",
      "|          transformer.layer.9.rel_attn.q         |   589824   |   19113.3242  |\n",
      "|          transformer.layer.9.rel_attn.k         |   589824   |   22511.4648  |\n",
      "|          transformer.layer.9.rel_attn.v         |   589824   |   17783.8086  |\n",
      "|          transformer.layer.9.rel_attn.o         |   589824   |   16196.8965  |\n",
      "|          transformer.layer.9.rel_attn.r         |   589824   |   4870.5957   |\n",
      "|      transformer.layer.9.rel_attn.r_r_bias      |    768     |    71.5733    |\n",
      "|      transformer.layer.9.rel_attn.r_s_bias      |    768     |    115.7824   |\n",
      "|      transformer.layer.9.rel_attn.r_w_bias      |    768     |    66.1586    |\n",
      "|      transformer.layer.9.rel_attn.seg_embed     |    1536    |    114.4543   |\n",
      "|  transformer.layer.9.rel_attn.layer_norm.weight |    768     |   1043.3905   |\n",
      "|   transformer.layer.9.rel_attn.layer_norm.bias  |    768     |     53.611    |\n",
      "|     transformer.layer.9.ff.layer_norm.weight    |    768     |   1179.9061   |\n",
      "|      transformer.layer.9.ff.layer_norm.bias     |    768     |    80.7012    |\n",
      "|      transformer.layer.9.ff.layer_1.weight      |  2359296   |   78616.1875  |\n",
      "|       transformer.layer.9.ff.layer_1.bias       |    3072    |    292.207    |\n",
      "|      transformer.layer.9.ff.layer_2.weight      |  2359296   |   52394.168   |\n",
      "|       transformer.layer.9.ff.layer_2.bias       |    768     |    45.2117    |\n",
      "|         transformer.layer.10.rel_attn.q         |   589824   |   15598.625   |\n",
      "|         transformer.layer.10.rel_attn.k         |   589824   |   18651.3164  |\n",
      "|         transformer.layer.10.rel_attn.v         |   589824   |   19207.5664  |\n",
      "|         transformer.layer.10.rel_attn.o         |   589824   |   17643.9102  |\n",
      "|         transformer.layer.10.rel_attn.r         |   589824   |   4347.3672   |\n",
      "|      transformer.layer.10.rel_attn.r_r_bias     |    768     |    94.0733    |\n",
      "|      transformer.layer.10.rel_attn.r_s_bias     |    768     |    110.1988   |\n",
      "|      transformer.layer.10.rel_attn.r_w_bias     |    768     |    93.5657    |\n",
      "|     transformer.layer.10.rel_attn.seg_embed     |    1536    |    111.9691   |\n",
      "| transformer.layer.10.rel_attn.layer_norm.weight |    768     |   1054.1653   |\n",
      "|  transformer.layer.10.rel_attn.layer_norm.bias  |    768     |    61.7568    |\n",
      "|    transformer.layer.10.ff.layer_norm.weight    |    768     |   1021.5345   |\n",
      "|     transformer.layer.10.ff.layer_norm.bias     |    768     |     63.315    |\n",
      "|      transformer.layer.10.ff.layer_1.weight     |  2359296   |   78410.9375  |\n",
      "|       transformer.layer.10.ff.layer_1.bias      |    3072    |    258.3932   |\n",
      "|      transformer.layer.10.ff.layer_2.weight     |  2359296   |   50200.793   |\n",
      "|       transformer.layer.10.ff.layer_2.bias      |    768     |    41.4479    |\n",
      "|         transformer.layer.11.rel_attn.q         |   589824   |   18407.0684  |\n",
      "|         transformer.layer.11.rel_attn.k         |   589824   |   23457.332   |\n",
      "|         transformer.layer.11.rel_attn.v         |   589824   |   24126.5527  |\n",
      "|         transformer.layer.11.rel_attn.o         |   589824   |   20705.4688  |\n",
      "|         transformer.layer.11.rel_attn.r         |   589824   |   3692.2422   |\n",
      "|      transformer.layer.11.rel_attn.r_r_bias     |    768     |    93.5545    |\n",
      "|      transformer.layer.11.rel_attn.r_s_bias     |    768     |    76.0979    |\n",
      "|      transformer.layer.11.rel_attn.r_w_bias     |    768     |    94.9552    |\n",
      "|     transformer.layer.11.rel_attn.seg_embed     |    1536    |    84.2926    |\n",
      "| transformer.layer.11.rel_attn.layer_norm.weight |    768     |    1216.405   |\n",
      "|  transformer.layer.11.rel_attn.layer_norm.bias  |    768     |    76.2078    |\n",
      "|    transformer.layer.11.ff.layer_norm.weight    |    768     |   1857.5792   |\n",
      "|     transformer.layer.11.ff.layer_norm.bias     |    768     |    135.2948   |\n",
      "|      transformer.layer.11.ff.layer_1.weight     |  2359296   |   82164.1094  |\n",
      "|       transformer.layer.11.ff.layer_1.bias      |    3072    |    300.1009   |\n",
      "|      transformer.layer.11.ff.layer_2.weight     |  2359296   |   45293.8633  |\n",
      "|       transformer.layer.11.ff.layer_2.bias      |    768     |    35.8956    |\n",
      "|         sequence_summary.summary.weight         |   589824   |   9454.0723   |\n",
      "|          sequence_summary.summary.bias          |    768     |     0.7698    |\n",
      "|                logits_proj.weight               |    4608    |    78.2072    |\n",
      "|                 logits_proj.bias                |     6      |     0.0063    |\n",
      "|              Total Trainable Params             | 117313542  |   3685509.25  |\n",
      "+-------------------------------------------------+------------+---------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 value: 0.7080168983557641\n",
      "accuracy value: 0.7100554578098796\n",
      "==========================================================================\n",
      "============================== bert-base-uncased ==============================\n",
      "==========================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/388761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97191 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+------------+---------------+\n",
      "|                         Modules                         | Parameters | Sum of Tensor |\n",
      "+---------------------------------------------------------+------------+---------------+\n",
      "|          bert.embeddings.word_embeddings.weight         |  23440896  |   956970.75   |\n",
      "|        bert.embeddings.position_embeddings.weight       |   393216   |   4574.8262   |\n",
      "|       bert.embeddings.token_type_embeddings.weight      |    1536    |    13.3915    |\n",
      "|             bert.embeddings.LayerNorm.weight            |    768     |    652.2689   |\n",
      "|              bert.embeddings.LayerNorm.bias             |    768     |     31.436    |\n",
      "|     bert.encoder.layer.0.attention.self.query.weight    |   589824   |   19902.5742  |\n",
      "|      bert.encoder.layer.0.attention.self.query.bias     |    768     |    167.2101   |\n",
      "|      bert.encoder.layer.0.attention.self.key.weight     |   589824   |   19553.2656  |\n",
      "|       bert.encoder.layer.0.attention.self.key.bias      |    768     |     1.8726    |\n",
      "|     bert.encoder.layer.0.attention.self.value.weight    |   589824   |   13380.5234  |\n",
      "|      bert.encoder.layer.0.attention.self.value.bias     |    768     |    20.1368    |\n",
      "|    bert.encoder.layer.0.attention.output.dense.weight   |   589824   |   12941.5518  |\n",
      "|     bert.encoder.layer.0.attention.output.dense.bias    |    768     |    19.1165    |\n",
      "|  bert.encoder.layer.0.attention.output.LayerNorm.weight |    768     |    736.113    |\n",
      "|   bert.encoder.layer.0.attention.output.LayerNorm.bias  |    768     |    153.3692   |\n",
      "|      bert.encoder.layer.0.intermediate.dense.weight     |  2359296   |   69865.4844  |\n",
      "|       bert.encoder.layer.0.intermediate.dense.bias      |    3072    |    347.0836   |\n",
      "|         bert.encoder.layer.0.output.dense.weight        |  2359296   |   65825.6328  |\n",
      "|          bert.encoder.layer.0.output.dense.bias         |    768     |     44.403    |\n",
      "|       bert.encoder.layer.0.output.LayerNorm.weight      |    768     |    580.5095   |\n",
      "|        bert.encoder.layer.0.output.LayerNorm.bias       |    768     |    56.8323    |\n",
      "|     bert.encoder.layer.1.attention.self.query.weight    |   589824   |   19746.2422  |\n",
      "|      bert.encoder.layer.1.attention.self.query.bias     |    768     |    86.4637    |\n",
      "|      bert.encoder.layer.1.attention.self.key.weight     |   589824   |   19663.7676  |\n",
      "|       bert.encoder.layer.1.attention.self.key.bias      |    768     |     2.796     |\n",
      "|     bert.encoder.layer.1.attention.self.value.weight    |   589824   |   13176.7109  |\n",
      "|      bert.encoder.layer.1.attention.self.value.bias     |    768     |     18.49     |\n",
      "|    bert.encoder.layer.1.attention.output.dense.weight   |   589824   |   12684.6445  |\n",
      "|     bert.encoder.layer.1.attention.output.dense.bias    |    768     |    35.3109    |\n",
      "|  bert.encoder.layer.1.attention.output.LayerNorm.weight |    768     |    672.5358   |\n",
      "|   bert.encoder.layer.1.attention.output.LayerNorm.bias  |    768     |    95.6849    |\n",
      "|      bert.encoder.layer.1.intermediate.dense.weight     |  2359296   |   73113.7109  |\n",
      "|       bert.encoder.layer.1.intermediate.dense.bias      |    3072    |    312.8192   |\n",
      "|         bert.encoder.layer.1.output.dense.weight        |  2359296   |   69156.8438  |\n",
      "|          bert.encoder.layer.1.output.dense.bias         |    768     |    35.5195    |\n",
      "|       bert.encoder.layer.1.output.LayerNorm.weight      |    768     |    667.8033   |\n",
      "|        bert.encoder.layer.1.output.LayerNorm.bias       |    768     |    54.5318    |\n",
      "|     bert.encoder.layer.2.attention.self.query.weight    |   589824   |   21642.4336  |\n",
      "|      bert.encoder.layer.2.attention.self.query.bias     |    768     |    72.4069    |\n",
      "|      bert.encoder.layer.2.attention.self.key.weight     |   589824   |   21140.6719  |\n",
      "|       bert.encoder.layer.2.attention.self.key.bias      |    768     |     2.3018    |\n",
      "|     bert.encoder.layer.2.attention.self.value.weight    |   589824   |   12923.6562  |\n",
      "|      bert.encoder.layer.2.attention.self.value.bias     |    768     |    24.5372    |\n",
      "|    bert.encoder.layer.2.attention.output.dense.weight   |   589824   |   12398.0244  |\n",
      "|     bert.encoder.layer.2.attention.output.dense.bias    |    768     |    50.9247    |\n",
      "|  bert.encoder.layer.2.attention.output.LayerNorm.weight |    768     |    666.256    |\n",
      "|   bert.encoder.layer.2.attention.output.LayerNorm.bias  |    768     |    95.2361    |\n",
      "|      bert.encoder.layer.2.intermediate.dense.weight     |  2359296   |   74150.125   |\n",
      "|       bert.encoder.layer.2.intermediate.dense.bias      |    3072    |    324.8885   |\n",
      "|         bert.encoder.layer.2.output.dense.weight        |  2359296   |   70013.5156  |\n",
      "|          bert.encoder.layer.2.output.dense.bias         |    768     |    41.9287    |\n",
      "|       bert.encoder.layer.2.output.LayerNorm.weight      |    768     |    653.818    |\n",
      "|        bert.encoder.layer.2.output.LayerNorm.bias       |    768     |    49.4152    |\n",
      "|     bert.encoder.layer.3.attention.self.query.weight    |   589824   |   20016.7793  |\n",
      "|      bert.encoder.layer.3.attention.self.query.bias     |    768     |    69.7128    |\n",
      "|      bert.encoder.layer.3.attention.self.key.weight     |   589824   |   19882.082   |\n",
      "|       bert.encoder.layer.3.attention.self.key.bias      |    768     |     2.7585    |\n",
      "|     bert.encoder.layer.3.attention.self.value.weight    |   589824   |   14413.4844  |\n",
      "|      bert.encoder.layer.3.attention.self.value.bias     |    768     |    16.1165    |\n",
      "|    bert.encoder.layer.3.attention.output.dense.weight   |   589824   |   13343.4863  |\n",
      "|     bert.encoder.layer.3.attention.output.dense.bias    |    768     |    42.7802    |\n",
      "|  bert.encoder.layer.3.attention.output.LayerNorm.weight |    768     |    662.5482   |\n",
      "|   bert.encoder.layer.3.attention.output.LayerNorm.bias  |    768     |    88.1258    |\n",
      "|      bert.encoder.layer.3.intermediate.dense.weight     |  2359296   |   75197.8203  |\n",
      "|       bert.encoder.layer.3.intermediate.dense.bias      |    3072    |    315.4952   |\n",
      "|         bert.encoder.layer.3.output.dense.weight        |  2359296   |   71196.3906  |\n",
      "|          bert.encoder.layer.3.output.dense.bias         |    768     |    41.5464    |\n",
      "|       bert.encoder.layer.3.output.LayerNorm.weight      |    768     |    622.9203   |\n",
      "|        bert.encoder.layer.3.output.LayerNorm.bias       |    768     |     37.824    |\n",
      "|     bert.encoder.layer.4.attention.self.query.weight    |   589824   |   19644.6445  |\n",
      "|      bert.encoder.layer.4.attention.self.query.bias     |    768     |    79.5782    |\n",
      "|      bert.encoder.layer.4.attention.self.key.weight     |   589824   |   19527.416   |\n",
      "|       bert.encoder.layer.4.attention.self.key.bias      |    768     |     2.7925    |\n",
      "|     bert.encoder.layer.4.attention.self.value.weight    |   589824   |   16207.4521  |\n",
      "|      bert.encoder.layer.4.attention.self.value.bias     |    768     |    14.4347    |\n",
      "|    bert.encoder.layer.4.attention.output.dense.weight   |   589824   |   14957.2637  |\n",
      "|     bert.encoder.layer.4.attention.output.dense.bias    |    768     |    22.5332    |\n",
      "|  bert.encoder.layer.4.attention.output.LayerNorm.weight |    768     |    644.097    |\n",
      "|   bert.encoder.layer.4.attention.output.LayerNorm.bias  |    768     |    77.3867    |\n",
      "|      bert.encoder.layer.4.intermediate.dense.weight     |  2359296   |   75656.5156  |\n",
      "|       bert.encoder.layer.4.intermediate.dense.bias      |    3072    |    328.3112   |\n",
      "|         bert.encoder.layer.4.output.dense.weight        |  2359296   |   72158.0078  |\n",
      "|          bert.encoder.layer.4.output.dense.bias         |    768     |    31.6531    |\n",
      "|       bert.encoder.layer.4.output.LayerNorm.weight      |    768     |    644.9095   |\n",
      "|        bert.encoder.layer.4.output.LayerNorm.bias       |    768     |    36.0191    |\n",
      "|     bert.encoder.layer.5.attention.self.query.weight    |   589824   |   20257.7578  |\n",
      "|      bert.encoder.layer.5.attention.self.query.bias     |    768     |    63.6408    |\n",
      "|      bert.encoder.layer.5.attention.self.key.weight     |   589824   |   20217.9199  |\n",
      "|       bert.encoder.layer.5.attention.self.key.bias      |    768     |     3.4998    |\n",
      "|     bert.encoder.layer.5.attention.self.value.weight    |   589824   |   16146.1465  |\n",
      "|      bert.encoder.layer.5.attention.self.value.bias     |    768     |    15.7546    |\n",
      "|    bert.encoder.layer.5.attention.output.dense.weight   |   589824   |   15252.1152  |\n",
      "|     bert.encoder.layer.5.attention.output.dense.bias    |    768     |    20.6033    |\n",
      "|  bert.encoder.layer.5.attention.output.LayerNorm.weight |    768     |    650.1935   |\n",
      "|   bert.encoder.layer.5.attention.output.LayerNorm.bias  |    768     |    74.7787    |\n",
      "|      bert.encoder.layer.5.intermediate.dense.weight     |  2359296   |   75340.9531  |\n",
      "|       bert.encoder.layer.5.intermediate.dense.bias      |    3072    |    322.1071   |\n",
      "|         bert.encoder.layer.5.output.dense.weight        |  2359296   |   71265.4844  |\n",
      "|          bert.encoder.layer.5.output.dense.bias         |    768     |     32.07     |\n",
      "|       bert.encoder.layer.5.output.LayerNorm.weight      |    768     |    639.1559   |\n",
      "|        bert.encoder.layer.5.output.LayerNorm.bias       |    768     |    39.9254    |\n",
      "|     bert.encoder.layer.6.attention.self.query.weight    |   589824   |   20119.7969  |\n",
      "|      bert.encoder.layer.6.attention.self.query.bias     |    768     |    67.2364    |\n",
      "|      bert.encoder.layer.6.attention.self.key.weight     |   589824   |   20083.8809  |\n",
      "|       bert.encoder.layer.6.attention.self.key.bias      |    768     |     3.1425    |\n",
      "|     bert.encoder.layer.6.attention.self.value.weight    |   589824   |   15752.168   |\n",
      "|      bert.encoder.layer.6.attention.self.value.bias     |    768     |    19.1678    |\n",
      "|    bert.encoder.layer.6.attention.output.dense.weight   |   589824   |   14830.0537  |\n",
      "|     bert.encoder.layer.6.attention.output.dense.bias    |    768     |    24.6613    |\n",
      "|  bert.encoder.layer.6.attention.output.LayerNorm.weight |    768     |    635.5405   |\n",
      "|   bert.encoder.layer.6.attention.output.LayerNorm.bias  |    768     |    72.5214    |\n",
      "|      bert.encoder.layer.6.intermediate.dense.weight     |  2359296   |   75829.7734  |\n",
      "|       bert.encoder.layer.6.intermediate.dense.bias      |    3072    |    311.455    |\n",
      "|         bert.encoder.layer.6.output.dense.weight        |  2359296   |   70756.1562  |\n",
      "|          bert.encoder.layer.6.output.dense.bias         |    768     |    40.5704    |\n",
      "|       bert.encoder.layer.6.output.LayerNorm.weight      |    768     |    640.2241   |\n",
      "|        bert.encoder.layer.6.output.LayerNorm.bias       |    768     |    41.0037    |\n",
      "|     bert.encoder.layer.7.attention.self.query.weight    |   589824   |   20290.375   |\n",
      "|      bert.encoder.layer.7.attention.self.query.bias     |    768     |    75.3683    |\n",
      "|      bert.encoder.layer.7.attention.self.key.weight     |   589824   |   20332.9844  |\n",
      "|       bert.encoder.layer.7.attention.self.key.bias      |    768     |     3.1869    |\n",
      "|     bert.encoder.layer.7.attention.self.value.weight    |   589824   |   15308.6338  |\n",
      "|      bert.encoder.layer.7.attention.self.value.bias     |    768     |    22.9576    |\n",
      "|    bert.encoder.layer.7.attention.output.dense.weight   |   589824   |   14669.0664  |\n",
      "|     bert.encoder.layer.7.attention.output.dense.bias    |    768     |    23.8091    |\n",
      "|  bert.encoder.layer.7.attention.output.LayerNorm.weight |    768     |    631.3505   |\n",
      "|   bert.encoder.layer.7.attention.output.LayerNorm.bias  |    768     |    77.4862    |\n",
      "|      bert.encoder.layer.7.intermediate.dense.weight     |  2359296   |   73016.9766  |\n",
      "|       bert.encoder.layer.7.intermediate.dense.bias      |    3072    |    329.6293   |\n",
      "|         bert.encoder.layer.7.output.dense.weight        |  2359296   |   68922.1719  |\n",
      "|          bert.encoder.layer.7.output.dense.bias         |    768     |    46.0541    |\n",
      "|       bert.encoder.layer.7.output.LayerNorm.weight      |    768     |    622.4636   |\n",
      "|        bert.encoder.layer.7.output.LayerNorm.bias       |    768     |    39.6531    |\n",
      "|     bert.encoder.layer.8.attention.self.query.weight    |   589824   |   20605.3867  |\n",
      "|      bert.encoder.layer.8.attention.self.query.bias     |    768     |    96.3513    |\n",
      "|      bert.encoder.layer.8.attention.self.key.weight     |   589824   |   20631.918   |\n",
      "|       bert.encoder.layer.8.attention.self.key.bias      |    768     |     4.0099    |\n",
      "|     bert.encoder.layer.8.attention.self.value.weight    |   589824   |   16522.7012  |\n",
      "|      bert.encoder.layer.8.attention.self.value.bias     |    768     |    20.8687    |\n",
      "|    bert.encoder.layer.8.attention.output.dense.weight   |   589824   |   15587.5254  |\n",
      "|     bert.encoder.layer.8.attention.output.dense.bias    |    768     |    29.0654    |\n",
      "|  bert.encoder.layer.8.attention.output.LayerNorm.weight |    768     |    642.627    |\n",
      "|   bert.encoder.layer.8.attention.output.LayerNorm.bias  |    768     |    72.1585    |\n",
      "|      bert.encoder.layer.8.intermediate.dense.weight     |  2359296   |    73046.25   |\n",
      "|       bert.encoder.layer.8.intermediate.dense.bias      |    3072    |    316.3909   |\n",
      "|         bert.encoder.layer.8.output.dense.weight        |  2359296   |   68910.6641  |\n",
      "|          bert.encoder.layer.8.output.dense.bias         |    768     |     47.362    |\n",
      "|       bert.encoder.layer.8.output.LayerNorm.weight      |    768     |    638.249    |\n",
      "|        bert.encoder.layer.8.output.LayerNorm.bias       |    768     |    41.6155    |\n",
      "|     bert.encoder.layer.9.attention.self.query.weight    |   589824   |   21545.4453  |\n",
      "|      bert.encoder.layer.9.attention.self.query.bias     |    768     |    107.7635   |\n",
      "|      bert.encoder.layer.9.attention.self.key.weight     |   589824   |   21474.584   |\n",
      "|       bert.encoder.layer.9.attention.self.key.bias      |    768     |     3.9468    |\n",
      "|     bert.encoder.layer.9.attention.self.value.weight    |   589824   |   16131.3477  |\n",
      "|      bert.encoder.layer.9.attention.self.value.bias     |    768     |    20.0817    |\n",
      "|    bert.encoder.layer.9.attention.output.dense.weight   |   589824   |   15280.708   |\n",
      "|     bert.encoder.layer.9.attention.output.dense.bias    |    768     |    35.4685    |\n",
      "|  bert.encoder.layer.9.attention.output.LayerNorm.weight |    768     |    627.6024   |\n",
      "|   bert.encoder.layer.9.attention.output.LayerNorm.bias  |    768     |    71.8752    |\n",
      "|      bert.encoder.layer.9.intermediate.dense.weight     |  2359296   |   73705.8125  |\n",
      "|       bert.encoder.layer.9.intermediate.dense.bias      |    3072    |    331.5111   |\n",
      "|         bert.encoder.layer.9.output.dense.weight        |  2359296   |   71358.0938  |\n",
      "|          bert.encoder.layer.9.output.dense.bias         |    768     |    45.3344    |\n",
      "|       bert.encoder.layer.9.output.LayerNorm.weight      |    768     |    614.7891   |\n",
      "|        bert.encoder.layer.9.output.LayerNorm.bias       |    768     |    36.6001    |\n",
      "|    bert.encoder.layer.10.attention.self.query.weight    |   589824   |   21507.7578  |\n",
      "|     bert.encoder.layer.10.attention.self.query.bias     |    768     |    106.2178   |\n",
      "|     bert.encoder.layer.10.attention.self.key.weight     |   589824   |   21413.4492  |\n",
      "|      bert.encoder.layer.10.attention.self.key.bias      |    768     |     3.7733    |\n",
      "|    bert.encoder.layer.10.attention.self.value.weight    |   589824   |   16429.6562  |\n",
      "|     bert.encoder.layer.10.attention.self.value.bias     |    768     |    13.6129    |\n",
      "|   bert.encoder.layer.10.attention.output.dense.weight   |   589824   |   15509.7578  |\n",
      "|    bert.encoder.layer.10.attention.output.dense.bias    |    768     |    32.5096    |\n",
      "| bert.encoder.layer.10.attention.output.LayerNorm.weight |    768     |    645.698    |\n",
      "|  bert.encoder.layer.10.attention.output.LayerNorm.bias  |    768     |    58.0583    |\n",
      "|     bert.encoder.layer.10.intermediate.dense.weight     |  2359296   |   72947.1875  |\n",
      "|      bert.encoder.layer.10.intermediate.dense.bias      |    3072    |    327.2945   |\n",
      "|        bert.encoder.layer.10.output.dense.weight        |  2359296   |   70145.6641  |\n",
      "|         bert.encoder.layer.10.output.dense.bias         |    768     |     57.584    |\n",
      "|      bert.encoder.layer.10.output.LayerNorm.weight      |    768     |    627.6772   |\n",
      "|       bert.encoder.layer.10.output.LayerNorm.bias       |    768     |     38.449    |\n",
      "|    bert.encoder.layer.11.attention.self.query.weight    |   589824   |   21195.918   |\n",
      "|     bert.encoder.layer.11.attention.self.query.bias     |    768     |    126.3037   |\n",
      "|     bert.encoder.layer.11.attention.self.key.weight     |   589824   |   20853.4551  |\n",
      "|      bert.encoder.layer.11.attention.self.key.bias      |    768     |     3.0224    |\n",
      "|    bert.encoder.layer.11.attention.self.value.weight    |   589824   |   18275.2285  |\n",
      "|     bert.encoder.layer.11.attention.self.value.bias     |    768     |    11.3498    |\n",
      "|   bert.encoder.layer.11.attention.output.dense.weight   |   589824   |   17235.9883  |\n",
      "|    bert.encoder.layer.11.attention.output.dense.bias    |    768     |    23.3065    |\n",
      "| bert.encoder.layer.11.attention.output.LayerNorm.weight |    768     |    654.9777   |\n",
      "|  bert.encoder.layer.11.attention.output.LayerNorm.bias  |    768     |    49.5027    |\n",
      "|     bert.encoder.layer.11.intermediate.dense.weight     |  2359296   |   73603.3828  |\n",
      "|      bert.encoder.layer.11.intermediate.dense.bias      |    3072    |    253.6448   |\n",
      "|        bert.encoder.layer.11.output.dense.weight        |  2359296   |   66928.0781  |\n",
      "|         bert.encoder.layer.11.output.dense.bias         |    768     |    33.4035    |\n",
      "|      bert.encoder.layer.11.output.LayerNorm.weight      |    768     |     486.17    |\n",
      "|       bert.encoder.layer.11.output.LayerNorm.bias       |    768     |    32.4309    |\n",
      "|                 bert.pooler.dense.weight                |   589824   |   11262.8877  |\n",
      "|                  bert.pooler.dense.bias                 |    768     |    19.3794    |\n",
      "|                    classifier.weight                    |    4608    |    74.4968    |\n",
      "|                     classifier.bias                     |     6      |      0.0      |\n",
      "|                  Total Trainable Params                 | 109486854  |   3569168.25  |\n",
      "+---------------------------------------------------------+------------+---------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 value: 0.04756803369114388\n",
      "accuracy value: 0.1665689209906267\n",
      "------------------------------ regular bert-base-uncased ------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3ec9c1f735419490bd2c1d1609f7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_214719-v21hkimf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/model1/runs/v21hkimf' target=\"_blank\">bert-base-uncased</a></strong> to <a href='https://wandb.ai/delta_lxr/model1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/model1' target=\"_blank\">https://wandb.ai/delta_lxr/model1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/model1/runs/v21hkimf' target=\"_blank\">https://wandb.ai/delta_lxr/model1/runs/v21hkimf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:359: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-06-09 21:47:34,226]\u001b[0m A new study created in memory with name: no-name-3dfd12f6-d3da-4867-9da1-75eeb7b62d97\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5352c0f0d040599db89267e2812517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert-base-uncased</strong> at: <a href='https://wandb.ai/delta_lxr/model1/runs/v21hkimf' target=\"_blank\">https://wandb.ai/delta_lxr/model1/runs/v21hkimf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_214719-v21hkimf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89de426516664097a9ed0857bb0563b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332902596, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_214745-lro1u60k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/lro1u60k' target=\"_blank\">laced-meadow-83</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/lro1u60k' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/lro1u60k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12149' max='12149' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12149/12149 54:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.002143</td>\n",
       "      <td>0.644742</td>\n",
       "      <td>0.646233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.938153</td>\n",
       "      <td>0.662803</td>\n",
       "      <td>0.666564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.908089</td>\n",
       "      <td>0.677850</td>\n",
       "      <td>0.678674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.884724</td>\n",
       "      <td>0.684254</td>\n",
       "      <td>0.684786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.883700</td>\n",
       "      <td>0.682898</td>\n",
       "      <td>0.685825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.835247</td>\n",
       "      <td>0.701074</td>\n",
       "      <td>0.700939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834478</td>\n",
       "      <td>0.701091</td>\n",
       "      <td>0.703347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.827585</td>\n",
       "      <td>0.704114</td>\n",
       "      <td>0.703892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.808970</td>\n",
       "      <td>0.712631</td>\n",
       "      <td>0.713111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.713769</td>\n",
       "      <td>0.715272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791766</td>\n",
       "      <td>0.717228</td>\n",
       "      <td>0.719048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.792979</td>\n",
       "      <td>0.713805</td>\n",
       "      <td>0.717381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.781715</td>\n",
       "      <td>0.718386</td>\n",
       "      <td>0.720602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.765342</td>\n",
       "      <td>0.724587</td>\n",
       "      <td>0.725767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.766058</td>\n",
       "      <td>0.724087</td>\n",
       "      <td>0.726230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.766271</td>\n",
       "      <td>0.724199</td>\n",
       "      <td>0.725283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.765161</td>\n",
       "      <td>0.726482</td>\n",
       "      <td>0.728144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.748307</td>\n",
       "      <td>0.732263</td>\n",
       "      <td>0.733123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.741381</td>\n",
       "      <td>0.733864</td>\n",
       "      <td>0.734934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.748637</td>\n",
       "      <td>0.731599</td>\n",
       "      <td>0.732506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.735605</td>\n",
       "      <td>0.736675</td>\n",
       "      <td>0.737260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.733628</td>\n",
       "      <td>0.737599</td>\n",
       "      <td>0.738546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.731925</td>\n",
       "      <td>0.737530</td>\n",
       "      <td>0.738813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.731378</td>\n",
       "      <td>0.737194</td>\n",
       "      <td>0.738638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 22:42:40,696]\u001b[0m Trial 0 finished with value: 1.4758326496018417 and parameters: {'learning_rate': 4.2512816728948675e-05, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 1, 'warmup_steps': 344, 'weight_decay': 0.006283907150985664, 'per_device_eval_batch_size': 16}. Best is trial 0 with value: 1.4758326496018417.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13dc5b4b76264eef80b2ffa79133a0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▃▄▄▅▅▅▆▆▇▆▇▇▇▇▇███████</td></tr><tr><td>eval/f1</td><td>▁▂▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇███████</td></tr><tr><td>eval/loss</td><td>█▆▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▄▅▄▆▁▇▄▅▆▃▇▁█▄▆▅▅▇▇▇▃▆▅</td></tr><tr><td>eval/samples_per_second</td><td>▅▅▃▅▃█▂▅▄▃▆▂█▁▅▃▄▄▂▂▂▆▃▄</td></tr><tr><td>eval/steps_per_second</td><td>▅▅▃▅▃█▂▅▄▃▆▂█▁▅▃▄▄▂▂▂▆▃▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.73864</td></tr><tr><td>eval/f1</td><td>0.73719</td></tr><tr><td>eval/loss</td><td>0.73138</td></tr><tr><td>eval/runtime</td><td>99.0215</td></tr><tr><td>eval/samples_per_second</td><td>981.514</td></tr><tr><td>eval/steps_per_second</td><td>61.35</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>12149</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8321</td></tr><tr><td>train/total_flos</td><td>6393186911187072.0</td></tr><tr><td>train/train_loss</td><td>0.83214</td></tr><tr><td>train/train_runtime</td><td>3306.4574</td></tr><tr><td>train/train_samples_per_second</td><td>117.576</td></tr><tr><td>train/train_steps_per_second</td><td>3.674</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-meadow-83</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/lro1u60k' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/lro1u60k</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_214745-lro1u60k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8578807b569e46a4ae29546d312ac066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_224246-nk73cabf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/nk73cabf' target=\"_blank\">laced-sponge-84</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/nk73cabf' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/nk73cabf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2024' max='2024' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2024/2024 16:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.819083</td>\n",
       "      <td>0.731600</td>\n",
       "      <td>0.732311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.849077</td>\n",
       "      <td>0.729343</td>\n",
       "      <td>0.730109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.807408</td>\n",
       "      <td>0.732253</td>\n",
       "      <td>0.732918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.748402</td>\n",
       "      <td>0.735984</td>\n",
       "      <td>0.737260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-09 22:59:49,880]\u001b[0m Trial 1 finished with value: 1.473243629480157 and parameters: {'learning_rate': 2.6888595823894276e-05, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 6, 'warmup_steps': 14, 'weight_decay': 0.006626619115607247, 'per_device_eval_batch_size': 16}. Best is trial 0 with value: 1.4758326496018417.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▃▁▄█</td></tr><tr><td>eval/f1</td><td>▃▁▄█</td></tr><tr><td>eval/loss</td><td>▆█▅▁</td></tr><tr><td>eval/runtime</td><td>█▂▆▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▇▃█</td></tr><tr><td>eval/steps_per_second</td><td>▁▇▃█</td></tr><tr><td>train/epoch</td><td>▁▃▆███</td></tr><tr><td>train/global_step</td><td>▁▃▆███</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.73726</td></tr><tr><td>eval/f1</td><td>0.73598</td></tr><tr><td>eval/loss</td><td>0.7484</td></tr><tr><td>eval/runtime</td><td>99.2493</td></tr><tr><td>eval/samples_per_second</td><td>979.261</td></tr><tr><td>eval/steps_per_second</td><td>61.209</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>2024</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4976</td></tr><tr><td>train/total_flos</td><td>6390670821359616.0</td></tr><tr><td>train/train_loss</td><td>0.49761</td></tr><tr><td>train/train_runtime</td><td>1029.1592</td></tr><tr><td>train/train_samples_per_second</td><td>377.746</td></tr><tr><td>train/train_steps_per_second</td><td>1.967</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-sponge-84</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/nk73cabf' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/nk73cabf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_224246-nk73cabf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec0003b65bb4fba9b563ee9f6cbb18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666414434, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230609_225957-gzyjqywf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/gzyjqywf' target=\"_blank\">misunderstood-cosmos-85</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/gzyjqywf' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/gzyjqywf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24298' max='24298' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24298/24298 1:49:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791558</td>\n",
       "      <td>0.727387</td>\n",
       "      <td>0.729337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858375</td>\n",
       "      <td>0.720278</td>\n",
       "      <td>0.719182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.894733</td>\n",
       "      <td>0.712331</td>\n",
       "      <td>0.710518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841431</td>\n",
       "      <td>0.714847</td>\n",
       "      <td>0.715303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.872561</td>\n",
       "      <td>0.711455</td>\n",
       "      <td>0.712340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.877086</td>\n",
       "      <td>0.716177</td>\n",
       "      <td>0.716692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.856135</td>\n",
       "      <td>0.714930</td>\n",
       "      <td>0.717320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.716680</td>\n",
       "      <td>0.716177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.864885</td>\n",
       "      <td>0.715608</td>\n",
       "      <td>0.715766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.868332</td>\n",
       "      <td>0.710892</td>\n",
       "      <td>0.712340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834744</td>\n",
       "      <td>0.721464</td>\n",
       "      <td>0.722752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833727</td>\n",
       "      <td>0.717883</td>\n",
       "      <td>0.718328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841012</td>\n",
       "      <td>0.718316</td>\n",
       "      <td>0.718883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830261</td>\n",
       "      <td>0.719468</td>\n",
       "      <td>0.720509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858529</td>\n",
       "      <td>0.719729</td>\n",
       "      <td>0.721157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.862406</td>\n",
       "      <td>0.718488</td>\n",
       "      <td>0.717422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.846883</td>\n",
       "      <td>0.719512</td>\n",
       "      <td>0.720056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.824781</td>\n",
       "      <td>0.721255</td>\n",
       "      <td>0.722649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.821730</td>\n",
       "      <td>0.718053</td>\n",
       "      <td>0.719326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830214</td>\n",
       "      <td>0.723861</td>\n",
       "      <td>0.724656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.823604</td>\n",
       "      <td>0.721847</td>\n",
       "      <td>0.724604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.812260</td>\n",
       "      <td>0.726933</td>\n",
       "      <td>0.728318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.801682</td>\n",
       "      <td>0.726051</td>\n",
       "      <td>0.726199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.800667</td>\n",
       "      <td>0.724701</td>\n",
       "      <td>0.727104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.809282</td>\n",
       "      <td>0.722637</td>\n",
       "      <td>0.725335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.794052</td>\n",
       "      <td>0.726331</td>\n",
       "      <td>0.728535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.792964</td>\n",
       "      <td>0.727491</td>\n",
       "      <td>0.727969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.777196</td>\n",
       "      <td>0.731670</td>\n",
       "      <td>0.732290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.783203</td>\n",
       "      <td>0.732363</td>\n",
       "      <td>0.733473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.783164</td>\n",
       "      <td>0.729599</td>\n",
       "      <td>0.731549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.774263</td>\n",
       "      <td>0.732010</td>\n",
       "      <td>0.734039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.774200</td>\n",
       "      <td>0.730882</td>\n",
       "      <td>0.731837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.763421</td>\n",
       "      <td>0.733693</td>\n",
       "      <td>0.734687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.765890</td>\n",
       "      <td>0.734223</td>\n",
       "      <td>0.735305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.772684</td>\n",
       "      <td>0.735678</td>\n",
       "      <td>0.736745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.764352</td>\n",
       "      <td>0.736704</td>\n",
       "      <td>0.737126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.752417</td>\n",
       "      <td>0.737882</td>\n",
       "      <td>0.738535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.740777</td>\n",
       "      <td>0.738563</td>\n",
       "      <td>0.739914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.742363</td>\n",
       "      <td>0.737638</td>\n",
       "      <td>0.738978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.753950</td>\n",
       "      <td>0.736771</td>\n",
       "      <td>0.737805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.742576</td>\n",
       "      <td>0.739617</td>\n",
       "      <td>0.740501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.732328</td>\n",
       "      <td>0.741663</td>\n",
       "      <td>0.742486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.728819</td>\n",
       "      <td>0.742279</td>\n",
       "      <td>0.742785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.729465</td>\n",
       "      <td>0.741920</td>\n",
       "      <td>0.742960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.728214</td>\n",
       "      <td>0.742243</td>\n",
       "      <td>0.743207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.725559</td>\n",
       "      <td>0.742912</td>\n",
       "      <td>0.744122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.724363</td>\n",
       "      <td>0.742471</td>\n",
       "      <td>0.743824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.723466</td>\n",
       "      <td>0.742354</td>\n",
       "      <td>0.743762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-10 00:49:33,466]\u001b[0m Trial 2 finished with value: 1.4861161532429588 and parameters: {'learning_rate': 3.190830237536043e-05, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 1, 'warmup_steps': 240, 'weight_decay': 0.0014452414420046068, 'per_device_eval_batch_size': 16}. Best is trial 2 with value: 1.4861161532429588.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5c1eafb7d24c4aa729426686793c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▅▃▁▂▁▂▂▂▁▄▃▃▃▂▃▃▄▄▅▄▄▅▅▆▆▆▅▆▆▆▇▇▇▇▇█████</td></tr><tr><td>eval/f1</td><td>▅▃▁▂▁▂▂▂▁▃▃▃▃▃▃▃▄▃▅▄▄▄▅▆▆▆▅▆▆▆▇▇▇▇▇█████</td></tr><tr><td>eval/loss</td><td>▄▇█▆▇▆▆▇▇▆▆▅▇▇▆▅▅▅▅▄▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▄▆▂▆▇▄▅▇▂▂▇▄▅▆▇▁▇▄▅▅▆▃▇▄▅▄█▂█▆▅▅▆▄▃▇▅▅▃</td></tr><tr><td>eval/samples_per_second</td><td>▄▅▃▇▃▂▅▄▂▇▇▂▄▄▃▂█▂▅▄▄▃▆▂▅▄▅▁▇▁▃▄▄▃▅▆▂▄▄▆</td></tr><tr><td>eval/steps_per_second</td><td>▄▅▃▇▃▂▅▄▂▇▇▂▄▄▃▂█▂▅▄▄▃▆▂▅▄▅▁▇▁▃▄▄▃▅▆▂▄▄▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.74376</td></tr><tr><td>eval/f1</td><td>0.74235</td></tr><tr><td>eval/loss</td><td>0.72347</td></tr><tr><td>eval/runtime</td><td>98.4426</td></tr><tr><td>eval/samples_per_second</td><td>987.286</td></tr><tr><td>eval/steps_per_second</td><td>61.711</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>24298</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6405</td></tr><tr><td>train/total_flos</td><td>6393186911187072.0</td></tr><tr><td>train/train_loss</td><td>0.64054</td></tr><tr><td>train/train_runtime</td><td>6583.5719</td></tr><tr><td>train/train_samples_per_second</td><td>59.05</td></tr><tr><td>train/train_steps_per_second</td><td>3.691</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misunderstood-cosmos-85</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/gzyjqywf' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/gzyjqywf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_225957-gzyjqywf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c8b1941dcd44b78e78bf298275eceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01718333333362049, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230610_004939-t7y0g8wt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/t7y0g8wt' target=\"_blank\">polished-fire-86</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/t7y0g8wt' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/t7y0g8wt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4049' max='4049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4049/4049 27:11, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.889665</td>\n",
       "      <td>0.737773</td>\n",
       "      <td>0.738258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.928884</td>\n",
       "      <td>0.734948</td>\n",
       "      <td>0.735243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.956085</td>\n",
       "      <td>0.734400</td>\n",
       "      <td>0.734996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.960743</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>0.733082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.949342</td>\n",
       "      <td>0.732524</td>\n",
       "      <td>0.732949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.929264</td>\n",
       "      <td>0.732641</td>\n",
       "      <td>0.732856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.891245</td>\n",
       "      <td>0.733513</td>\n",
       "      <td>0.733936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.863092</td>\n",
       "      <td>0.734274</td>\n",
       "      <td>0.734862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-10 01:17:00,191]\u001b[0m Trial 3 finished with value: 1.4691362907112078 and parameters: {'learning_rate': 2.57245317971525e-06, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 6, 'warmup_steps': 76, 'weight_decay': 0.0077149454026886844, 'per_device_eval_batch_size': 32}. Best is trial 2 with value: 1.4861161532429588.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af8c8b61ba041349d81a0ce38c32b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>█▄▄▁▁▁▂▄</td></tr><tr><td>eval/f1</td><td>█▄▄▁▁▁▂▃</td></tr><tr><td>eval/loss</td><td>▃▆██▇▆▃▁</td></tr><tr><td>eval/runtime</td><td>▂▁█▆▆▅▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▇█▁▃▃▄██</td></tr><tr><td>eval/steps_per_second</td><td>▇█▁▃▃▄██</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.73486</td></tr><tr><td>eval/f1</td><td>0.73427</td></tr><tr><td>eval/loss</td><td>0.86309</td></tr><tr><td>eval/runtime</td><td>51.6278</td></tr><tr><td>eval/samples_per_second</td><td>1882.532</td></tr><tr><td>eval/steps_per_second</td><td>58.844</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>4049</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3267</td></tr><tr><td>train/total_flos</td><td>6392249544388608.0</td></tr><tr><td>train/train_loss</td><td>0.32672</td></tr><tr><td>train/train_runtime</td><td>1646.7005</td></tr><tr><td>train/train_samples_per_second</td><td>236.085</td></tr><tr><td>train/train_steps_per_second</td><td>2.459</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-fire-86</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/t7y0g8wt' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/t7y0g8wt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230610_004939-t7y0g8wt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215dd47924a2431d9769a88b717c3db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230610_011706-gk2mpxi7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/gk2mpxi7' target=\"_blank\">grateful-donkey-87</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/gk2mpxi7' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/gk2mpxi7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4049' max='4049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4049/4049 24:41, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.011748</td>\n",
       "      <td>0.730944</td>\n",
       "      <td>0.730757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.032112</td>\n",
       "      <td>0.727625</td>\n",
       "      <td>0.727547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.066266</td>\n",
       "      <td>0.728502</td>\n",
       "      <td>0.729131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.035790</td>\n",
       "      <td>0.728037</td>\n",
       "      <td>0.728483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.988677</td>\n",
       "      <td>0.730336</td>\n",
       "      <td>0.730819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.939157</td>\n",
       "      <td>0.729653</td>\n",
       "      <td>0.729707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858958</td>\n",
       "      <td>0.731917</td>\n",
       "      <td>0.732475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.810890</td>\n",
       "      <td>0.733184</td>\n",
       "      <td>0.734008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-10 01:41:57,055]\u001b[0m Trial 4 finished with value: 1.467192258080892 and parameters: {'learning_rate': 6.381587463469347e-06, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 32, 'gradient_accumulation_steps': 3, 'warmup_steps': 204, 'weight_decay': 0.008165108889678311, 'per_device_eval_batch_size': 16}. Best is trial 2 with value: 1.4861161532429588.\u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95e5ae258c048a0a6bf6930a287eb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▄▁▃▂▅▃▆█</td></tr><tr><td>eval/f1</td><td>▅▁▂▂▄▄▆█</td></tr><tr><td>eval/loss</td><td>▇▇█▇▆▅▂▁</td></tr><tr><td>eval/runtime</td><td>▅▄▃▁▄▅█▆</td></tr><tr><td>eval/samples_per_second</td><td>▄▅▆█▅▄▁▃</td></tr><tr><td>eval/steps_per_second</td><td>▄▅▆█▅▄▁▃</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.73401</td></tr><tr><td>eval/f1</td><td>0.73318</td></tr><tr><td>eval/loss</td><td>0.81089</td></tr><tr><td>eval/runtime</td><td>101.1945</td></tr><tr><td>eval/samples_per_second</td><td>960.438</td></tr><tr><td>eval/steps_per_second</td><td>60.033</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>4049</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3341</td></tr><tr><td>train/total_flos</td><td>6392249544388608.0</td></tr><tr><td>train/train_loss</td><td>0.33412</td></tr><tr><td>train/train_runtime</td><td>1496.8407</td></tr><tr><td>train/train_samples_per_second</td><td>259.721</td></tr><tr><td>train/train_steps_per_second</td><td>2.705</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-donkey-87</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/gk2mpxi7' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/gk2mpxi7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230610_011706-gk2mpxi7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264e0465b3af486897e5a13c4cf4bb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666414434, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230610_014204-qxea4ig2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/qxea4ig2' target=\"_blank\">dauntless-rain-88</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/qxea4ig2' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/qxea4ig2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='8099' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 500/8099 03:01 < 46:14, 2.74 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.252930</td>\n",
       "      <td>0.718447</td>\n",
       "      <td>0.718709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-10 01:45:15,454]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb4f3295c0a4231a2634bfed2342260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.71871</td></tr><tr><td>eval/f1</td><td>0.71845</td></tr><tr><td>eval/loss</td><td>1.25293</td></tr><tr><td>eval/runtime</td><td>98.5007</td></tr><tr><td>eval/samples_per_second</td><td>986.704</td></tr><tr><td>eval/steps_per_second</td><td>61.675</td></tr><tr><td>train/epoch</td><td>0.06</td></tr><tr><td>train/global_step</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dauntless-rain-88</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/qxea4ig2' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/qxea4ig2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230610_014204-qxea4ig2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a56f95d666483ba40d5eaf5e1e4fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230610_014520-clgvahr0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/clgvahr0' target=\"_blank\">lyric-thunder-89</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/clgvahr0' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/clgvahr0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='8099' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 500/8099 03:01 < 46:09, 2.74 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.518942</td>\n",
       "      <td>0.714587</td>\n",
       "      <td>0.713595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-10 01:48:30,658]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae7d39276b6433387f729efe50b59be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.71359</td></tr><tr><td>eval/f1</td><td>0.71459</td></tr><tr><td>eval/loss</td><td>1.51894</td></tr><tr><td>eval/runtime</td><td>98.6977</td></tr><tr><td>eval/samples_per_second</td><td>984.734</td></tr><tr><td>eval/steps_per_second</td><td>61.552</td></tr><tr><td>train/epoch</td><td>0.06</td></tr><tr><td>train/global_step</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-thunder-89</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/clgvahr0' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/clgvahr0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230610_014520-clgvahr0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae62d41c68be47c4ae302e9b70e72631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230610_014836-fagffxhx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/fagffxhx' target=\"_blank\">avid-fire-90</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/fagffxhx' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/fagffxhx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='8099' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 500/8099 02:13 < 33:58, 3.73 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.548297</td>\n",
       "      <td>0.716773</td>\n",
       "      <td>0.716311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-10 01:50:58,656]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2d7d3b5289472483c505c1c63981cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.71631</td></tr><tr><td>eval/f1</td><td>0.71677</td></tr><tr><td>eval/loss</td><td>1.5483</td></tr><tr><td>eval/runtime</td><td>51.8206</td></tr><tr><td>eval/samples_per_second</td><td>1875.53</td></tr><tr><td>eval/steps_per_second</td><td>58.625</td></tr><tr><td>train/epoch</td><td>0.06</td></tr><tr><td>train/global_step</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">avid-fire-90</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/fagffxhx' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/fagffxhx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230610_014836-fagffxhx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702e4915cda74e3c973a687636f88908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230610_015104-iuz0xjx2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/iuz0xjx2' target=\"_blank\">smart-field-91</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/iuz0xjx2' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/iuz0xjx2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='24298' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  500/24298 02:17 < 1:49:07, 3.63 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.097112</td>\n",
       "      <td>0.706798</td>\n",
       "      <td>0.707720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-10 01:53:30,386]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc85670b722642639efbd80cb445a092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.70772</td></tr><tr><td>eval/f1</td><td>0.7068</td></tr><tr><td>eval/loss</td><td>2.09711</td></tr><tr><td>eval/runtime</td><td>100.0294</td></tr><tr><td>eval/samples_per_second</td><td>971.624</td></tr><tr><td>eval/steps_per_second</td><td>60.732</td></tr><tr><td>train/epoch</td><td>0.02</td></tr><tr><td>train/global_step</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-field-91</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/iuz0xjx2' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/iuz0xjx2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230610_015104-iuz0xjx2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649b35f30b484021b4ad4a32ea381188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\wandb\\run-20230610_015337-06hb3h6x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/06hb3h6x' target=\"_blank\">autumn-valley-92</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/06hb3h6x' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/06hb3h6x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='6074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 500/6074 02:39 < 29:43, 3.12 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.893293</td>\n",
       "      <td>0.712568</td>\n",
       "      <td>0.712298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-10 01:56:25,360]\u001b[0m Trial 9 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 3.190830237536043e-05, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 1, 'warmup_steps': 240, 'weight_decay': 0.0014452414420046068, 'per_device_eval_batch_size': 16}\n",
      "{'output_dir': WindowsPath('C:/Users/liyag/OneDrive - mail.tau.ac.il/Desktop/NLP/results'), 'overwrite_output_dir': True, 'greater_is_better': True, 'evaluation_strategy': 'steps', 'do_train': True, 'logging_strategy': 'epoch', 'save_strategy': 'epoch', 'report_to': 'wandb', 'learning_rate': 3.190830237536043e-05, 'num_train_epochs': 1, 'seed': 0, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 1, 'warmup_steps': 240, 'weight_decay': 0.0014452414420046068, 'per_device_eval_batch_size': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24298' max='24298' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24298/24298 1:48:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.146050</td>\n",
       "      <td>0.712637</td>\n",
       "      <td>0.714336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.507567</td>\n",
       "      <td>0.707754</td>\n",
       "      <td>0.707267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.476681</td>\n",
       "      <td>0.707443</td>\n",
       "      <td>0.709027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.485119</td>\n",
       "      <td>0.697412</td>\n",
       "      <td>0.698377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.436344</td>\n",
       "      <td>0.702620</td>\n",
       "      <td>0.704942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.480121</td>\n",
       "      <td>0.712002</td>\n",
       "      <td>0.711949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.337037</td>\n",
       "      <td>0.699879</td>\n",
       "      <td>0.698871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.428410</td>\n",
       "      <td>0.702242</td>\n",
       "      <td>0.701372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.372612</td>\n",
       "      <td>0.705264</td>\n",
       "      <td>0.705281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.395945</td>\n",
       "      <td>0.702004</td>\n",
       "      <td>0.702503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.297437</td>\n",
       "      <td>0.708662</td>\n",
       "      <td>0.709140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.414074</td>\n",
       "      <td>0.704770</td>\n",
       "      <td>0.705590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.327062</td>\n",
       "      <td>0.705132</td>\n",
       "      <td>0.706629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.318702</td>\n",
       "      <td>0.708549</td>\n",
       "      <td>0.710385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.376531</td>\n",
       "      <td>0.706557</td>\n",
       "      <td>0.706434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.272346</td>\n",
       "      <td>0.708514</td>\n",
       "      <td>0.707638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.357180</td>\n",
       "      <td>0.709114</td>\n",
       "      <td>0.708718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.372019</td>\n",
       "      <td>0.712840</td>\n",
       "      <td>0.712689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.301918</td>\n",
       "      <td>0.706621</td>\n",
       "      <td>0.707113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.289521</td>\n",
       "      <td>0.709397</td>\n",
       "      <td>0.709664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.253449</td>\n",
       "      <td>0.707098</td>\n",
       "      <td>0.707185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.221772</td>\n",
       "      <td>0.714872</td>\n",
       "      <td>0.715735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.202462</td>\n",
       "      <td>0.712137</td>\n",
       "      <td>0.713543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.197380</td>\n",
       "      <td>0.709964</td>\n",
       "      <td>0.710663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.192951</td>\n",
       "      <td>0.714248</td>\n",
       "      <td>0.714943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.149326</td>\n",
       "      <td>0.713655</td>\n",
       "      <td>0.714871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.185136</td>\n",
       "      <td>0.711315</td>\n",
       "      <td>0.711949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.064838</td>\n",
       "      <td>0.717790</td>\n",
       "      <td>0.719058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.156451</td>\n",
       "      <td>0.715149</td>\n",
       "      <td>0.715714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.069976</td>\n",
       "      <td>0.714155</td>\n",
       "      <td>0.715601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.050530</td>\n",
       "      <td>0.719261</td>\n",
       "      <td>0.720653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.026040</td>\n",
       "      <td>0.715345</td>\n",
       "      <td>0.715797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>0.717930</td>\n",
       "      <td>0.718667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.022920</td>\n",
       "      <td>0.720610</td>\n",
       "      <td>0.721044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.012746</td>\n",
       "      <td>0.718219</td>\n",
       "      <td>0.718606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.032577</td>\n",
       "      <td>0.718826</td>\n",
       "      <td>0.718606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.955583</td>\n",
       "      <td>0.723573</td>\n",
       "      <td>0.723524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.904236</td>\n",
       "      <td>0.723557</td>\n",
       "      <td>0.725016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.888013</td>\n",
       "      <td>0.723380</td>\n",
       "      <td>0.724738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.894029</td>\n",
       "      <td>0.722778</td>\n",
       "      <td>0.723771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.862863</td>\n",
       "      <td>0.727087</td>\n",
       "      <td>0.727845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.823856</td>\n",
       "      <td>0.728265</td>\n",
       "      <td>0.728946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.813230</td>\n",
       "      <td>0.728873</td>\n",
       "      <td>0.729203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.806150</td>\n",
       "      <td>0.730874</td>\n",
       "      <td>0.731611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791107</td>\n",
       "      <td>0.732206</td>\n",
       "      <td>0.733082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.780211</td>\n",
       "      <td>0.732833</td>\n",
       "      <td>0.733947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.771831</td>\n",
       "      <td>0.733074</td>\n",
       "      <td>0.734317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.765832</td>\n",
       "      <td>0.733442</td>\n",
       "      <td>0.734749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model was saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▄▄▃▃▁▄▁▂▂▃▂▃▃▃▃▄▃▃▄▄▃▄▄▅▄▄▄▅▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>eval/f1</td><td>▄▄▃▃▁▄▁▂▃▃▂▂▃▃▃▄▃▃▄▄▃▄▄▅▄▄▄▅▆▅▅▆▆▆▇▇████</td></tr><tr><td>eval/loss</td><td>▆▇██▄▄▃▄▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▆▄▇▂▃▅▅▃▁▇▁▇▄▆▁▇▂▄▅▆▃▂▇▃▇▄▇▂▇▃▅▅█▂▂█▆▆▃</td></tr><tr><td>eval/samples_per_second</td><td>▅▃▅▂▇▆▄▄▆█▂█▂▅▃█▂▇▅▄▃▆▇▂▆▂▅▂▇▂▆▄▄▁▇▇▁▃▃▆</td></tr><tr><td>eval/steps_per_second</td><td>▅▃▅▂▇▆▄▄▆█▂█▂▅▃█▂▇▅▄▃▆▇▂▆▂▅▂▇▂▆▄▄▁▇▇▁▃▃▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.73475</td></tr><tr><td>eval/f1</td><td>0.73344</td></tr><tr><td>eval/loss</td><td>0.76583</td></tr><tr><td>eval/runtime</td><td>98.3466</td></tr><tr><td>eval/samples_per_second</td><td>988.249</td></tr><tr><td>eval/steps_per_second</td><td>61.771</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>24298</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3683</td></tr><tr><td>train/total_flos</td><td>6393186911187072.0</td></tr><tr><td>train/train_loss</td><td>0.36831</td></tr><tr><td>train/train_runtime</td><td>6535.7289</td></tr><tr><td>train/train_samples_per_second</td><td>59.482</td></tr><tr><td>train/train_steps_per_second</td><td>3.718</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autumn-valley-92</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/06hb3h6x' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/06hb3h6x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230610_015337-06hb3h6x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+------------+---------------+\n",
      "|                         Modules                         | Parameters | Sum of Tensor |\n",
      "+---------------------------------------------------------+------------+---------------+\n",
      "|          bert.embeddings.word_embeddings.weight         |  23440896  |  955872.1875  |\n",
      "|        bert.embeddings.position_embeddings.weight       |   393216   |    4577.082   |\n",
      "|       bert.embeddings.token_type_embeddings.weight      |    1536    |    13.9256    |\n",
      "|             bert.embeddings.LayerNorm.weight            |    768     |    652.7019   |\n",
      "|              bert.embeddings.LayerNorm.bias             |    768     |    31.7363    |\n",
      "|     bert.encoder.layer.0.attention.self.query.weight    |   589824   |   20028.5293  |\n",
      "|      bert.encoder.layer.0.attention.self.query.bias     |    768     |    167.1584   |\n",
      "|      bert.encoder.layer.0.attention.self.key.weight     |   589824   |   19688.5469  |\n",
      "|       bert.encoder.layer.0.attention.self.key.bias      |    768     |     1.8726    |\n",
      "|     bert.encoder.layer.0.attention.self.value.weight    |   589824   |   13483.459   |\n",
      "|      bert.encoder.layer.0.attention.self.value.bias     |    768     |    19.8902    |\n",
      "|    bert.encoder.layer.0.attention.output.dense.weight   |   589824   |   13041.502   |\n",
      "|     bert.encoder.layer.0.attention.output.dense.bias    |    768     |    19.1094    |\n",
      "|  bert.encoder.layer.0.attention.output.LayerNorm.weight |    768     |    736.7421   |\n",
      "|   bert.encoder.layer.0.attention.output.LayerNorm.bias  |    768     |    153.808    |\n",
      "|      bert.encoder.layer.0.intermediate.dense.weight     |  2359296   |   70574.1094  |\n",
      "|       bert.encoder.layer.0.intermediate.dense.bias      |    3072    |    347.1645   |\n",
      "|         bert.encoder.layer.0.output.dense.weight        |  2359296   |   66433.7188  |\n",
      "|          bert.encoder.layer.0.output.dense.bias         |    768     |    44.2128    |\n",
      "|       bert.encoder.layer.0.output.LayerNorm.weight      |    768     |    581.1021   |\n",
      "|        bert.encoder.layer.0.output.LayerNorm.bias       |    768     |    56.6882    |\n",
      "|     bert.encoder.layer.1.attention.self.query.weight    |   589824   |   19865.168   |\n",
      "|      bert.encoder.layer.1.attention.self.query.bias     |    768     |    86.6469    |\n",
      "|      bert.encoder.layer.1.attention.self.key.weight     |   589824   |   19779.4844  |\n",
      "|       bert.encoder.layer.1.attention.self.key.bias      |    768     |     2.7955    |\n",
      "|     bert.encoder.layer.1.attention.self.value.weight    |   589824   |   13284.4414  |\n",
      "|      bert.encoder.layer.1.attention.self.value.bias     |    768     |    18.3231    |\n",
      "|    bert.encoder.layer.1.attention.output.dense.weight   |   589824   |   12785.3408  |\n",
      "|     bert.encoder.layer.1.attention.output.dense.bias    |    768     |    35.0339    |\n",
      "|  bert.encoder.layer.1.attention.output.LayerNorm.weight |    768     |    674.0969   |\n",
      "|   bert.encoder.layer.1.attention.output.LayerNorm.bias  |    768     |    95.9663    |\n",
      "|      bert.encoder.layer.1.intermediate.dense.weight     |  2359296   |   73821.6875  |\n",
      "|       bert.encoder.layer.1.intermediate.dense.bias      |    3072    |    312.738    |\n",
      "|         bert.encoder.layer.1.output.dense.weight        |  2359296   |   69723.8438  |\n",
      "|          bert.encoder.layer.1.output.dense.bias         |    768     |    35.4701    |\n",
      "|       bert.encoder.layer.1.output.LayerNorm.weight      |    768     |    668.4733   |\n",
      "|        bert.encoder.layer.1.output.LayerNorm.bias       |    768     |    54.3538    |\n",
      "|     bert.encoder.layer.2.attention.self.query.weight    |   589824   |   21745.498   |\n",
      "|      bert.encoder.layer.2.attention.self.query.bias     |    768     |    72.3464    |\n",
      "|      bert.encoder.layer.2.attention.self.key.weight     |   589824   |   21244.5898  |\n",
      "|       bert.encoder.layer.2.attention.self.key.bias      |    768     |     2.3018    |\n",
      "|     bert.encoder.layer.2.attention.self.value.weight    |   589824   |   13073.9082  |\n",
      "|      bert.encoder.layer.2.attention.self.value.bias     |    768     |    24.4538    |\n",
      "|    bert.encoder.layer.2.attention.output.dense.weight   |   589824   |   12550.2734  |\n",
      "|     bert.encoder.layer.2.attention.output.dense.bias    |    768     |    50.5331    |\n",
      "|  bert.encoder.layer.2.attention.output.LayerNorm.weight |    768     |    667.9754   |\n",
      "|   bert.encoder.layer.2.attention.output.LayerNorm.bias  |    768     |    95.7135    |\n",
      "|      bert.encoder.layer.2.intermediate.dense.weight     |  2359296   |   74872.5156  |\n",
      "|       bert.encoder.layer.2.intermediate.dense.bias      |    3072    |    325.0009   |\n",
      "|         bert.encoder.layer.2.output.dense.weight        |  2359296   |   70590.1016  |\n",
      "|          bert.encoder.layer.2.output.dense.bias         |    768     |    41.7743    |\n",
      "|       bert.encoder.layer.2.output.LayerNorm.weight      |    768     |    654.642    |\n",
      "|        bert.encoder.layer.2.output.LayerNorm.bias       |    768     |    49.2222    |\n",
      "|     bert.encoder.layer.3.attention.self.query.weight    |   589824   |   20137.668   |\n",
      "|      bert.encoder.layer.3.attention.self.query.bias     |    768     |    69.8848    |\n",
      "|      bert.encoder.layer.3.attention.self.key.weight     |   589824   |   20002.4551  |\n",
      "|       bert.encoder.layer.3.attention.self.key.bias      |    768     |     2.7587    |\n",
      "|     bert.encoder.layer.3.attention.self.value.weight    |   589824   |   14548.2344  |\n",
      "|      bert.encoder.layer.3.attention.self.value.bias     |    768     |    16.1418    |\n",
      "|    bert.encoder.layer.3.attention.output.dense.weight   |   589824   |   13480.0713  |\n",
      "|     bert.encoder.layer.3.attention.output.dense.bias    |    768     |    42.4292    |\n",
      "|  bert.encoder.layer.3.attention.output.LayerNorm.weight |    768     |    664.456    |\n",
      "|   bert.encoder.layer.3.attention.output.LayerNorm.bias  |    768     |    88.5838    |\n",
      "|      bert.encoder.layer.3.intermediate.dense.weight     |  2359296   |   75918.4375  |\n",
      "|       bert.encoder.layer.3.intermediate.dense.bias      |    3072    |    315.8936   |\n",
      "|         bert.encoder.layer.3.output.dense.weight        |  2359296   |   71775.4766  |\n",
      "|          bert.encoder.layer.3.output.dense.bias         |    768     |    41.4417    |\n",
      "|       bert.encoder.layer.3.output.LayerNorm.weight      |    768     |    623.7876   |\n",
      "|        bert.encoder.layer.3.output.LayerNorm.bias       |    768     |    37.5538    |\n",
      "|     bert.encoder.layer.4.attention.self.query.weight    |   589824   |   19775.7402  |\n",
      "|      bert.encoder.layer.4.attention.self.query.bias     |    768     |     79.606    |\n",
      "|      bert.encoder.layer.4.attention.self.key.weight     |   589824   |   19661.0352  |\n",
      "|       bert.encoder.layer.4.attention.self.key.bias      |    768     |     2.7924    |\n",
      "|     bert.encoder.layer.4.attention.self.value.weight    |   589824   |   16330.3994  |\n",
      "|      bert.encoder.layer.4.attention.self.value.bias     |    768     |    14.3952    |\n",
      "|    bert.encoder.layer.4.attention.output.dense.weight   |   589824   |   15085.4043  |\n",
      "|     bert.encoder.layer.4.attention.output.dense.bias    |    768     |    22.2512    |\n",
      "|  bert.encoder.layer.4.attention.output.LayerNorm.weight |    768     |    646.472    |\n",
      "|   bert.encoder.layer.4.attention.output.LayerNorm.bias  |    768     |    77.7614    |\n",
      "|      bert.encoder.layer.4.intermediate.dense.weight     |  2359296   |   76398.9062  |\n",
      "|       bert.encoder.layer.4.intermediate.dense.bias      |    3072    |    328.5191   |\n",
      "|         bert.encoder.layer.4.output.dense.weight        |  2359296   |   72692.6406  |\n",
      "|          bert.encoder.layer.4.output.dense.bias         |    768     |    31.5857    |\n",
      "|       bert.encoder.layer.4.output.LayerNorm.weight      |    768     |    645.8607   |\n",
      "|        bert.encoder.layer.4.output.LayerNorm.bias       |    768     |    35.7282    |\n",
      "|     bert.encoder.layer.5.attention.self.query.weight    |   589824   |   20381.0879  |\n",
      "|      bert.encoder.layer.5.attention.self.query.bias     |    768     |    63.5214    |\n",
      "|      bert.encoder.layer.5.attention.self.key.weight     |   589824   |   20346.1836  |\n",
      "|       bert.encoder.layer.5.attention.self.key.bias      |    768     |     3.4997    |\n",
      "|     bert.encoder.layer.5.attention.self.value.weight    |   589824   |   16283.8984  |\n",
      "|      bert.encoder.layer.5.attention.self.value.bias     |    768     |    15.7276    |\n",
      "|    bert.encoder.layer.5.attention.output.dense.weight   |   589824   |   15383.6055  |\n",
      "|     bert.encoder.layer.5.attention.output.dense.bias    |    768     |    20.2823    |\n",
      "|  bert.encoder.layer.5.attention.output.LayerNorm.weight |    768     |    652.7899   |\n",
      "|   bert.encoder.layer.5.attention.output.LayerNorm.bias  |    768     |    75.2953    |\n",
      "|      bert.encoder.layer.5.intermediate.dense.weight     |  2359296   |   76113.8125  |\n",
      "|       bert.encoder.layer.5.intermediate.dense.bias      |    3072    |    322.708    |\n",
      "|         bert.encoder.layer.5.output.dense.weight        |  2359296   |   71797.2656  |\n",
      "|          bert.encoder.layer.5.output.dense.bias         |    768     |    31.9916    |\n",
      "|       bert.encoder.layer.5.output.LayerNorm.weight      |    768     |    640.3918   |\n",
      "|        bert.encoder.layer.5.output.LayerNorm.bias       |    768     |    39.6545    |\n",
      "|     bert.encoder.layer.6.attention.self.query.weight    |   589824   |   20259.0156  |\n",
      "|      bert.encoder.layer.6.attention.self.query.bias     |    768     |    67.0877    |\n",
      "|      bert.encoder.layer.6.attention.self.key.weight     |   589824   |   20221.4297  |\n",
      "|       bert.encoder.layer.6.attention.self.key.bias      |    768     |     3.1424    |\n",
      "|     bert.encoder.layer.6.attention.self.value.weight    |   589824   |   15906.0625  |\n",
      "|      bert.encoder.layer.6.attention.self.value.bias     |    768     |    19.1875    |\n",
      "|    bert.encoder.layer.6.attention.output.dense.weight   |   589824   |   14973.3818  |\n",
      "|     bert.encoder.layer.6.attention.output.dense.bias    |    768     |    24.2693    |\n",
      "|  bert.encoder.layer.6.attention.output.LayerNorm.weight |    768     |    638.5289   |\n",
      "|   bert.encoder.layer.6.attention.output.LayerNorm.bias  |    768     |    73.0932    |\n",
      "|      bert.encoder.layer.6.intermediate.dense.weight     |  2359296   |   76620.875   |\n",
      "|       bert.encoder.layer.6.intermediate.dense.bias      |    3072    |    312.2703   |\n",
      "|         bert.encoder.layer.6.output.dense.weight        |  2359296   |   71295.5938  |\n",
      "|          bert.encoder.layer.6.output.dense.bias         |    768     |    40.4945    |\n",
      "|       bert.encoder.layer.6.output.LayerNorm.weight      |    768     |    641.578    |\n",
      "|        bert.encoder.layer.6.output.LayerNorm.bias       |    768     |    40.7075    |\n",
      "|     bert.encoder.layer.7.attention.self.query.weight    |   589824   |   20434.5117  |\n",
      "|      bert.encoder.layer.7.attention.self.query.bias     |    768     |    75.4389    |\n",
      "|      bert.encoder.layer.7.attention.self.key.weight     |   589824   |   20480.7188  |\n",
      "|       bert.encoder.layer.7.attention.self.key.bias      |    768     |     3.187     |\n",
      "|     bert.encoder.layer.7.attention.self.value.weight    |   589824   |   15458.998   |\n",
      "|      bert.encoder.layer.7.attention.self.value.bias     |    768     |    22.9986    |\n",
      "|    bert.encoder.layer.7.attention.output.dense.weight   |   589824   |   14816.877   |\n",
      "|     bert.encoder.layer.7.attention.output.dense.bias    |    768     |    23.4668    |\n",
      "|  bert.encoder.layer.7.attention.output.LayerNorm.weight |    768     |    634.6394   |\n",
      "|   bert.encoder.layer.7.attention.output.LayerNorm.bias  |    768     |    78.1085    |\n",
      "|      bert.encoder.layer.7.intermediate.dense.weight     |  2359296   |   73928.8516  |\n",
      "|       bert.encoder.layer.7.intermediate.dense.bias      |    3072    |    331.5463   |\n",
      "|         bert.encoder.layer.7.output.dense.weight        |  2359296   |   69434.2891  |\n",
      "|          bert.encoder.layer.7.output.dense.bias         |    768     |    46.0416    |\n",
      "|       bert.encoder.layer.7.output.LayerNorm.weight      |    768     |    623.5295   |\n",
      "|        bert.encoder.layer.7.output.LayerNorm.bias       |    768     |    39.3849    |\n",
      "|     bert.encoder.layer.8.attention.self.query.weight    |   589824   |   20727.0605  |\n",
      "|      bert.encoder.layer.8.attention.self.query.bias     |    768     |    96.3741    |\n",
      "|      bert.encoder.layer.8.attention.self.key.weight     |   589824   |   20760.668   |\n",
      "|       bert.encoder.layer.8.attention.self.key.bias      |    768     |     4.0098    |\n",
      "|     bert.encoder.layer.8.attention.self.value.weight    |   589824   |   16632.2598  |\n",
      "|      bert.encoder.layer.8.attention.self.value.bias     |    768     |    21.0117    |\n",
      "|    bert.encoder.layer.8.attention.output.dense.weight   |   589824   |   15689.9092  |\n",
      "|     bert.encoder.layer.8.attention.output.dense.bias    |    768     |    28.4987    |\n",
      "|  bert.encoder.layer.8.attention.output.LayerNorm.weight |    768     |    645.7555   |\n",
      "|   bert.encoder.layer.8.attention.output.LayerNorm.bias  |    768     |    72.9835    |\n",
      "|      bert.encoder.layer.8.intermediate.dense.weight     |  2359296   |   73969.9688  |\n",
      "|       bert.encoder.layer.8.intermediate.dense.bias      |    3072    |    319.9904   |\n",
      "|         bert.encoder.layer.8.output.dense.weight        |  2359296   |   69333.2969  |\n",
      "|          bert.encoder.layer.8.output.dense.bias         |    768     |    47.3477    |\n",
      "|       bert.encoder.layer.8.output.LayerNorm.weight      |    768     |    638.6874   |\n",
      "|        bert.encoder.layer.8.output.LayerNorm.bias       |    768     |    41.5431    |\n",
      "|     bert.encoder.layer.9.attention.self.query.weight    |   589824   |   21632.6133  |\n",
      "|      bert.encoder.layer.9.attention.self.query.bias     |    768     |    108.3274   |\n",
      "|      bert.encoder.layer.9.attention.self.key.weight     |   589824   |   21571.6133  |\n",
      "|       bert.encoder.layer.9.attention.self.key.bias      |    768     |     3.9467    |\n",
      "|     bert.encoder.layer.9.attention.self.value.weight    |   589824   |   16224.5713  |\n",
      "|      bert.encoder.layer.9.attention.self.value.bias     |    768     |    20.1136    |\n",
      "|    bert.encoder.layer.9.attention.output.dense.weight   |   589824   |   15372.8428  |\n",
      "|     bert.encoder.layer.9.attention.output.dense.bias    |    768     |    35.0712    |\n",
      "|  bert.encoder.layer.9.attention.output.LayerNorm.weight |    768     |    630.1633   |\n",
      "|   bert.encoder.layer.9.attention.output.LayerNorm.bias  |    768     |    72.8558    |\n",
      "|      bert.encoder.layer.9.intermediate.dense.weight     |  2359296   |   74594.3281  |\n",
      "|       bert.encoder.layer.9.intermediate.dense.bias      |    3072    |    335.3336   |\n",
      "|         bert.encoder.layer.9.output.dense.weight        |  2359296   |   71697.6719  |\n",
      "|          bert.encoder.layer.9.output.dense.bias         |    768     |    45.3287    |\n",
      "|       bert.encoder.layer.9.output.LayerNorm.weight      |    768     |    614.9038   |\n",
      "|        bert.encoder.layer.9.output.LayerNorm.bias       |    768     |     36.441    |\n",
      "|    bert.encoder.layer.10.attention.self.query.weight    |   589824   |   21587.8828  |\n",
      "|     bert.encoder.layer.10.attention.self.query.bias     |    768     |    106.401    |\n",
      "|     bert.encoder.layer.10.attention.self.key.weight     |   589824   |   21496.9102  |\n",
      "|      bert.encoder.layer.10.attention.self.key.bias      |    768     |     3.7733    |\n",
      "|    bert.encoder.layer.10.attention.self.value.weight    |   589824   |   16522.5156  |\n",
      "|     bert.encoder.layer.10.attention.self.value.bias     |    768     |    13.7612    |\n",
      "|   bert.encoder.layer.10.attention.output.dense.weight   |   589824   |   15596.6279  |\n",
      "|    bert.encoder.layer.10.attention.output.dense.bias    |    768     |    32.2508    |\n",
      "| bert.encoder.layer.10.attention.output.LayerNorm.weight |    768     |    647.7273   |\n",
      "|  bert.encoder.layer.10.attention.output.LayerNorm.bias  |    768     |    59.2629    |\n",
      "|     bert.encoder.layer.10.intermediate.dense.weight     |  2359296   |   73744.0469  |\n",
      "|      bert.encoder.layer.10.intermediate.dense.bias      |    3072    |    332.1095   |\n",
      "|        bert.encoder.layer.10.output.dense.weight        |  2359296   |   70426.7969  |\n",
      "|         bert.encoder.layer.10.output.dense.bias         |    768     |    57.3588    |\n",
      "|      bert.encoder.layer.10.output.LayerNorm.weight      |    768     |    627.3969   |\n",
      "|       bert.encoder.layer.10.output.LayerNorm.bias       |    768     |    38.2564    |\n",
      "|    bert.encoder.layer.11.attention.self.query.weight    |   589824   |   21273.6055  |\n",
      "|     bert.encoder.layer.11.attention.self.query.bias     |    768     |    126.0014   |\n",
      "|     bert.encoder.layer.11.attention.self.key.weight     |   589824   |   20931.7188  |\n",
      "|      bert.encoder.layer.11.attention.self.key.bias      |    768     |     3.0224    |\n",
      "|    bert.encoder.layer.11.attention.self.value.weight    |   589824   |   18334.918   |\n",
      "|     bert.encoder.layer.11.attention.self.value.bias     |    768     |    11.3952    |\n",
      "|   bert.encoder.layer.11.attention.output.dense.weight   |   589824   |   17290.9531  |\n",
      "|    bert.encoder.layer.11.attention.output.dense.bias    |    768     |    22.9623    |\n",
      "| bert.encoder.layer.11.attention.output.LayerNorm.weight |    768     |    655.2746   |\n",
      "|  bert.encoder.layer.11.attention.output.LayerNorm.bias  |    768     |    50.4074    |\n",
      "|     bert.encoder.layer.11.intermediate.dense.weight     |  2359296   |   74162.2656  |\n",
      "|      bert.encoder.layer.11.intermediate.dense.bias      |    3072    |    257.1159   |\n",
      "|        bert.encoder.layer.11.output.dense.weight        |  2359296   |   67212.3281  |\n",
      "|         bert.encoder.layer.11.output.dense.bias         |    768     |    32.8712    |\n",
      "|      bert.encoder.layer.11.output.LayerNorm.weight      |    768     |    485.9475   |\n",
      "|       bert.encoder.layer.11.output.LayerNorm.bias       |    768     |    32.0865    |\n",
      "|                 bert.pooler.dense.weight                |   589824   |   11560.4287  |\n",
      "|                  bert.pooler.dense.bias                 |    768     |    19.6365    |\n",
      "|                    classifier.weight                    |    4608    |    78.2943    |\n",
      "|                     classifier.bias                     |     6      |     0.0218    |\n",
      "|                  Total Trainable Params                 | 109486854  |   3589025.0   |\n",
      "+---------------------------------------------------------+------------+---------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 value: 0.733591514935646\n",
      "accuracy value: 0.734841703449908\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"xlnet-base-cased\",\"bert-base-uncased\"]\n",
    "for model_name in model_names:\n",
    "    #Initializing the model\n",
    "    print('==========================================================================')\n",
    "    print(f'============================== {model_name} ==============================')\n",
    "    print('==========================================================================')\n",
    "    this_model=OurAwesomeModel(model_name,dataset)\n",
    "    this_model.show_parameters()\n",
    "    this_model.evaluate(args)\n",
    "    #finding the best hyperparameters and retraining the model\n",
    "    print(f'------------------------------ regular {model_name} ------------------------------')\n",
    "    this_model.hpm_search(args)\n",
    "    this_model.show_parameters()\n",
    "    this_model.evaluate(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f0b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fb0a736",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a876613e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================\n",
      "============================== xlnet-base-cased ==============================\n",
      "=============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at C:\\Users\\liyag\\.cache\\huggingface\\datasets\\csv\\default-2e78edd390e366fa\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-f9f74d18eb609106.arrow\n",
      "Loading cached processed dataset at C:\\Users\\liyag\\.cache\\huggingface\\datasets\\csv\\default-2e78edd390e366fa\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b87b853908bc16f2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ regular xlnet-base-cased ------------------------------\n",
      "model was loaded\n",
      "------------------------------ xlnet-base-cased weights ------------------------------\n",
      "+-------------------------------------------------+------------+---------------+\n",
      "|                     Modules                     | Parameters | Sum of Tensor |\n",
      "+-------------------------------------------------+------------+---------------+\n",
      "|               transformer.mask_emb              |    768     |     7.254     |\n",
      "|        transformer.word_embedding.weight        |  24576000  |   1073323.0   |\n",
      "|          transformer.layer.0.rel_attn.q         |   589824   |   44516.6953  |\n",
      "|          transformer.layer.0.rel_attn.k         |   589824   |    94528.0    |\n",
      "|          transformer.layer.0.rel_attn.v         |   589824   |   15092.6914  |\n",
      "|          transformer.layer.0.rel_attn.o         |   589824   |   16119.0615  |\n",
      "|          transformer.layer.0.rel_attn.r         |   589824   |   10848.5449  |\n",
      "|      transformer.layer.0.rel_attn.r_r_bias      |    768     |    117.9462   |\n",
      "|      transformer.layer.0.rel_attn.r_s_bias      |    768     |    136.0362   |\n",
      "|      transformer.layer.0.rel_attn.r_w_bias      |    768     |    158.9057   |\n",
      "|      transformer.layer.0.rel_attn.seg_embed     |    1536    |    355.0389   |\n",
      "|  transformer.layer.0.rel_attn.layer_norm.weight |    768     |    739.7091   |\n",
      "|   transformer.layer.0.rel_attn.layer_norm.bias  |    768     |    93.3119    |\n",
      "|     transformer.layer.0.ff.layer_norm.weight    |    768     |    818.2251   |\n",
      "|      transformer.layer.0.ff.layer_norm.bias     |    768     |    89.1191    |\n",
      "|      transformer.layer.0.ff.layer_1.weight      |  2359296   |   65556.9531  |\n",
      "|       transformer.layer.0.ff.layer_1.bias       |    3072    |    360.2599   |\n",
      "|      transformer.layer.0.ff.layer_2.weight      |  2359296   |   57193.0273  |\n",
      "|       transformer.layer.0.ff.layer_2.bias       |    768     |    30.7977    |\n",
      "|          transformer.layer.1.rel_attn.q         |   589824   |   16621.5742  |\n",
      "|          transformer.layer.1.rel_attn.k         |   589824   |    20351.25   |\n",
      "|          transformer.layer.1.rel_attn.v         |   589824   |   15446.0039  |\n",
      "|          transformer.layer.1.rel_attn.o         |   589824   |   15908.8301  |\n",
      "|          transformer.layer.1.rel_attn.r         |   589824   |    5369.167   |\n",
      "|      transformer.layer.1.rel_attn.r_r_bias      |    768     |    211.618    |\n",
      "|      transformer.layer.1.rel_attn.r_s_bias      |    768     |    209.2944   |\n",
      "|      transformer.layer.1.rel_attn.r_w_bias      |    768     |    128.2724   |\n",
      "|      transformer.layer.1.rel_attn.seg_embed     |    1536    |    210.7794   |\n",
      "|  transformer.layer.1.rel_attn.layer_norm.weight |    768     |    724.1213   |\n",
      "|   transformer.layer.1.rel_attn.layer_norm.bias  |    768     |    111.2842   |\n",
      "|     transformer.layer.1.ff.layer_norm.weight    |    768     |    713.9308   |\n",
      "|      transformer.layer.1.ff.layer_norm.bias     |    768     |    114.9962   |\n",
      "|      transformer.layer.1.ff.layer_1.weight      |  2359296   |   70957.2891  |\n",
      "|       transformer.layer.1.ff.layer_1.bias       |    3072    |    339.3696   |\n",
      "|      transformer.layer.1.ff.layer_2.weight      |  2359296   |    61329.5    |\n",
      "|       transformer.layer.1.ff.layer_2.bias       |    768     |    35.8274    |\n",
      "|          transformer.layer.2.rel_attn.q         |   589824   |   16489.3828  |\n",
      "|          transformer.layer.2.rel_attn.k         |   589824   |   19956.7227  |\n",
      "|          transformer.layer.2.rel_attn.v         |   589824   |   15804.4395  |\n",
      "|          transformer.layer.2.rel_attn.o         |   589824   |   16022.5977  |\n",
      "|          transformer.layer.2.rel_attn.r         |   589824   |   5338.2607   |\n",
      "|      transformer.layer.2.rel_attn.r_r_bias      |    768     |    247.8882   |\n",
      "|      transformer.layer.2.rel_attn.r_s_bias      |    768     |    197.3545   |\n",
      "|      transformer.layer.2.rel_attn.r_w_bias      |    768     |    112.4253   |\n",
      "|      transformer.layer.2.rel_attn.seg_embed     |    1536    |    236.2521   |\n",
      "|  transformer.layer.2.rel_attn.layer_norm.weight |    768     |    707.5247   |\n",
      "|   transformer.layer.2.rel_attn.layer_norm.bias  |    768     |    123.1936   |\n",
      "|     transformer.layer.2.ff.layer_norm.weight    |    768     |    832.1479   |\n",
      "|      transformer.layer.2.ff.layer_norm.bias     |    768     |    95.9052    |\n",
      "|      transformer.layer.2.ff.layer_1.weight      |  2359296   |    71196.5    |\n",
      "|       transformer.layer.2.ff.layer_1.bias       |    3072    |    327.6783   |\n",
      "|      transformer.layer.2.ff.layer_2.weight      |  2359296   |   60559.1758  |\n",
      "|       transformer.layer.2.ff.layer_2.bias       |    768     |    46.5203    |\n",
      "|          transformer.layer.3.rel_attn.q         |   589824   |   14201.9092  |\n",
      "|          transformer.layer.3.rel_attn.k         |   589824   |   18440.4395  |\n",
      "|          transformer.layer.3.rel_attn.v         |   589824   |   16347.0186  |\n",
      "|          transformer.layer.3.rel_attn.o         |   589824   |   16101.8408  |\n",
      "|          transformer.layer.3.rel_attn.r         |   589824   |   5741.2773   |\n",
      "|      transformer.layer.3.rel_attn.r_r_bias      |    768     |    300.1675   |\n",
      "|      transformer.layer.3.rel_attn.r_s_bias      |    768     |    241.8109   |\n",
      "|      transformer.layer.3.rel_attn.r_w_bias      |    768     |    248.1956   |\n",
      "|      transformer.layer.3.rel_attn.seg_embed     |    1536    |    227.1258   |\n",
      "|  transformer.layer.3.rel_attn.layer_norm.weight |    768     |    695.2471   |\n",
      "|   transformer.layer.3.rel_attn.layer_norm.bias  |    768     |    110.2333   |\n",
      "|     transformer.layer.3.ff.layer_norm.weight    |    768     |    859.3008   |\n",
      "|      transformer.layer.3.ff.layer_norm.bias     |    768     |    142.6005   |\n",
      "|      transformer.layer.3.ff.layer_1.weight      |  2359296   |   71190.8281  |\n",
      "|       transformer.layer.3.ff.layer_1.bias       |    3072    |    337.9751   |\n",
      "|      transformer.layer.3.ff.layer_2.weight      |  2359296   |   59924.7969  |\n",
      "|       transformer.layer.3.ff.layer_2.bias       |    768     |    46.4886    |\n",
      "|          transformer.layer.4.rel_attn.q         |   589824   |   16041.9648  |\n",
      "|          transformer.layer.4.rel_attn.k         |   589824   |   18267.7031  |\n",
      "|          transformer.layer.4.rel_attn.v         |   589824   |   16337.3252  |\n",
      "|          transformer.layer.4.rel_attn.o         |   589824   |   16669.9023  |\n",
      "|          transformer.layer.4.rel_attn.r         |   589824   |   5013.7627   |\n",
      "|      transformer.layer.4.rel_attn.r_r_bias      |    768     |    149.1275   |\n",
      "|      transformer.layer.4.rel_attn.r_s_bias      |    768     |    159.751    |\n",
      "|      transformer.layer.4.rel_attn.r_w_bias      |    768     |    85.6629    |\n",
      "|      transformer.layer.4.rel_attn.seg_embed     |    1536    |    193.0969   |\n",
      "|  transformer.layer.4.rel_attn.layer_norm.weight |    768     |    697.0922   |\n",
      "|   transformer.layer.4.rel_attn.layer_norm.bias  |    768     |    119.1436   |\n",
      "|     transformer.layer.4.ff.layer_norm.weight    |    768     |    842.9413   |\n",
      "|      transformer.layer.4.ff.layer_norm.bias     |    768     |    74.3242    |\n",
      "|      transformer.layer.4.ff.layer_1.weight      |  2359296   |   70438.3594  |\n",
      "|       transformer.layer.4.ff.layer_1.bias       |    3072    |    289.3455   |\n",
      "|      transformer.layer.4.ff.layer_2.weight      |  2359296   |   59469.4609  |\n",
      "|       transformer.layer.4.ff.layer_2.bias       |    768     |     52.86     |\n",
      "|          transformer.layer.5.rel_attn.q         |   589824   |   20057.9277  |\n",
      "|          transformer.layer.5.rel_attn.k         |   589824   |   22951.125   |\n",
      "|          transformer.layer.5.rel_attn.v         |   589824   |   16004.6465  |\n",
      "|          transformer.layer.5.rel_attn.o         |   589824   |   15618.6191  |\n",
      "|          transformer.layer.5.rel_attn.r         |   589824   |   5893.2017   |\n",
      "|      transformer.layer.5.rel_attn.r_r_bias      |    768     |    135.0706   |\n",
      "|      transformer.layer.5.rel_attn.r_s_bias      |    768     |    144.1386   |\n",
      "|      transformer.layer.5.rel_attn.r_w_bias      |    768     |    91.5611    |\n",
      "|      transformer.layer.5.rel_attn.seg_embed     |    1536    |    191.4123   |\n",
      "|  transformer.layer.5.rel_attn.layer_norm.weight |    768     |    710.2025   |\n",
      "|   transformer.layer.5.rel_attn.layer_norm.bias  |    768     |    67.4739    |\n",
      "|     transformer.layer.5.ff.layer_norm.weight    |    768     |   1048.9409   |\n",
      "|      transformer.layer.5.ff.layer_norm.bias     |    768     |    112.0247   |\n",
      "|      transformer.layer.5.ff.layer_1.weight      |  2359296   |   67723.125   |\n",
      "|       transformer.layer.5.ff.layer_1.bias       |    3072    |    309.3526   |\n",
      "|      transformer.layer.5.ff.layer_2.weight      |  2359296   |   57270.0078  |\n",
      "|       transformer.layer.5.ff.layer_2.bias       |    768     |     48.547    |\n",
      "|          transformer.layer.6.rel_attn.q         |   589824   |   17462.4492  |\n",
      "|          transformer.layer.6.rel_attn.k         |   589824   |   20022.082   |\n",
      "|          transformer.layer.6.rel_attn.v         |   589824   |   16422.5898  |\n",
      "|          transformer.layer.6.rel_attn.o         |   589824   |   15961.0244  |\n",
      "|          transformer.layer.6.rel_attn.r         |   589824   |   5371.1924   |\n",
      "|      transformer.layer.6.rel_attn.r_r_bias      |    768     |    79.5023    |\n",
      "|      transformer.layer.6.rel_attn.r_s_bias      |    768     |    143.3034   |\n",
      "|      transformer.layer.6.rel_attn.r_w_bias      |    768     |    74.7939    |\n",
      "|      transformer.layer.6.rel_attn.seg_embed     |    1536    |    123.7623   |\n",
      "|  transformer.layer.6.rel_attn.layer_norm.weight |    768     |    824.7728   |\n",
      "|   transformer.layer.6.rel_attn.layer_norm.bias  |    768     |    100.4918   |\n",
      "|     transformer.layer.6.ff.layer_norm.weight    |    768     |   1045.1604   |\n",
      "|      transformer.layer.6.ff.layer_norm.bias     |    768     |    77.5212    |\n",
      "|      transformer.layer.6.ff.layer_1.weight      |  2359296   |   71031.4609  |\n",
      "|       transformer.layer.6.ff.layer_1.bias       |    3072    |    286.2841   |\n",
      "|      transformer.layer.6.ff.layer_2.weight      |  2359296   |   55266.4453  |\n",
      "|       transformer.layer.6.ff.layer_2.bias       |    768     |    54.1989    |\n",
      "|          transformer.layer.7.rel_attn.q         |   589824   |   17523.916   |\n",
      "|          transformer.layer.7.rel_attn.k         |   589824   |   20381.0098  |\n",
      "|          transformer.layer.7.rel_attn.v         |   589824   |   16730.4355  |\n",
      "|          transformer.layer.7.rel_attn.o         |   589824   |   15741.9336  |\n",
      "|          transformer.layer.7.rel_attn.r         |   589824   |   5178.6074   |\n",
      "|      transformer.layer.7.rel_attn.r_r_bias      |    768     |    86.9653    |\n",
      "|      transformer.layer.7.rel_attn.r_s_bias      |    768     |    141.4761   |\n",
      "|      transformer.layer.7.rel_attn.r_w_bias      |    768     |    53.3114    |\n",
      "|      transformer.layer.7.rel_attn.seg_embed     |    1536    |    146.9689   |\n",
      "|  transformer.layer.7.rel_attn.layer_norm.weight |    768     |    859.1363   |\n",
      "|   transformer.layer.7.rel_attn.layer_norm.bias  |    768     |     75.514    |\n",
      "|     transformer.layer.7.ff.layer_norm.weight    |    768     |   1135.0479   |\n",
      "|      transformer.layer.7.ff.layer_norm.bias     |    768     |    86.7028    |\n",
      "|      transformer.layer.7.ff.layer_1.weight      |  2359296   |   70600.5938  |\n",
      "|       transformer.layer.7.ff.layer_1.bias       |    3072    |    301.3833   |\n",
      "|      transformer.layer.7.ff.layer_2.weight      |  2359296   |   53523.6914  |\n",
      "|       transformer.layer.7.ff.layer_2.bias       |    768     |    58.7541    |\n",
      "|          transformer.layer.8.rel_attn.q         |   589824   |   17310.2656  |\n",
      "|          transformer.layer.8.rel_attn.k         |   589824   |   20113.9023  |\n",
      "|          transformer.layer.8.rel_attn.v         |   589824   |   17486.1016  |\n",
      "|          transformer.layer.8.rel_attn.o         |   589824   |   16634.457   |\n",
      "|          transformer.layer.8.rel_attn.r         |   589824   |   4995.8047   |\n",
      "|      transformer.layer.8.rel_attn.r_r_bias      |    768     |    76.3157    |\n",
      "|      transformer.layer.8.rel_attn.r_s_bias      |    768     |    104.5686   |\n",
      "|      transformer.layer.8.rel_attn.r_w_bias      |    768     |    50.6241    |\n",
      "|      transformer.layer.8.rel_attn.seg_embed     |    1536    |    131.8607   |\n",
      "|  transformer.layer.8.rel_attn.layer_norm.weight |    768     |   1081.1025   |\n",
      "|   transformer.layer.8.rel_attn.layer_norm.bias  |    768     |    68.8813    |\n",
      "|     transformer.layer.8.ff.layer_norm.weight    |    768     |   1077.2712   |\n",
      "|      transformer.layer.8.ff.layer_norm.bias     |    768     |    70.7522    |\n",
      "|      transformer.layer.8.ff.layer_1.weight      |  2359296   |   80630.8281  |\n",
      "|       transformer.layer.8.ff.layer_1.bias       |    3072    |    258.1456   |\n",
      "|      transformer.layer.8.ff.layer_2.weight      |  2359296   |   49929.6367  |\n",
      "|       transformer.layer.8.ff.layer_2.bias       |    768     |    48.4146    |\n",
      "|          transformer.layer.9.rel_attn.q         |   589824   |   19113.3242  |\n",
      "|          transformer.layer.9.rel_attn.k         |   589824   |   22511.4648  |\n",
      "|          transformer.layer.9.rel_attn.v         |   589824   |   17783.8086  |\n",
      "|          transformer.layer.9.rel_attn.o         |   589824   |   16196.8965  |\n",
      "|          transformer.layer.9.rel_attn.r         |   589824   |   4870.5957   |\n",
      "|      transformer.layer.9.rel_attn.r_r_bias      |    768     |    71.5733    |\n",
      "|      transformer.layer.9.rel_attn.r_s_bias      |    768     |    115.7824   |\n",
      "|      transformer.layer.9.rel_attn.r_w_bias      |    768     |    66.1586    |\n",
      "|      transformer.layer.9.rel_attn.seg_embed     |    1536    |    114.4543   |\n",
      "|  transformer.layer.9.rel_attn.layer_norm.weight |    768     |   1043.3905   |\n",
      "|   transformer.layer.9.rel_attn.layer_norm.bias  |    768     |     53.611    |\n",
      "|     transformer.layer.9.ff.layer_norm.weight    |    768     |   1179.9061   |\n",
      "|      transformer.layer.9.ff.layer_norm.bias     |    768     |    80.7012    |\n",
      "|      transformer.layer.9.ff.layer_1.weight      |  2359296   |   78616.1875  |\n",
      "|       transformer.layer.9.ff.layer_1.bias       |    3072    |    292.207    |\n",
      "|      transformer.layer.9.ff.layer_2.weight      |  2359296   |   52394.168   |\n",
      "|       transformer.layer.9.ff.layer_2.bias       |    768     |    45.2117    |\n",
      "|         transformer.layer.10.rel_attn.q         |   589824   |   15598.625   |\n",
      "|         transformer.layer.10.rel_attn.k         |   589824   |   18651.3164  |\n",
      "|         transformer.layer.10.rel_attn.v         |   589824   |   19207.5664  |\n",
      "|         transformer.layer.10.rel_attn.o         |   589824   |   17643.9102  |\n",
      "|         transformer.layer.10.rel_attn.r         |   589824   |   4347.3672   |\n",
      "|      transformer.layer.10.rel_attn.r_r_bias     |    768     |    94.0733    |\n",
      "|      transformer.layer.10.rel_attn.r_s_bias     |    768     |    110.1988   |\n",
      "|      transformer.layer.10.rel_attn.r_w_bias     |    768     |    93.5657    |\n",
      "|     transformer.layer.10.rel_attn.seg_embed     |    1536    |    111.9691   |\n",
      "| transformer.layer.10.rel_attn.layer_norm.weight |    768     |   1054.1653   |\n",
      "|  transformer.layer.10.rel_attn.layer_norm.bias  |    768     |    61.7568    |\n",
      "|    transformer.layer.10.ff.layer_norm.weight    |    768     |   1021.5345   |\n",
      "|     transformer.layer.10.ff.layer_norm.bias     |    768     |     63.315    |\n",
      "|      transformer.layer.10.ff.layer_1.weight     |  2359296   |   78410.9375  |\n",
      "|       transformer.layer.10.ff.layer_1.bias      |    3072    |    258.3932   |\n",
      "|      transformer.layer.10.ff.layer_2.weight     |  2359296   |   50200.793   |\n",
      "|       transformer.layer.10.ff.layer_2.bias      |    768     |    41.4479    |\n",
      "|         transformer.layer.11.rel_attn.q         |   589824   |   18407.0684  |\n",
      "|         transformer.layer.11.rel_attn.k         |   589824   |   23457.332   |\n",
      "|         transformer.layer.11.rel_attn.v         |   589824   |   24126.5527  |\n",
      "|         transformer.layer.11.rel_attn.o         |   589824   |   20705.4688  |\n",
      "|         transformer.layer.11.rel_attn.r         |   589824   |   3692.2422   |\n",
      "|      transformer.layer.11.rel_attn.r_r_bias     |    768     |    93.5545    |\n",
      "|      transformer.layer.11.rel_attn.r_s_bias     |    768     |    76.0979    |\n",
      "|      transformer.layer.11.rel_attn.r_w_bias     |    768     |    94.9552    |\n",
      "|     transformer.layer.11.rel_attn.seg_embed     |    1536    |    84.2926    |\n",
      "| transformer.layer.11.rel_attn.layer_norm.weight |    768     |    1216.405   |\n",
      "|  transformer.layer.11.rel_attn.layer_norm.bias  |    768     |    76.2078    |\n",
      "|    transformer.layer.11.ff.layer_norm.weight    |    768     |   1857.5792   |\n",
      "|     transformer.layer.11.ff.layer_norm.bias     |    768     |    135.2948   |\n",
      "|      transformer.layer.11.ff.layer_1.weight     |  2359296   |   82164.1094  |\n",
      "|       transformer.layer.11.ff.layer_1.bias      |    3072    |    300.1009   |\n",
      "|      transformer.layer.11.ff.layer_2.weight     |  2359296   |   45293.8633  |\n",
      "|       transformer.layer.11.ff.layer_2.bias      |    768     |    35.8956    |\n",
      "|         sequence_summary.summary.weight         |   589824   |   9454.0723   |\n",
      "|          sequence_summary.summary.bias          |    768     |     0.7698    |\n",
      "|                logits_proj.weight               |    4608    |    78.2072    |\n",
      "|                 logits_proj.bias                |     6      |     0.0063    |\n",
      "|              Total Trainable Params             | 117313542  |   3685509.25  |\n",
      "+-------------------------------------------------+------------+---------------+\n",
      "------------------------------ xlnet-base-cased metrics ------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 value: 0.7080168983557641\n",
      "accuracy value: 0.7100554578098796\n",
      "\n",
      "------------------------------ pruned xlnet-base-cased ------------------------------\n",
      "\n",
      "transformer.layer.0.ff.layer_1:\n",
      "old_total_weights = -103.30775451660156\n",
      "new_total_weights = -91.01393127441406\n",
      "\n",
      "transformer.layer.0.ff.layer_2:\n",
      "old_total_weights = -84.21345520019531\n",
      "new_total_weights = -85.44734191894531\n",
      "\n",
      "transformer.layer.1.ff.layer_1:\n",
      "old_total_weights = -572.9107666015625\n",
      "new_total_weights = -556.9412841796875\n",
      "\n",
      "transformer.layer.1.ff.layer_2:\n",
      "old_total_weights = 48.340579986572266\n",
      "new_total_weights = 62.087646484375\n",
      "\n",
      "transformer.layer.2.ff.layer_1:\n",
      "old_total_weights = -391.30279541015625\n",
      "new_total_weights = -354.6177978515625\n",
      "\n",
      "transformer.layer.2.ff.layer_2:\n",
      "old_total_weights = -18.800872802734375\n",
      "new_total_weights = -9.930244445800781\n",
      "\n",
      "transformer.layer.3.ff.layer_1:\n",
      "old_total_weights = 245.47509765625\n",
      "new_total_weights = 186.15032958984375\n",
      "\n",
      "transformer.layer.3.ff.layer_2:\n",
      "old_total_weights = -102.96053314208984\n",
      "new_total_weights = -105.81788635253906\n",
      "\n",
      "transformer.layer.4.ff.layer_1:\n",
      "old_total_weights = -221.44171142578125\n",
      "new_total_weights = -215.08334350585938\n",
      "\n",
      "transformer.layer.4.ff.layer_2:\n",
      "old_total_weights = -70.40290832519531\n",
      "new_total_weights = -74.33273315429688\n",
      "\n",
      "transformer.layer.5.ff.layer_1:\n",
      "old_total_weights = -561.5191650390625\n",
      "new_total_weights = -531.55224609375\n",
      "\n",
      "transformer.layer.5.ff.layer_2:\n",
      "old_total_weights = -69.0758056640625\n",
      "new_total_weights = -59.17731475830078\n",
      "\n",
      "transformer.layer.6.ff.layer_1:\n",
      "old_total_weights = -1177.7933349609375\n",
      "new_total_weights = -1120.612548828125\n",
      "\n",
      "transformer.layer.6.ff.layer_2:\n",
      "old_total_weights = -39.358642578125\n",
      "new_total_weights = -32.47034454345703\n",
      "\n",
      "transformer.layer.7.ff.layer_1:\n",
      "old_total_weights = -726.6224365234375\n",
      "new_total_weights = -685.234375\n",
      "\n",
      "transformer.layer.7.ff.layer_2:\n",
      "old_total_weights = -93.46343994140625\n",
      "new_total_weights = -104.52298736572266\n",
      "\n",
      "transformer.layer.8.ff.layer_1:\n",
      "old_total_weights = -1783.2421875\n",
      "new_total_weights = -1670.220458984375\n",
      "\n",
      "transformer.layer.8.ff.layer_2:\n",
      "old_total_weights = -53.68107604980469\n",
      "new_total_weights = -53.00212860107422\n",
      "\n",
      "transformer.layer.9.ff.layer_1:\n",
      "old_total_weights = -2138.02978515625\n",
      "new_total_weights = -2051.72314453125\n",
      "\n",
      "transformer.layer.9.ff.layer_2:\n",
      "old_total_weights = -62.41457748413086\n",
      "new_total_weights = -80.59735870361328\n",
      "\n",
      "transformer.layer.10.ff.layer_1:\n",
      "old_total_weights = -1560.965087890625\n",
      "new_total_weights = -1495.5225830078125\n",
      "\n",
      "transformer.layer.10.ff.layer_2:\n",
      "old_total_weights = -61.86067199707031\n",
      "new_total_weights = -64.6903076171875\n",
      "\n",
      "transformer.layer.11.ff.layer_1:\n",
      "old_total_weights = -845.776611328125\n",
      "new_total_weights = -808.4384765625\n",
      "\n",
      "transformer.layer.11.ff.layer_2:\n",
      "old_total_weights = 51.680519104003906\n",
      "new_total_weights = 47.33669662475586\n",
      "\n",
      "sequence_summary.summary:\n",
      "old_total_weights = -0.7814615964889526\n",
      "new_total_weights = 5.110146522521973\n",
      "\n",
      "logits_proj:\n",
      "old_total_weights = 1.8477519750595093\n",
      "new_total_weights = 2.246659278869629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48596' max='48596' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48596/48596 1:23:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.986300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.990600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.942900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.975300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.930600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.954300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.950500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.968600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.944800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.924900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.923800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.917900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.920300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.902900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.920500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.919400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.923700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.906500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.872400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.911600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.890300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.888500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.885200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.893100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.876600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.846800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.864200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.844900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.852400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.861200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.839700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.837100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.826400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.805600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.841500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.834600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.827400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.847200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.809600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.816700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.821200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.806100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.830300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model was saved\n",
      "------------------------------ pruned xlnet-base-cased weights ------------------------------\n",
      "+-------------------------------------------------+------------+---------------+\n",
      "|                     Modules                     | Parameters | Sum of Tensor |\n",
      "+-------------------------------------------------+------------+---------------+\n",
      "|               transformer.mask_emb              |    768     |     7.254     |\n",
      "|        transformer.word_embedding.weight        |  24576000  |  1074652.125  |\n",
      "|          transformer.layer.0.rel_attn.q         |   589824   |   44642.8477  |\n",
      "|          transformer.layer.0.rel_attn.k         |   589824   |   94493.9531  |\n",
      "|          transformer.layer.0.rel_attn.v         |   589824   |   15240.1621  |\n",
      "|          transformer.layer.0.rel_attn.o         |   589824   |   16221.5801  |\n",
      "|          transformer.layer.0.rel_attn.r         |   589824   |   11345.1953  |\n",
      "|      transformer.layer.0.rel_attn.r_r_bias      |    768     |    114.6121   |\n",
      "|      transformer.layer.0.rel_attn.r_s_bias      |    768     |    136.6564   |\n",
      "|      transformer.layer.0.rel_attn.r_w_bias      |    768     |    157.8763   |\n",
      "|      transformer.layer.0.rel_attn.seg_embed     |    1536    |    356.2233   |\n",
      "|  transformer.layer.0.rel_attn.layer_norm.weight |    768     |    741.6852   |\n",
      "|   transformer.layer.0.rel_attn.layer_norm.bias  |    768     |    94.0212    |\n",
      "|     transformer.layer.0.ff.layer_norm.weight    |    768     |    817.6913   |\n",
      "|      transformer.layer.0.ff.layer_norm.bias     |    768     |    88.9587    |\n",
      "|       transformer.layer.0.ff.layer_1.bias       |    3072    |    361.0219   |\n",
      "|      transformer.layer.0.ff.layer_1.weight      |  2359296   |   58696.3047  |\n",
      "|       transformer.layer.0.ff.layer_2.bias       |    768     |    30.8244    |\n",
      "|      transformer.layer.0.ff.layer_2.weight      |  2359296   |   51499.2109  |\n",
      "|          transformer.layer.1.rel_attn.q         |   589824   |   16875.8789  |\n",
      "|          transformer.layer.1.rel_attn.k         |   589824   |   20562.0566  |\n",
      "|          transformer.layer.1.rel_attn.v         |   589824   |   15608.0723  |\n",
      "|          transformer.layer.1.rel_attn.o         |   589824   |   16025.3359  |\n",
      "|          transformer.layer.1.rel_attn.r         |   589824   |   6123.9287   |\n",
      "|      transformer.layer.1.rel_attn.r_r_bias      |    768     |    210.9695   |\n",
      "|      transformer.layer.1.rel_attn.r_s_bias      |    768     |    205.9525   |\n",
      "|      transformer.layer.1.rel_attn.r_w_bias      |    768     |    127.975    |\n",
      "|      transformer.layer.1.rel_attn.seg_embed     |    1536    |    208.9754   |\n",
      "|  transformer.layer.1.rel_attn.layer_norm.weight |    768     |    724.6876   |\n",
      "|   transformer.layer.1.rel_attn.layer_norm.bias  |    768     |    112.511    |\n",
      "|     transformer.layer.1.ff.layer_norm.weight    |    768     |    713.7956   |\n",
      "|      transformer.layer.1.ff.layer_norm.bias     |    768     |    114.6288   |\n",
      "|       transformer.layer.1.ff.layer_1.bias       |    3072    |    339.963    |\n",
      "|      transformer.layer.1.ff.layer_1.weight      |  2359296   |   62778.6953  |\n",
      "|       transformer.layer.1.ff.layer_2.bias       |    768     |    35.2207    |\n",
      "|      transformer.layer.1.ff.layer_2.weight      |  2359296   |   54905.3594  |\n",
      "|          transformer.layer.2.rel_attn.q         |   589824   |   16754.5352  |\n",
      "|          transformer.layer.2.rel_attn.k         |   589824   |   20181.5273  |\n",
      "|          transformer.layer.2.rel_attn.v         |   589824   |   15973.8945  |\n",
      "|          transformer.layer.2.rel_attn.o         |   589824   |   16172.0078  |\n",
      "|          transformer.layer.2.rel_attn.r         |   589824   |   6103.0728   |\n",
      "|      transformer.layer.2.rel_attn.r_r_bias      |    768     |    248.9393   |\n",
      "|      transformer.layer.2.rel_attn.r_s_bias      |    768     |    195.1954   |\n",
      "|      transformer.layer.2.rel_attn.r_w_bias      |    768     |    112.9556   |\n",
      "|      transformer.layer.2.rel_attn.seg_embed     |    1536    |    234.319    |\n",
      "|  transformer.layer.2.rel_attn.layer_norm.weight |    768     |    708.0233   |\n",
      "|   transformer.layer.2.rel_attn.layer_norm.bias  |    768     |    124.1004   |\n",
      "|     transformer.layer.2.ff.layer_norm.weight    |    768     |    831.1628   |\n",
      "|      transformer.layer.2.ff.layer_norm.bias     |    768     |    96.1713    |\n",
      "|       transformer.layer.2.ff.layer_1.bias       |    3072    |    328.4273   |\n",
      "|      transformer.layer.2.ff.layer_1.weight      |  2359296   |   62837.3906  |\n",
      "|       transformer.layer.2.ff.layer_2.bias       |    768     |    45.9793    |\n",
      "|      transformer.layer.2.ff.layer_2.weight      |  2359296   |   54151.5586  |\n",
      "|          transformer.layer.3.rel_attn.q         |   589824   |   14490.1738  |\n",
      "|          transformer.layer.3.rel_attn.k         |   589824   |   18648.5078  |\n",
      "|          transformer.layer.3.rel_attn.v         |   589824   |   16486.1992  |\n",
      "|          transformer.layer.3.rel_attn.o         |   589824   |   16248.5469  |\n",
      "|          transformer.layer.3.rel_attn.r         |   589824   |   6546.8975   |\n",
      "|      transformer.layer.3.rel_attn.r_r_bias      |    768     |    302.4217   |\n",
      "|      transformer.layer.3.rel_attn.r_s_bias      |    768     |    242.0781   |\n",
      "|      transformer.layer.3.rel_attn.r_w_bias      |    768     |    246.3671   |\n",
      "|      transformer.layer.3.rel_attn.seg_embed     |    1536    |    227.468    |\n",
      "|  transformer.layer.3.rel_attn.layer_norm.weight |    768     |    695.472    |\n",
      "|   transformer.layer.3.rel_attn.layer_norm.bias  |    768     |    112.8356   |\n",
      "|     transformer.layer.3.ff.layer_norm.weight    |    768     |    859.3215   |\n",
      "|      transformer.layer.3.ff.layer_norm.bias     |    768     |    140.5322   |\n",
      "|       transformer.layer.3.ff.layer_1.bias       |    3072    |    339.9103   |\n",
      "|      transformer.layer.3.ff.layer_1.weight      |  2359296   |   62742.4453  |\n",
      "|       transformer.layer.3.ff.layer_2.bias       |    768     |    46.5707    |\n",
      "|      transformer.layer.3.ff.layer_2.weight      |  2359296   |   53667.3789  |\n",
      "|          transformer.layer.4.rel_attn.q         |   589824   |   16263.0234  |\n",
      "|          transformer.layer.4.rel_attn.k         |   589824   |   18470.6465  |\n",
      "|          transformer.layer.4.rel_attn.v         |   589824   |   16409.4609  |\n",
      "|          transformer.layer.4.rel_attn.o         |   589824   |   16700.6367  |\n",
      "|          transformer.layer.4.rel_attn.r         |   589824   |   5725.9424   |\n",
      "|      transformer.layer.4.rel_attn.r_r_bias      |    768     |    149.1271   |\n",
      "|      transformer.layer.4.rel_attn.r_s_bias      |    768     |    158.7247   |\n",
      "|      transformer.layer.4.rel_attn.r_w_bias      |    768     |    86.4166    |\n",
      "|      transformer.layer.4.rel_attn.seg_embed     |    1536    |    193.0262   |\n",
      "|  transformer.layer.4.rel_attn.layer_norm.weight |    768     |    697.5854   |\n",
      "|   transformer.layer.4.rel_attn.layer_norm.bias  |    768     |    120.2386   |\n",
      "|     transformer.layer.4.ff.layer_norm.weight    |    768     |    842.0974   |\n",
      "|      transformer.layer.4.ff.layer_norm.bias     |    768     |    73.4543    |\n",
      "|       transformer.layer.4.ff.layer_1.bias       |    3072    |    291.4529   |\n",
      "|      transformer.layer.4.ff.layer_1.weight      |  2359296   |   62035.1758  |\n",
      "|       transformer.layer.4.ff.layer_2.bias       |    768     |     52.652    |\n",
      "|      transformer.layer.4.ff.layer_2.weight      |  2359296   |   53124.1172  |\n",
      "|          transformer.layer.5.rel_attn.q         |   589824   |   20255.8418  |\n",
      "|          transformer.layer.5.rel_attn.k         |   589824   |   23125.5254  |\n",
      "|          transformer.layer.5.rel_attn.v         |   589824   |   16073.5303  |\n",
      "|          transformer.layer.5.rel_attn.o         |   589824   |   15668.3262  |\n",
      "|          transformer.layer.5.rel_attn.r         |   589824   |   6565.2988   |\n",
      "|      transformer.layer.5.rel_attn.r_r_bias      |    768     |    134.9256   |\n",
      "|      transformer.layer.5.rel_attn.r_s_bias      |    768     |    143.5105   |\n",
      "|      transformer.layer.5.rel_attn.r_w_bias      |    768     |    92.9993    |\n",
      "|      transformer.layer.5.rel_attn.seg_embed     |    1536    |    190.6307   |\n",
      "|  transformer.layer.5.rel_attn.layer_norm.weight |    768     |    710.1032   |\n",
      "|   transformer.layer.5.rel_attn.layer_norm.bias  |    768     |    68.9322    |\n",
      "|     transformer.layer.5.ff.layer_norm.weight    |    768     |   1047.9591   |\n",
      "|      transformer.layer.5.ff.layer_norm.bias     |    768     |    111.3008   |\n",
      "|       transformer.layer.5.ff.layer_1.bias       |    3072    |    311.3035   |\n",
      "|      transformer.layer.5.ff.layer_1.weight      |  2359296   |   59808.2344  |\n",
      "|       transformer.layer.5.ff.layer_2.bias       |    768     |    48.2549    |\n",
      "|      transformer.layer.5.ff.layer_2.weight      |  2359296   |   51125.7422  |\n",
      "|          transformer.layer.6.rel_attn.q         |   589824   |   17701.0039  |\n",
      "|          transformer.layer.6.rel_attn.k         |   589824   |   20206.7695  |\n",
      "|          transformer.layer.6.rel_attn.v         |   589824   |   16463.8125  |\n",
      "|          transformer.layer.6.rel_attn.o         |   589824   |   15993.6719  |\n",
      "|          transformer.layer.6.rel_attn.r         |   589824   |   6174.3105   |\n",
      "|      transformer.layer.6.rel_attn.r_r_bias      |    768     |     79.779    |\n",
      "|      transformer.layer.6.rel_attn.r_s_bias      |    768     |    144.0668   |\n",
      "|      transformer.layer.6.rel_attn.r_w_bias      |    768     |    75.4777    |\n",
      "|      transformer.layer.6.rel_attn.seg_embed     |    1536    |    123.8582   |\n",
      "|  transformer.layer.6.rel_attn.layer_norm.weight |    768     |    824.1987   |\n",
      "|   transformer.layer.6.rel_attn.layer_norm.bias  |    768     |    101.8823   |\n",
      "|     transformer.layer.6.ff.layer_norm.weight    |    768     |   1043.9094   |\n",
      "|      transformer.layer.6.ff.layer_norm.bias     |    768     |    77.0052    |\n",
      "|       transformer.layer.6.ff.layer_1.bias       |    3072    |    289.8256   |\n",
      "|      transformer.layer.6.ff.layer_1.weight      |  2359296   |   62392.6875  |\n",
      "|       transformer.layer.6.ff.layer_2.bias       |    768     |    54.2648    |\n",
      "|      transformer.layer.6.ff.layer_2.weight      |  2359296   |   49447.3828  |\n",
      "|          transformer.layer.7.rel_attn.q         |   589824   |   17737.3008  |\n",
      "|          transformer.layer.7.rel_attn.k         |   589824   |   20553.1055  |\n",
      "|          transformer.layer.7.rel_attn.v         |   589824   |   16744.8125  |\n",
      "|          transformer.layer.7.rel_attn.o         |   589824   |   15738.2891  |\n",
      "|          transformer.layer.7.rel_attn.r         |   589824   |   5917.3809   |\n",
      "|      transformer.layer.7.rel_attn.r_r_bias      |    768     |    87.6611    |\n",
      "|      transformer.layer.7.rel_attn.r_s_bias      |    768     |    140.2031   |\n",
      "|      transformer.layer.7.rel_attn.r_w_bias      |    768     |    54.0064    |\n",
      "|      transformer.layer.7.rel_attn.seg_embed     |    1536    |    146.5765   |\n",
      "|  transformer.layer.7.rel_attn.layer_norm.weight |    768     |    858.7091   |\n",
      "|   transformer.layer.7.rel_attn.layer_norm.bias  |    768     |    76.7471    |\n",
      "|     transformer.layer.7.ff.layer_norm.weight    |    768     |   1133.4043   |\n",
      "|      transformer.layer.7.ff.layer_norm.bias     |    768     |    85.9664    |\n",
      "|       transformer.layer.7.ff.layer_1.bias       |    3072    |    305.4758   |\n",
      "|      transformer.layer.7.ff.layer_1.weight      |  2359296   |   62086.293   |\n",
      "|       transformer.layer.7.ff.layer_2.bias       |    768     |    58.4999    |\n",
      "|      transformer.layer.7.ff.layer_2.weight      |  2359296   |   47919.7969  |\n",
      "|          transformer.layer.8.rel_attn.q         |   589824   |   17538.6152  |\n",
      "|          transformer.layer.8.rel_attn.k         |   589824   |   20280.9277  |\n",
      "|          transformer.layer.8.rel_attn.v         |   589824   |   17556.2422  |\n",
      "|          transformer.layer.8.rel_attn.o         |   589824   |   16686.3633  |\n",
      "|          transformer.layer.8.rel_attn.r         |   589824   |   6064.4551   |\n",
      "|      transformer.layer.8.rel_attn.r_r_bias      |    768     |    77.2371    |\n",
      "|      transformer.layer.8.rel_attn.r_s_bias      |    768     |    103.6193   |\n",
      "|      transformer.layer.8.rel_attn.r_w_bias      |    768     |    50.2046    |\n",
      "|      transformer.layer.8.rel_attn.seg_embed     |    1536    |    131.586    |\n",
      "|  transformer.layer.8.rel_attn.layer_norm.weight |    768     |   1079.7732   |\n",
      "|   transformer.layer.8.rel_attn.layer_norm.bias  |    768     |    70.8434    |\n",
      "|     transformer.layer.8.ff.layer_norm.weight    |    768     |   1075.4242   |\n",
      "|      transformer.layer.8.ff.layer_norm.bias     |    768     |    70.8659    |\n",
      "|       transformer.layer.8.ff.layer_1.bias       |    3072    |    263.1426   |\n",
      "|      transformer.layer.8.ff.layer_1.weight      |  2359296   |   69925.0625  |\n",
      "|       transformer.layer.8.ff.layer_2.bias       |    768     |    48.1487    |\n",
      "|      transformer.layer.8.ff.layer_2.weight      |  2359296   |   45093.0977  |\n",
      "|          transformer.layer.9.rel_attn.q         |   589824   |   19340.1211  |\n",
      "|          transformer.layer.9.rel_attn.k         |   589824   |   22675.5801  |\n",
      "|          transformer.layer.9.rel_attn.v         |   589824   |   17882.0527  |\n",
      "|          transformer.layer.9.rel_attn.o         |   589824   |   16324.376   |\n",
      "|          transformer.layer.9.rel_attn.r         |   589824   |   6131.5703   |\n",
      "|      transformer.layer.9.rel_attn.r_r_bias      |    768     |    71.9018    |\n",
      "|      transformer.layer.9.rel_attn.r_s_bias      |    768     |    115.8533   |\n",
      "|      transformer.layer.9.rel_attn.r_w_bias      |    768     |     65.164    |\n",
      "|      transformer.layer.9.rel_attn.seg_embed     |    1536    |    115.0058   |\n",
      "|  transformer.layer.9.rel_attn.layer_norm.weight |    768     |   1042.3492   |\n",
      "|   transformer.layer.9.rel_attn.layer_norm.bias  |    768     |    56.3204    |\n",
      "|     transformer.layer.9.ff.layer_norm.weight    |    768     |   1178.6633   |\n",
      "|      transformer.layer.9.ff.layer_norm.bias     |    768     |    81.0866    |\n",
      "|       transformer.layer.9.ff.layer_1.bias       |    3072    |    299.0774   |\n",
      "|      transformer.layer.9.ff.layer_1.weight      |  2359296   |   67979.9922  |\n",
      "|       transformer.layer.9.ff.layer_2.bias       |    768     |     45.077    |\n",
      "|      transformer.layer.9.ff.layer_2.weight      |  2359296   |   46764.7422  |\n",
      "|         transformer.layer.10.rel_attn.q         |   589824   |   15854.4844  |\n",
      "|         transformer.layer.10.rel_attn.k         |   589824   |   18874.4414  |\n",
      "|         transformer.layer.10.rel_attn.v         |   589824   |   19330.9102  |\n",
      "|         transformer.layer.10.rel_attn.o         |   589824   |   17793.3555  |\n",
      "|         transformer.layer.10.rel_attn.r         |   589824   |   5522.6963   |\n",
      "|      transformer.layer.10.rel_attn.r_r_bias     |    768     |    94.8428    |\n",
      "|      transformer.layer.10.rel_attn.r_s_bias     |    768     |    111.5846   |\n",
      "|      transformer.layer.10.rel_attn.r_w_bias     |    768     |    93.6858    |\n",
      "|     transformer.layer.10.rel_attn.seg_embed     |    1536    |    112.3223   |\n",
      "| transformer.layer.10.rel_attn.layer_norm.weight |    768     |   1053.6326   |\n",
      "|  transformer.layer.10.rel_attn.layer_norm.bias  |    768     |    63.6825    |\n",
      "|    transformer.layer.10.ff.layer_norm.weight    |    768     |   1020.4498   |\n",
      "|     transformer.layer.10.ff.layer_norm.bias     |    768     |    63.6866    |\n",
      "|       transformer.layer.10.ff.layer_1.bias      |    3072    |    267.0273   |\n",
      "|      transformer.layer.10.ff.layer_1.weight     |  2359296   |   67669.9766  |\n",
      "|       transformer.layer.10.ff.layer_2.bias      |    768     |    41.6033    |\n",
      "|      transformer.layer.10.ff.layer_2.weight     |  2359296   |   44493.8281  |\n",
      "|         transformer.layer.11.rel_attn.q         |   589824   |   18623.7266  |\n",
      "|         transformer.layer.11.rel_attn.k         |   589824   |   23637.7539  |\n",
      "|         transformer.layer.11.rel_attn.v         |   589824   |   24247.2383  |\n",
      "|         transformer.layer.11.rel_attn.o         |   589824   |   20846.9863  |\n",
      "|         transformer.layer.11.rel_attn.r         |   589824   |   4707.9893   |\n",
      "|      transformer.layer.11.rel_attn.r_r_bias     |    768     |    94.2334    |\n",
      "|      transformer.layer.11.rel_attn.r_s_bias     |    768     |    74.6874    |\n",
      "|      transformer.layer.11.rel_attn.r_w_bias     |    768     |    95.0732    |\n",
      "|     transformer.layer.11.rel_attn.seg_embed     |    1536    |    83.1704    |\n",
      "| transformer.layer.11.rel_attn.layer_norm.weight |    768     |   1214.8956   |\n",
      "|  transformer.layer.11.rel_attn.layer_norm.bias  |    768     |    77.8024    |\n",
      "|    transformer.layer.11.ff.layer_norm.weight    |    768     |   1851.3843   |\n",
      "|     transformer.layer.11.ff.layer_norm.bias     |    768     |    133.433    |\n",
      "|       transformer.layer.11.ff.layer_1.bias      |    3072    |    307.6002   |\n",
      "|      transformer.layer.11.ff.layer_1.weight     |  2359296   |   70390.0781  |\n",
      "|       transformer.layer.11.ff.layer_2.bias      |    768     |    35.3199    |\n",
      "|      transformer.layer.11.ff.layer_2.weight     |  2359296   |   40762.7695  |\n",
      "|          sequence_summary.summary.bias          |    768     |     2.2859    |\n",
      "|         sequence_summary.summary.weight         |   589824   |   8690.1855   |\n",
      "|                 logits_proj.bias                |     6      |     0.0107    |\n",
      "|                logits_proj.weight               |    4608    |     56.202    |\n",
      "|              Total Trainable Params             | 117313542  |   3524908.5   |\n",
      "+-------------------------------------------------+------------+---------------+\n",
      "------------------------------ pruned xlnet-base-cased metrics ------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 value: 0.7110512444419043\n",
      "accuracy value: 0.712349908942186\n",
      "\n",
      "------------------------------ half xlnet-base-cased ------------------------------\n",
      "model was saved\n",
      "------------------------------ half xlnet-base-cased weights ------------------------------\n",
      "+-------------------------------------------------+------------+---------------+\n",
      "|                     Modules                     | Parameters | Sum of Tensor |\n",
      "+-------------------------------------------------+------------+---------------+\n",
      "|               transformer.mask_emb              |    768     |     7.2539    |\n",
      "|        transformer.word_embedding.weight        |  24576000  |      inf      |\n",
      "|          transformer.layer.0.rel_attn.q         |   589824   |    44512.0    |\n",
      "|          transformer.layer.0.rel_attn.k         |   589824   |      inf      |\n",
      "|          transformer.layer.0.rel_attn.v         |   589824   |    15096.0    |\n",
      "|          transformer.layer.0.rel_attn.o         |   589824   |    16120.0    |\n",
      "|          transformer.layer.0.rel_attn.r         |   589824   |    10848.0    |\n",
      "|      transformer.layer.0.rel_attn.r_r_bias      |    768     |    117.9375   |\n",
      "|      transformer.layer.0.rel_attn.r_s_bias      |    768     |     136.0     |\n",
      "|      transformer.layer.0.rel_attn.r_w_bias      |    768     |    158.875    |\n",
      "|      transformer.layer.0.rel_attn.seg_embed     |    1536    |     355.0     |\n",
      "|  transformer.layer.0.rel_attn.layer_norm.weight |    768     |     739.5     |\n",
      "|   transformer.layer.0.rel_attn.layer_norm.bias  |    768     |    93.3125    |\n",
      "|     transformer.layer.0.ff.layer_norm.weight    |    768     |     818.0     |\n",
      "|      transformer.layer.0.ff.layer_norm.bias     |    768     |     89.125    |\n",
      "|      transformer.layer.0.ff.layer_1.weight      |  2359296   |      inf      |\n",
      "|       transformer.layer.0.ff.layer_1.bias       |    3072    |     360.25    |\n",
      "|      transformer.layer.0.ff.layer_2.weight      |  2359296   |    57184.0    |\n",
      "|       transformer.layer.0.ff.layer_2.bias       |    768     |    30.7969    |\n",
      "|          transformer.layer.1.rel_attn.q         |   589824   |    16624.0    |\n",
      "|          transformer.layer.1.rel_attn.k         |   589824   |    20352.0    |\n",
      "|          transformer.layer.1.rel_attn.v         |   589824   |    15448.0    |\n",
      "|          transformer.layer.1.rel_attn.o         |   589824   |    15912.0    |\n",
      "|          transformer.layer.1.rel_attn.r         |   589824   |     5368.0    |\n",
      "|      transformer.layer.1.rel_attn.r_r_bias      |    768     |    211.625    |\n",
      "|      transformer.layer.1.rel_attn.r_s_bias      |    768     |     209.25    |\n",
      "|      transformer.layer.1.rel_attn.r_w_bias      |    768     |     128.25    |\n",
      "|      transformer.layer.1.rel_attn.seg_embed     |    1536    |     210.75    |\n",
      "|  transformer.layer.1.rel_attn.layer_norm.weight |    768     |     724.0     |\n",
      "|   transformer.layer.1.rel_attn.layer_norm.bias  |    768     |    111.3125   |\n",
      "|     transformer.layer.1.ff.layer_norm.weight    |    768     |     714.0     |\n",
      "|      transformer.layer.1.ff.layer_norm.bias     |    768     |     115.0     |\n",
      "|      transformer.layer.1.ff.layer_1.weight      |  2359296   |      inf      |\n",
      "|       transformer.layer.1.ff.layer_1.bias       |    3072    |     339.25    |\n",
      "|      transformer.layer.1.ff.layer_2.weight      |  2359296   |    61344.0    |\n",
      "|       transformer.layer.1.ff.layer_2.bias       |    768     |    35.8125    |\n",
      "|          transformer.layer.2.rel_attn.q         |   589824   |    16496.0    |\n",
      "|          transformer.layer.2.rel_attn.k         |   589824   |    19952.0    |\n",
      "|          transformer.layer.2.rel_attn.v         |   589824   |    15808.0    |\n",
      "|          transformer.layer.2.rel_attn.o         |   589824   |    16024.0    |\n",
      "|          transformer.layer.2.rel_attn.r         |   589824   |     5340.0    |\n",
      "|      transformer.layer.2.rel_attn.r_r_bias      |    768     |    247.875    |\n",
      "|      transformer.layer.2.rel_attn.r_s_bias      |    768     |    197.375    |\n",
      "|      transformer.layer.2.rel_attn.r_w_bias      |    768     |    112.4375   |\n",
      "|      transformer.layer.2.rel_attn.seg_embed     |    1536    |     236.25    |\n",
      "|  transformer.layer.2.rel_attn.layer_norm.weight |    768     |     707.5     |\n",
      "|   transformer.layer.2.rel_attn.layer_norm.bias  |    768     |    123.1875   |\n",
      "|     transformer.layer.2.ff.layer_norm.weight    |    768     |     832.0     |\n",
      "|      transformer.layer.2.ff.layer_norm.bias     |    768     |     95.875    |\n",
      "|      transformer.layer.2.ff.layer_1.weight      |  2359296   |      inf      |\n",
      "|       transformer.layer.2.ff.layer_1.bias       |    3072    |     327.75    |\n",
      "|      transformer.layer.2.ff.layer_2.weight      |  2359296   |    60544.0    |\n",
      "|       transformer.layer.2.ff.layer_2.bias       |    768     |    46.5312    |\n",
      "|          transformer.layer.3.rel_attn.q         |   589824   |    14200.0    |\n",
      "|          transformer.layer.3.rel_attn.k         |   589824   |    18448.0    |\n",
      "|          transformer.layer.3.rel_attn.v         |   589824   |    16344.0    |\n",
      "|          transformer.layer.3.rel_attn.o         |   589824   |    16104.0    |\n",
      "|          transformer.layer.3.rel_attn.r         |   589824   |     5740.0    |\n",
      "|      transformer.layer.3.rel_attn.r_r_bias      |    768     |     300.25    |\n",
      "|      transformer.layer.3.rel_attn.r_s_bias      |    768     |     241.75    |\n",
      "|      transformer.layer.3.rel_attn.r_w_bias      |    768     |     248.25    |\n",
      "|      transformer.layer.3.rel_attn.seg_embed     |    1536    |    227.125    |\n",
      "|  transformer.layer.3.rel_attn.layer_norm.weight |    768     |     695.5     |\n",
      "|   transformer.layer.3.rel_attn.layer_norm.bias  |    768     |     110.25    |\n",
      "|     transformer.layer.3.ff.layer_norm.weight    |    768     |     859.5     |\n",
      "|      transformer.layer.3.ff.layer_norm.bias     |    768     |    142.625    |\n",
      "|      transformer.layer.3.ff.layer_1.weight      |  2359296   |      inf      |\n",
      "|       transformer.layer.3.ff.layer_1.bias       |    3072    |     338.0     |\n",
      "|      transformer.layer.3.ff.layer_2.weight      |  2359296   |    59936.0    |\n",
      "|       transformer.layer.3.ff.layer_2.bias       |    768     |      46.5     |\n",
      "|          transformer.layer.4.rel_attn.q         |   589824   |    16040.0    |\n",
      "|          transformer.layer.4.rel_attn.k         |   589824   |    18272.0    |\n",
      "|          transformer.layer.4.rel_attn.v         |   589824   |    16336.0    |\n",
      "|          transformer.layer.4.rel_attn.o         |   589824   |    16672.0    |\n",
      "|          transformer.layer.4.rel_attn.r         |   589824   |     5012.0    |\n",
      "|      transformer.layer.4.rel_attn.r_r_bias      |    768     |    149.125    |\n",
      "|      transformer.layer.4.rel_attn.r_s_bias      |    768     |     159.75    |\n",
      "|      transformer.layer.4.rel_attn.r_w_bias      |    768     |    85.6875    |\n",
      "|      transformer.layer.4.rel_attn.seg_embed     |    1536    |    193.125    |\n",
      "|  transformer.layer.4.rel_attn.layer_norm.weight |    768     |     697.0     |\n",
      "|   transformer.layer.4.rel_attn.layer_norm.bias  |    768     |    119.125    |\n",
      "|     transformer.layer.4.ff.layer_norm.weight    |    768     |     843.0     |\n",
      "|      transformer.layer.4.ff.layer_norm.bias     |    768     |    74.3125    |\n",
      "|      transformer.layer.4.ff.layer_1.weight      |  2359296   |      inf      |\n",
      "|       transformer.layer.4.ff.layer_1.bias       |    3072    |     289.25    |\n",
      "|      transformer.layer.4.ff.layer_2.weight      |  2359296   |    59456.0    |\n",
      "|       transformer.layer.4.ff.layer_2.bias       |    768     |     52.875    |\n",
      "|          transformer.layer.5.rel_attn.q         |   589824   |    20064.0    |\n",
      "|          transformer.layer.5.rel_attn.k         |   589824   |    22944.0    |\n",
      "|          transformer.layer.5.rel_attn.v         |   589824   |    16008.0    |\n",
      "|          transformer.layer.5.rel_attn.o         |   589824   |    15616.0    |\n",
      "|          transformer.layer.5.rel_attn.r         |   589824   |     5892.0    |\n",
      "|      transformer.layer.5.rel_attn.r_r_bias      |    768     |    135.125    |\n",
      "|      transformer.layer.5.rel_attn.r_s_bias      |    768     |    144.125    |\n",
      "|      transformer.layer.5.rel_attn.r_w_bias      |    768     |    91.5625    |\n",
      "|      transformer.layer.5.rel_attn.seg_embed     |    1536    |    191.375    |\n",
      "|  transformer.layer.5.rel_attn.layer_norm.weight |    768     |     710.0     |\n",
      "|   transformer.layer.5.rel_attn.layer_norm.bias  |    768     |      67.5     |\n",
      "|     transformer.layer.5.ff.layer_norm.weight    |    768     |     1049.0    |\n",
      "|      transformer.layer.5.ff.layer_norm.bias     |    768     |     112.0     |\n",
      "|      transformer.layer.5.ff.layer_1.weight      |  2359296   |      inf      |\n",
      "|       transformer.layer.5.ff.layer_1.bias       |    3072    |     309.25    |\n",
      "|      transformer.layer.5.ff.layer_2.weight      |  2359296   |    57280.0    |\n",
      "|       transformer.layer.5.ff.layer_2.bias       |    768     |    48.5312    |\n",
      "|          transformer.layer.6.rel_attn.q         |   589824   |    17456.0    |\n",
      "|          transformer.layer.6.rel_attn.k         |   589824   |    20016.0    |\n",
      "|          transformer.layer.6.rel_attn.v         |   589824   |    16416.0    |\n",
      "|          transformer.layer.6.rel_attn.o         |   589824   |    15960.0    |\n",
      "|          transformer.layer.6.rel_attn.r         |   589824   |     5372.0    |\n",
      "|      transformer.layer.6.rel_attn.r_r_bias      |    768     |      79.5     |\n",
      "|      transformer.layer.6.rel_attn.r_s_bias      |    768     |     143.25    |\n",
      "|      transformer.layer.6.rel_attn.r_w_bias      |    768     |    74.8125    |\n",
      "|      transformer.layer.6.rel_attn.seg_embed     |    1536    |     123.75    |\n",
      "|  transformer.layer.6.rel_attn.layer_norm.weight |    768     |     825.0     |\n",
      "|   transformer.layer.6.rel_attn.layer_norm.bias  |    768     |     100.5     |\n",
      "|     transformer.layer.6.ff.layer_norm.weight    |    768     |     1045.0    |\n",
      "|      transformer.layer.6.ff.layer_norm.bias     |    768     |      77.5     |\n",
      "|      transformer.layer.6.ff.layer_1.weight      |  2359296   |      inf      |\n",
      "|       transformer.layer.6.ff.layer_1.bias       |    3072    |     286.25    |\n",
      "|      transformer.layer.6.ff.layer_2.weight      |  2359296   |    55264.0    |\n",
      "|       transformer.layer.6.ff.layer_2.bias       |    768     |    54.1875    |\n",
      "|          transformer.layer.7.rel_attn.q         |   589824   |    17520.0    |\n",
      "|          transformer.layer.7.rel_attn.k         |   589824   |    20384.0    |\n",
      "|          transformer.layer.7.rel_attn.v         |   589824   |    16736.0    |\n",
      "|          transformer.layer.7.rel_attn.o         |   589824   |    15744.0    |\n",
      "|          transformer.layer.7.rel_attn.r         |   589824   |     5180.0    |\n",
      "|      transformer.layer.7.rel_attn.r_r_bias      |    768     |    86.9375    |\n",
      "|      transformer.layer.7.rel_attn.r_s_bias      |    768     |     141.5     |\n",
      "|      transformer.layer.7.rel_attn.r_w_bias      |    768     |    53.3125    |\n",
      "|      transformer.layer.7.rel_attn.seg_embed     |    1536    |     147.0     |\n",
      "|  transformer.layer.7.rel_attn.layer_norm.weight |    768     |     859.0     |\n",
      "|   transformer.layer.7.rel_attn.layer_norm.bias  |    768     |      75.5     |\n",
      "|     transformer.layer.7.ff.layer_norm.weight    |    768     |     1135.0    |\n",
      "|      transformer.layer.7.ff.layer_norm.bias     |    768     |    86.6875    |\n",
      "|      transformer.layer.7.ff.layer_1.weight      |  2359296   |      inf      |\n",
      "|       transformer.layer.7.ff.layer_1.bias       |    3072    |     301.5     |\n",
      "|      transformer.layer.7.ff.layer_2.weight      |  2359296   |    53536.0    |\n",
      "|       transformer.layer.7.ff.layer_2.bias       |    768     |     58.75     |\n",
      "|          transformer.layer.8.rel_attn.q         |   589824   |    17312.0    |\n",
      "|          transformer.layer.8.rel_attn.k         |   589824   |    20112.0    |\n",
      "|          transformer.layer.8.rel_attn.v         |   589824   |    17488.0    |\n",
      "|          transformer.layer.8.rel_attn.o         |   589824   |    16640.0    |\n",
      "|          transformer.layer.8.rel_attn.r         |   589824   |     4996.0    |\n",
      "|      transformer.layer.8.rel_attn.r_r_bias      |    768     |    76.3125    |\n",
      "|      transformer.layer.8.rel_attn.r_s_bias      |    768     |    104.5625   |\n",
      "|      transformer.layer.8.rel_attn.r_w_bias      |    768     |     50.625    |\n",
      "|      transformer.layer.8.rel_attn.seg_embed     |    1536    |    131.875    |\n",
      "|  transformer.layer.8.rel_attn.layer_norm.weight |    768     |     1081.0    |\n",
      "|   transformer.layer.8.rel_attn.layer_norm.bias  |    768     |     68.875    |\n",
      "|     transformer.layer.8.ff.layer_norm.weight    |    768     |     1077.0    |\n",
      "|      transformer.layer.8.ff.layer_norm.bias     |    768     |     70.75     |\n",
      "|      transformer.layer.8.ff.layer_1.weight      |  2359296   |      inf      |\n",
      "|       transformer.layer.8.ff.layer_1.bias       |    3072    |     258.25    |\n",
      "|      transformer.layer.8.ff.layer_2.weight      |  2359296   |    49920.0    |\n",
      "|       transformer.layer.8.ff.layer_2.bias       |    768     |    48.4062    |\n",
      "|          transformer.layer.9.rel_attn.q         |   589824   |    19120.0    |\n",
      "|          transformer.layer.9.rel_attn.k         |   589824   |    22512.0    |\n",
      "|          transformer.layer.9.rel_attn.v         |   589824   |    17776.0    |\n",
      "|          transformer.layer.9.rel_attn.o         |   589824   |    16200.0    |\n",
      "|          transformer.layer.9.rel_attn.r         |   589824   |     4872.0    |\n",
      "|      transformer.layer.9.rel_attn.r_r_bias      |    768     |    71.5625    |\n",
      "|      transformer.layer.9.rel_attn.r_s_bias      |    768     |     115.75    |\n",
      "|      transformer.layer.9.rel_attn.r_w_bias      |    768     |    66.1875    |\n",
      "|      transformer.layer.9.rel_attn.seg_embed     |    1536    |    114.4375   |\n",
      "|  transformer.layer.9.rel_attn.layer_norm.weight |    768     |     1043.0    |\n",
      "|   transformer.layer.9.rel_attn.layer_norm.bias  |    768     |     53.625    |\n",
      "|     transformer.layer.9.ff.layer_norm.weight    |    768     |     1180.0    |\n",
      "|      transformer.layer.9.ff.layer_norm.bias     |    768     |    80.6875    |\n",
      "|      transformer.layer.9.ff.layer_1.weight      |  2359296   |      inf      |\n",
      "|       transformer.layer.9.ff.layer_1.bias       |    3072    |     292.25    |\n",
      "|      transformer.layer.9.ff.layer_2.weight      |  2359296   |    52384.0    |\n",
      "|       transformer.layer.9.ff.layer_2.bias       |    768     |    45.2188    |\n",
      "|         transformer.layer.10.rel_attn.q         |   589824   |    15600.0    |\n",
      "|         transformer.layer.10.rel_attn.k         |   589824   |    18656.0    |\n",
      "|         transformer.layer.10.rel_attn.v         |   589824   |    19200.0    |\n",
      "|         transformer.layer.10.rel_attn.o         |   589824   |    17648.0    |\n",
      "|         transformer.layer.10.rel_attn.r         |   589824   |     4348.0    |\n",
      "|      transformer.layer.10.rel_attn.r_r_bias     |    768     |    94.0625    |\n",
      "|      transformer.layer.10.rel_attn.r_s_bias     |    768     |    110.1875   |\n",
      "|      transformer.layer.10.rel_attn.r_w_bias     |    768     |    93.5625    |\n",
      "|     transformer.layer.10.rel_attn.seg_embed     |    1536    |    111.9375   |\n",
      "| transformer.layer.10.rel_attn.layer_norm.weight |    768     |     1054.0    |\n",
      "|  transformer.layer.10.rel_attn.layer_norm.bias  |    768     |     61.75     |\n",
      "|    transformer.layer.10.ff.layer_norm.weight    |    768     |     1021.5    |\n",
      "|     transformer.layer.10.ff.layer_norm.bias     |    768     |    63.3125    |\n",
      "|      transformer.layer.10.ff.layer_1.weight     |  2359296   |      inf      |\n",
      "|       transformer.layer.10.ff.layer_1.bias      |    3072    |     258.5     |\n",
      "|      transformer.layer.10.ff.layer_2.weight     |  2359296   |    50208.0    |\n",
      "|       transformer.layer.10.ff.layer_2.bias      |    768     |    41.4375    |\n",
      "|         transformer.layer.11.rel_attn.q         |   589824   |    18400.0    |\n",
      "|         transformer.layer.11.rel_attn.k         |   589824   |    23456.0    |\n",
      "|         transformer.layer.11.rel_attn.v         |   589824   |    24128.0    |\n",
      "|         transformer.layer.11.rel_attn.o         |   589824   |    20704.0    |\n",
      "|         transformer.layer.11.rel_attn.r         |   589824   |     3692.0    |\n",
      "|      transformer.layer.11.rel_attn.r_r_bias     |    768     |    93.5625    |\n",
      "|      transformer.layer.11.rel_attn.r_s_bias     |    768     |     76.125    |\n",
      "|      transformer.layer.11.rel_attn.r_w_bias     |    768     |    94.9375    |\n",
      "|     transformer.layer.11.rel_attn.seg_embed     |    1536    |    84.3125    |\n",
      "| transformer.layer.11.rel_attn.layer_norm.weight |    768     |     1216.0    |\n",
      "|  transformer.layer.11.rel_attn.layer_norm.bias  |    768     |    76.1875    |\n",
      "|    transformer.layer.11.ff.layer_norm.weight    |    768     |     1858.0    |\n",
      "|     transformer.layer.11.ff.layer_norm.bias     |    768     |     135.25    |\n",
      "|      transformer.layer.11.ff.layer_1.weight     |  2359296   |      inf      |\n",
      "|       transformer.layer.11.ff.layer_1.bias      |    3072    |     300.0     |\n",
      "|      transformer.layer.11.ff.layer_2.weight     |  2359296   |    45280.0    |\n",
      "|       transformer.layer.11.ff.layer_2.bias      |    768     |    35.9062    |\n",
      "|         sequence_summary.summary.weight         |   589824   |     9456.0    |\n",
      "|          sequence_summary.summary.bias          |    768     |      0.77     |\n",
      "|                logits_proj.weight               |    4608    |    78.1875    |\n",
      "|                 logits_proj.bias                |     6      |     0.0063    |\n",
      "|              Total Trainable Params             | 117313542  |      inf      |\n",
      "+-------------------------------------------------+------------+---------------+\n",
      "------------------------------ half xlnet-base-cased metrics ------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 value: 0.7080762324706387\n",
      "accuracy value: 0.7101171919210626\n",
      "\n",
      "=============================================================================\n",
      "============================== bert-base-uncased ==============================\n",
      "=============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at C:\\Users\\liyag\\.cache\\huggingface\\datasets\\csv\\default-2e78edd390e366fa\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b6144e5409c32750.arrow\n",
      "Loading cached processed dataset at C:\\Users\\liyag\\.cache\\huggingface\\datasets\\csv\\default-2e78edd390e366fa\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-c41e60853528f9c0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ regular bert-base-uncased ------------------------------\n",
      "model was loaded\n",
      "------------------------------ bert-base-uncased weights ------------------------------\n",
      "+---------------------------------------------------------+------------+---------------+\n",
      "|                         Modules                         | Parameters | Sum of Tensor |\n",
      "+---------------------------------------------------------+------------+---------------+\n",
      "|          bert.embeddings.word_embeddings.weight         |  23440896  |  955872.1875  |\n",
      "|        bert.embeddings.position_embeddings.weight       |   393216   |    4577.082   |\n",
      "|       bert.embeddings.token_type_embeddings.weight      |    1536    |    13.9256    |\n",
      "|             bert.embeddings.LayerNorm.weight            |    768     |    652.7019   |\n",
      "|              bert.embeddings.LayerNorm.bias             |    768     |    31.7363    |\n",
      "|     bert.encoder.layer.0.attention.self.query.weight    |   589824   |   20028.5293  |\n",
      "|      bert.encoder.layer.0.attention.self.query.bias     |    768     |    167.1584   |\n",
      "|      bert.encoder.layer.0.attention.self.key.weight     |   589824   |   19688.5469  |\n",
      "|       bert.encoder.layer.0.attention.self.key.bias      |    768     |     1.8726    |\n",
      "|     bert.encoder.layer.0.attention.self.value.weight    |   589824   |   13483.459   |\n",
      "|      bert.encoder.layer.0.attention.self.value.bias     |    768     |    19.8902    |\n",
      "|    bert.encoder.layer.0.attention.output.dense.weight   |   589824   |   13041.502   |\n",
      "|     bert.encoder.layer.0.attention.output.dense.bias    |    768     |    19.1094    |\n",
      "|  bert.encoder.layer.0.attention.output.LayerNorm.weight |    768     |    736.7421   |\n",
      "|   bert.encoder.layer.0.attention.output.LayerNorm.bias  |    768     |    153.808    |\n",
      "|      bert.encoder.layer.0.intermediate.dense.weight     |  2359296   |   70574.1094  |\n",
      "|       bert.encoder.layer.0.intermediate.dense.bias      |    3072    |    347.1645   |\n",
      "|         bert.encoder.layer.0.output.dense.weight        |  2359296   |   66433.7188  |\n",
      "|          bert.encoder.layer.0.output.dense.bias         |    768     |    44.2128    |\n",
      "|       bert.encoder.layer.0.output.LayerNorm.weight      |    768     |    581.1021   |\n",
      "|        bert.encoder.layer.0.output.LayerNorm.bias       |    768     |    56.6882    |\n",
      "|     bert.encoder.layer.1.attention.self.query.weight    |   589824   |   19865.168   |\n",
      "|      bert.encoder.layer.1.attention.self.query.bias     |    768     |    86.6469    |\n",
      "|      bert.encoder.layer.1.attention.self.key.weight     |   589824   |   19779.4844  |\n",
      "|       bert.encoder.layer.1.attention.self.key.bias      |    768     |     2.7955    |\n",
      "|     bert.encoder.layer.1.attention.self.value.weight    |   589824   |   13284.4414  |\n",
      "|      bert.encoder.layer.1.attention.self.value.bias     |    768     |    18.3231    |\n",
      "|    bert.encoder.layer.1.attention.output.dense.weight   |   589824   |   12785.3408  |\n",
      "|     bert.encoder.layer.1.attention.output.dense.bias    |    768     |    35.0339    |\n",
      "|  bert.encoder.layer.1.attention.output.LayerNorm.weight |    768     |    674.0969   |\n",
      "|   bert.encoder.layer.1.attention.output.LayerNorm.bias  |    768     |    95.9663    |\n",
      "|      bert.encoder.layer.1.intermediate.dense.weight     |  2359296   |   73821.6875  |\n",
      "|       bert.encoder.layer.1.intermediate.dense.bias      |    3072    |    312.738    |\n",
      "|         bert.encoder.layer.1.output.dense.weight        |  2359296   |   69723.8438  |\n",
      "|          bert.encoder.layer.1.output.dense.bias         |    768     |    35.4701    |\n",
      "|       bert.encoder.layer.1.output.LayerNorm.weight      |    768     |    668.4733   |\n",
      "|        bert.encoder.layer.1.output.LayerNorm.bias       |    768     |    54.3538    |\n",
      "|     bert.encoder.layer.2.attention.self.query.weight    |   589824   |   21745.498   |\n",
      "|      bert.encoder.layer.2.attention.self.query.bias     |    768     |    72.3464    |\n",
      "|      bert.encoder.layer.2.attention.self.key.weight     |   589824   |   21244.5898  |\n",
      "|       bert.encoder.layer.2.attention.self.key.bias      |    768     |     2.3018    |\n",
      "|     bert.encoder.layer.2.attention.self.value.weight    |   589824   |   13073.9082  |\n",
      "|      bert.encoder.layer.2.attention.self.value.bias     |    768     |    24.4538    |\n",
      "|    bert.encoder.layer.2.attention.output.dense.weight   |   589824   |   12550.2734  |\n",
      "|     bert.encoder.layer.2.attention.output.dense.bias    |    768     |    50.5331    |\n",
      "|  bert.encoder.layer.2.attention.output.LayerNorm.weight |    768     |    667.9754   |\n",
      "|   bert.encoder.layer.2.attention.output.LayerNorm.bias  |    768     |    95.7135    |\n",
      "|      bert.encoder.layer.2.intermediate.dense.weight     |  2359296   |   74872.5156  |\n",
      "|       bert.encoder.layer.2.intermediate.dense.bias      |    3072    |    325.0009   |\n",
      "|         bert.encoder.layer.2.output.dense.weight        |  2359296   |   70590.1016  |\n",
      "|          bert.encoder.layer.2.output.dense.bias         |    768     |    41.7743    |\n",
      "|       bert.encoder.layer.2.output.LayerNorm.weight      |    768     |    654.642    |\n",
      "|        bert.encoder.layer.2.output.LayerNorm.bias       |    768     |    49.2222    |\n",
      "|     bert.encoder.layer.3.attention.self.query.weight    |   589824   |   20137.668   |\n",
      "|      bert.encoder.layer.3.attention.self.query.bias     |    768     |    69.8848    |\n",
      "|      bert.encoder.layer.3.attention.self.key.weight     |   589824   |   20002.4551  |\n",
      "|       bert.encoder.layer.3.attention.self.key.bias      |    768     |     2.7587    |\n",
      "|     bert.encoder.layer.3.attention.self.value.weight    |   589824   |   14548.2344  |\n",
      "|      bert.encoder.layer.3.attention.self.value.bias     |    768     |    16.1418    |\n",
      "|    bert.encoder.layer.3.attention.output.dense.weight   |   589824   |   13480.0713  |\n",
      "|     bert.encoder.layer.3.attention.output.dense.bias    |    768     |    42.4292    |\n",
      "|  bert.encoder.layer.3.attention.output.LayerNorm.weight |    768     |    664.456    |\n",
      "|   bert.encoder.layer.3.attention.output.LayerNorm.bias  |    768     |    88.5838    |\n",
      "|      bert.encoder.layer.3.intermediate.dense.weight     |  2359296   |   75918.4375  |\n",
      "|       bert.encoder.layer.3.intermediate.dense.bias      |    3072    |    315.8936   |\n",
      "|         bert.encoder.layer.3.output.dense.weight        |  2359296   |   71775.4766  |\n",
      "|          bert.encoder.layer.3.output.dense.bias         |    768     |    41.4417    |\n",
      "|       bert.encoder.layer.3.output.LayerNorm.weight      |    768     |    623.7876   |\n",
      "|        bert.encoder.layer.3.output.LayerNorm.bias       |    768     |    37.5538    |\n",
      "|     bert.encoder.layer.4.attention.self.query.weight    |   589824   |   19775.7402  |\n",
      "|      bert.encoder.layer.4.attention.self.query.bias     |    768     |     79.606    |\n",
      "|      bert.encoder.layer.4.attention.self.key.weight     |   589824   |   19661.0352  |\n",
      "|       bert.encoder.layer.4.attention.self.key.bias      |    768     |     2.7924    |\n",
      "|     bert.encoder.layer.4.attention.self.value.weight    |   589824   |   16330.3994  |\n",
      "|      bert.encoder.layer.4.attention.self.value.bias     |    768     |    14.3952    |\n",
      "|    bert.encoder.layer.4.attention.output.dense.weight   |   589824   |   15085.4043  |\n",
      "|     bert.encoder.layer.4.attention.output.dense.bias    |    768     |    22.2512    |\n",
      "|  bert.encoder.layer.4.attention.output.LayerNorm.weight |    768     |    646.472    |\n",
      "|   bert.encoder.layer.4.attention.output.LayerNorm.bias  |    768     |    77.7614    |\n",
      "|      bert.encoder.layer.4.intermediate.dense.weight     |  2359296   |   76398.9062  |\n",
      "|       bert.encoder.layer.4.intermediate.dense.bias      |    3072    |    328.5191   |\n",
      "|         bert.encoder.layer.4.output.dense.weight        |  2359296   |   72692.6406  |\n",
      "|          bert.encoder.layer.4.output.dense.bias         |    768     |    31.5857    |\n",
      "|       bert.encoder.layer.4.output.LayerNorm.weight      |    768     |    645.8607   |\n",
      "|        bert.encoder.layer.4.output.LayerNorm.bias       |    768     |    35.7282    |\n",
      "|     bert.encoder.layer.5.attention.self.query.weight    |   589824   |   20381.0879  |\n",
      "|      bert.encoder.layer.5.attention.self.query.bias     |    768     |    63.5214    |\n",
      "|      bert.encoder.layer.5.attention.self.key.weight     |   589824   |   20346.1836  |\n",
      "|       bert.encoder.layer.5.attention.self.key.bias      |    768     |     3.4997    |\n",
      "|     bert.encoder.layer.5.attention.self.value.weight    |   589824   |   16283.8984  |\n",
      "|      bert.encoder.layer.5.attention.self.value.bias     |    768     |    15.7276    |\n",
      "|    bert.encoder.layer.5.attention.output.dense.weight   |   589824   |   15383.6055  |\n",
      "|     bert.encoder.layer.5.attention.output.dense.bias    |    768     |    20.2823    |\n",
      "|  bert.encoder.layer.5.attention.output.LayerNorm.weight |    768     |    652.7899   |\n",
      "|   bert.encoder.layer.5.attention.output.LayerNorm.bias  |    768     |    75.2953    |\n",
      "|      bert.encoder.layer.5.intermediate.dense.weight     |  2359296   |   76113.8125  |\n",
      "|       bert.encoder.layer.5.intermediate.dense.bias      |    3072    |    322.708    |\n",
      "|         bert.encoder.layer.5.output.dense.weight        |  2359296   |   71797.2656  |\n",
      "|          bert.encoder.layer.5.output.dense.bias         |    768     |    31.9916    |\n",
      "|       bert.encoder.layer.5.output.LayerNorm.weight      |    768     |    640.3918   |\n",
      "|        bert.encoder.layer.5.output.LayerNorm.bias       |    768     |    39.6545    |\n",
      "|     bert.encoder.layer.6.attention.self.query.weight    |   589824   |   20259.0156  |\n",
      "|      bert.encoder.layer.6.attention.self.query.bias     |    768     |    67.0877    |\n",
      "|      bert.encoder.layer.6.attention.self.key.weight     |   589824   |   20221.4297  |\n",
      "|       bert.encoder.layer.6.attention.self.key.bias      |    768     |     3.1424    |\n",
      "|     bert.encoder.layer.6.attention.self.value.weight    |   589824   |   15906.0625  |\n",
      "|      bert.encoder.layer.6.attention.self.value.bias     |    768     |    19.1875    |\n",
      "|    bert.encoder.layer.6.attention.output.dense.weight   |   589824   |   14973.3818  |\n",
      "|     bert.encoder.layer.6.attention.output.dense.bias    |    768     |    24.2693    |\n",
      "|  bert.encoder.layer.6.attention.output.LayerNorm.weight |    768     |    638.5289   |\n",
      "|   bert.encoder.layer.6.attention.output.LayerNorm.bias  |    768     |    73.0932    |\n",
      "|      bert.encoder.layer.6.intermediate.dense.weight     |  2359296   |   76620.875   |\n",
      "|       bert.encoder.layer.6.intermediate.dense.bias      |    3072    |    312.2703   |\n",
      "|         bert.encoder.layer.6.output.dense.weight        |  2359296   |   71295.5938  |\n",
      "|          bert.encoder.layer.6.output.dense.bias         |    768     |    40.4945    |\n",
      "|       bert.encoder.layer.6.output.LayerNorm.weight      |    768     |    641.578    |\n",
      "|        bert.encoder.layer.6.output.LayerNorm.bias       |    768     |    40.7075    |\n",
      "|     bert.encoder.layer.7.attention.self.query.weight    |   589824   |   20434.5117  |\n",
      "|      bert.encoder.layer.7.attention.self.query.bias     |    768     |    75.4389    |\n",
      "|      bert.encoder.layer.7.attention.self.key.weight     |   589824   |   20480.7188  |\n",
      "|       bert.encoder.layer.7.attention.self.key.bias      |    768     |     3.187     |\n",
      "|     bert.encoder.layer.7.attention.self.value.weight    |   589824   |   15458.998   |\n",
      "|      bert.encoder.layer.7.attention.self.value.bias     |    768     |    22.9986    |\n",
      "|    bert.encoder.layer.7.attention.output.dense.weight   |   589824   |   14816.877   |\n",
      "|     bert.encoder.layer.7.attention.output.dense.bias    |    768     |    23.4668    |\n",
      "|  bert.encoder.layer.7.attention.output.LayerNorm.weight |    768     |    634.6394   |\n",
      "|   bert.encoder.layer.7.attention.output.LayerNorm.bias  |    768     |    78.1085    |\n",
      "|      bert.encoder.layer.7.intermediate.dense.weight     |  2359296   |   73928.8516  |\n",
      "|       bert.encoder.layer.7.intermediate.dense.bias      |    3072    |    331.5463   |\n",
      "|         bert.encoder.layer.7.output.dense.weight        |  2359296   |   69434.2891  |\n",
      "|          bert.encoder.layer.7.output.dense.bias         |    768     |    46.0416    |\n",
      "|       bert.encoder.layer.7.output.LayerNorm.weight      |    768     |    623.5295   |\n",
      "|        bert.encoder.layer.7.output.LayerNorm.bias       |    768     |    39.3849    |\n",
      "|     bert.encoder.layer.8.attention.self.query.weight    |   589824   |   20727.0605  |\n",
      "|      bert.encoder.layer.8.attention.self.query.bias     |    768     |    96.3741    |\n",
      "|      bert.encoder.layer.8.attention.self.key.weight     |   589824   |   20760.668   |\n",
      "|       bert.encoder.layer.8.attention.self.key.bias      |    768     |     4.0098    |\n",
      "|     bert.encoder.layer.8.attention.self.value.weight    |   589824   |   16632.2598  |\n",
      "|      bert.encoder.layer.8.attention.self.value.bias     |    768     |    21.0117    |\n",
      "|    bert.encoder.layer.8.attention.output.dense.weight   |   589824   |   15689.9092  |\n",
      "|     bert.encoder.layer.8.attention.output.dense.bias    |    768     |    28.4987    |\n",
      "|  bert.encoder.layer.8.attention.output.LayerNorm.weight |    768     |    645.7555   |\n",
      "|   bert.encoder.layer.8.attention.output.LayerNorm.bias  |    768     |    72.9835    |\n",
      "|      bert.encoder.layer.8.intermediate.dense.weight     |  2359296   |   73969.9688  |\n",
      "|       bert.encoder.layer.8.intermediate.dense.bias      |    3072    |    319.9904   |\n",
      "|         bert.encoder.layer.8.output.dense.weight        |  2359296   |   69333.2969  |\n",
      "|          bert.encoder.layer.8.output.dense.bias         |    768     |    47.3477    |\n",
      "|       bert.encoder.layer.8.output.LayerNorm.weight      |    768     |    638.6874   |\n",
      "|        bert.encoder.layer.8.output.LayerNorm.bias       |    768     |    41.5431    |\n",
      "|     bert.encoder.layer.9.attention.self.query.weight    |   589824   |   21632.6133  |\n",
      "|      bert.encoder.layer.9.attention.self.query.bias     |    768     |    108.3274   |\n",
      "|      bert.encoder.layer.9.attention.self.key.weight     |   589824   |   21571.6133  |\n",
      "|       bert.encoder.layer.9.attention.self.key.bias      |    768     |     3.9467    |\n",
      "|     bert.encoder.layer.9.attention.self.value.weight    |   589824   |   16224.5713  |\n",
      "|      bert.encoder.layer.9.attention.self.value.bias     |    768     |    20.1136    |\n",
      "|    bert.encoder.layer.9.attention.output.dense.weight   |   589824   |   15372.8428  |\n",
      "|     bert.encoder.layer.9.attention.output.dense.bias    |    768     |    35.0712    |\n",
      "|  bert.encoder.layer.9.attention.output.LayerNorm.weight |    768     |    630.1633   |\n",
      "|   bert.encoder.layer.9.attention.output.LayerNorm.bias  |    768     |    72.8558    |\n",
      "|      bert.encoder.layer.9.intermediate.dense.weight     |  2359296   |   74594.3281  |\n",
      "|       bert.encoder.layer.9.intermediate.dense.bias      |    3072    |    335.3336   |\n",
      "|         bert.encoder.layer.9.output.dense.weight        |  2359296   |   71697.6719  |\n",
      "|          bert.encoder.layer.9.output.dense.bias         |    768     |    45.3287    |\n",
      "|       bert.encoder.layer.9.output.LayerNorm.weight      |    768     |    614.9038   |\n",
      "|        bert.encoder.layer.9.output.LayerNorm.bias       |    768     |     36.441    |\n",
      "|    bert.encoder.layer.10.attention.self.query.weight    |   589824   |   21587.8828  |\n",
      "|     bert.encoder.layer.10.attention.self.query.bias     |    768     |    106.401    |\n",
      "|     bert.encoder.layer.10.attention.self.key.weight     |   589824   |   21496.9102  |\n",
      "|      bert.encoder.layer.10.attention.self.key.bias      |    768     |     3.7733    |\n",
      "|    bert.encoder.layer.10.attention.self.value.weight    |   589824   |   16522.5156  |\n",
      "|     bert.encoder.layer.10.attention.self.value.bias     |    768     |    13.7612    |\n",
      "|   bert.encoder.layer.10.attention.output.dense.weight   |   589824   |   15596.6279  |\n",
      "|    bert.encoder.layer.10.attention.output.dense.bias    |    768     |    32.2508    |\n",
      "| bert.encoder.layer.10.attention.output.LayerNorm.weight |    768     |    647.7273   |\n",
      "|  bert.encoder.layer.10.attention.output.LayerNorm.bias  |    768     |    59.2629    |\n",
      "|     bert.encoder.layer.10.intermediate.dense.weight     |  2359296   |   73744.0469  |\n",
      "|      bert.encoder.layer.10.intermediate.dense.bias      |    3072    |    332.1095   |\n",
      "|        bert.encoder.layer.10.output.dense.weight        |  2359296   |   70426.7969  |\n",
      "|         bert.encoder.layer.10.output.dense.bias         |    768     |    57.3588    |\n",
      "|      bert.encoder.layer.10.output.LayerNorm.weight      |    768     |    627.3969   |\n",
      "|       bert.encoder.layer.10.output.LayerNorm.bias       |    768     |    38.2564    |\n",
      "|    bert.encoder.layer.11.attention.self.query.weight    |   589824   |   21273.6055  |\n",
      "|     bert.encoder.layer.11.attention.self.query.bias     |    768     |    126.0014   |\n",
      "|     bert.encoder.layer.11.attention.self.key.weight     |   589824   |   20931.7188  |\n",
      "|      bert.encoder.layer.11.attention.self.key.bias      |    768     |     3.0224    |\n",
      "|    bert.encoder.layer.11.attention.self.value.weight    |   589824   |   18334.918   |\n",
      "|     bert.encoder.layer.11.attention.self.value.bias     |    768     |    11.3952    |\n",
      "|   bert.encoder.layer.11.attention.output.dense.weight   |   589824   |   17290.9531  |\n",
      "|    bert.encoder.layer.11.attention.output.dense.bias    |    768     |    22.9623    |\n",
      "| bert.encoder.layer.11.attention.output.LayerNorm.weight |    768     |    655.2746   |\n",
      "|  bert.encoder.layer.11.attention.output.LayerNorm.bias  |    768     |    50.4074    |\n",
      "|     bert.encoder.layer.11.intermediate.dense.weight     |  2359296   |   74162.2656  |\n",
      "|      bert.encoder.layer.11.intermediate.dense.bias      |    3072    |    257.1159   |\n",
      "|        bert.encoder.layer.11.output.dense.weight        |  2359296   |   67212.3281  |\n",
      "|         bert.encoder.layer.11.output.dense.bias         |    768     |    32.8712    |\n",
      "|      bert.encoder.layer.11.output.LayerNorm.weight      |    768     |    485.9475   |\n",
      "|       bert.encoder.layer.11.output.LayerNorm.bias       |    768     |    32.0865    |\n",
      "|                 bert.pooler.dense.weight                |   589824   |   11560.4287  |\n",
      "|                  bert.pooler.dense.bias                 |    768     |    19.6365    |\n",
      "|                    classifier.weight                    |    4608    |    78.2943    |\n",
      "|                     classifier.bias                     |     6      |     0.0218    |\n",
      "|                  Total Trainable Params                 | 109486854  |   3589025.0   |\n",
      "+---------------------------------------------------------+------------+---------------+\n",
      "------------------------------ bert-base-uncased metrics ------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 value: 0.733591514935646\n",
      "accuracy value: 0.734841703449908\n",
      "\n",
      "------------------------------ pruned bert-base-uncased ------------------------------\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query:\n",
      "old_total_weights = 23.72532844543457\n",
      "new_total_weights = 31.292236328125\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key:\n",
      "old_total_weights = 5.339641571044922\n",
      "new_total_weights = 4.593292236328125\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value:\n",
      "old_total_weights = -19.7689151763916\n",
      "new_total_weights = -14.732784271240234\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense:\n",
      "old_total_weights = -11.822534561157227\n",
      "new_total_weights = -5.044295787811279\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense:\n",
      "old_total_weights = -78.56645202636719\n",
      "new_total_weights = -75.34112548828125\n",
      "\n",
      "bert.encoder.layer.0.output.dense:\n",
      "old_total_weights = -199.65719604492188\n",
      "new_total_weights = -181.06861877441406\n",
      "\n",
      "bert.encoder.layer.1.attention.self.query:\n",
      "old_total_weights = -172.47718811035156\n",
      "new_total_weights = -147.8240203857422\n",
      "\n",
      "bert.encoder.layer.1.attention.self.key:\n",
      "old_total_weights = 15.925116539001465\n",
      "new_total_weights = -3.735687255859375\n",
      "\n",
      "bert.encoder.layer.1.attention.self.value:\n",
      "old_total_weights = 4.898892402648926\n",
      "new_total_weights = 6.2427263259887695\n",
      "\n",
      "bert.encoder.layer.1.attention.output.dense:\n",
      "old_total_weights = 17.954797744750977\n",
      "new_total_weights = 29.458873748779297\n",
      "\n",
      "bert.encoder.layer.1.intermediate.dense:\n",
      "old_total_weights = 216.91973876953125\n",
      "new_total_weights = 206.94187927246094\n",
      "\n",
      "bert.encoder.layer.1.output.dense:\n",
      "old_total_weights = -113.06352996826172\n",
      "new_total_weights = -86.32957458496094\n",
      "\n",
      "bert.encoder.layer.2.attention.self.query:\n",
      "old_total_weights = 25.60422134399414\n",
      "new_total_weights = 17.421222686767578\n",
      "\n",
      "bert.encoder.layer.2.attention.self.key:\n",
      "old_total_weights = 22.683753967285156\n",
      "new_total_weights = 31.8819580078125\n",
      "\n",
      "bert.encoder.layer.2.attention.self.value:\n",
      "old_total_weights = -31.459964752197266\n",
      "new_total_weights = -29.492570877075195\n",
      "\n",
      "bert.encoder.layer.2.attention.output.dense:\n",
      "old_total_weights = -2.091892719268799\n",
      "new_total_weights = -6.930056571960449\n",
      "\n",
      "bert.encoder.layer.2.intermediate.dense:\n",
      "old_total_weights = 179.33651733398438\n",
      "new_total_weights = 195.5978240966797\n",
      "\n",
      "bert.encoder.layer.2.output.dense:\n",
      "old_total_weights = -161.84573364257812\n",
      "new_total_weights = -154.96267700195312\n",
      "\n",
      "bert.encoder.layer.3.attention.self.query:\n",
      "old_total_weights = 168.6527862548828\n",
      "new_total_weights = 138.2045440673828\n",
      "\n",
      "bert.encoder.layer.3.attention.self.key:\n",
      "old_total_weights = 10.664956092834473\n",
      "new_total_weights = 8.696677207946777\n",
      "\n",
      "bert.encoder.layer.3.attention.self.value:\n",
      "old_total_weights = -40.47368621826172\n",
      "new_total_weights = -32.69964599609375\n",
      "\n",
      "bert.encoder.layer.3.attention.output.dense:\n",
      "old_total_weights = 1.8081789016723633\n",
      "new_total_weights = -4.235532760620117\n",
      "\n",
      "bert.encoder.layer.3.intermediate.dense:\n",
      "old_total_weights = -166.23297119140625\n",
      "new_total_weights = -111.07532501220703\n",
      "\n",
      "bert.encoder.layer.3.output.dense:\n",
      "old_total_weights = -63.389793395996094\n",
      "new_total_weights = -72.08753967285156\n",
      "\n",
      "bert.encoder.layer.4.attention.self.query:\n",
      "old_total_weights = -68.96369934082031\n",
      "new_total_weights = -71.46240997314453\n",
      "\n",
      "bert.encoder.layer.4.attention.self.key:\n",
      "old_total_weights = 8.548141479492188\n",
      "new_total_weights = 13.649600982666016\n",
      "\n",
      "bert.encoder.layer.4.attention.self.value:\n",
      "old_total_weights = 37.15562438964844\n",
      "new_total_weights = 41.575645446777344\n",
      "\n",
      "bert.encoder.layer.4.attention.output.dense:\n",
      "old_total_weights = 12.137937545776367\n",
      "new_total_weights = 12.181829452514648\n",
      "\n",
      "bert.encoder.layer.4.intermediate.dense:\n",
      "old_total_weights = 206.0537872314453\n",
      "new_total_weights = 216.02679443359375\n",
      "\n",
      "bert.encoder.layer.4.output.dense:\n",
      "old_total_weights = 56.355369567871094\n",
      "new_total_weights = 56.406776428222656\n",
      "\n",
      "bert.encoder.layer.5.attention.self.query:\n",
      "old_total_weights = -11.414525985717773\n",
      "new_total_weights = -13.101999282836914\n",
      "\n",
      "bert.encoder.layer.5.attention.self.key:\n",
      "old_total_weights = -36.50926971435547\n",
      "new_total_weights = -26.264617919921875\n",
      "\n",
      "bert.encoder.layer.5.attention.self.value:\n",
      "old_total_weights = -22.979393005371094\n",
      "new_total_weights = -26.622791290283203\n",
      "\n",
      "bert.encoder.layer.5.attention.output.dense:\n",
      "old_total_weights = 8.377960205078125\n",
      "new_total_weights = 18.972211837768555\n",
      "\n",
      "bert.encoder.layer.5.intermediate.dense:\n",
      "old_total_weights = 116.83935546875\n",
      "new_total_weights = 138.3880615234375\n",
      "\n",
      "bert.encoder.layer.5.output.dense:\n",
      "old_total_weights = -5.927032470703125\n",
      "new_total_weights = -26.357295989990234\n",
      "\n",
      "bert.encoder.layer.6.attention.self.query:\n",
      "old_total_weights = 42.004615783691406\n",
      "new_total_weights = 39.623207092285156\n",
      "\n",
      "bert.encoder.layer.6.attention.self.key:\n",
      "old_total_weights = -30.934654235839844\n",
      "new_total_weights = -37.15207290649414\n",
      "\n",
      "bert.encoder.layer.6.attention.self.value:\n",
      "old_total_weights = 15.727470397949219\n",
      "new_total_weights = 25.7087459564209\n",
      "\n",
      "bert.encoder.layer.6.attention.output.dense:\n",
      "old_total_weights = -0.9991016387939453\n",
      "new_total_weights = -3.4545600414276123\n",
      "\n",
      "bert.encoder.layer.6.intermediate.dense:\n",
      "old_total_weights = -0.3661346435546875\n",
      "new_total_weights = -12.06637191772461\n",
      "\n",
      "bert.encoder.layer.6.output.dense:\n",
      "old_total_weights = 18.438753128051758\n",
      "new_total_weights = -12.028874397277832\n",
      "\n",
      "bert.encoder.layer.7.attention.self.query:\n",
      "old_total_weights = -5.138084411621094\n",
      "new_total_weights = -19.611968994140625\n",
      "\n",
      "bert.encoder.layer.7.attention.self.key:\n",
      "old_total_weights = -7.9675703048706055\n",
      "new_total_weights = -6.013546466827393\n",
      "\n",
      "bert.encoder.layer.7.attention.self.value:\n",
      "old_total_weights = 55.10824203491211\n",
      "new_total_weights = 45.70771026611328\n",
      "\n",
      "bert.encoder.layer.7.attention.output.dense:\n",
      "old_total_weights = -8.208744049072266\n",
      "new_total_weights = -5.099782943725586\n",
      "\n",
      "bert.encoder.layer.7.intermediate.dense:\n",
      "old_total_weights = 780.0004272460938\n",
      "new_total_weights = 727.3012084960938\n",
      "\n",
      "bert.encoder.layer.7.output.dense:\n",
      "old_total_weights = 27.525745391845703\n",
      "new_total_weights = 23.833786010742188\n",
      "\n",
      "bert.encoder.layer.8.attention.self.query:\n",
      "old_total_weights = 73.14311981201172\n",
      "new_total_weights = 65.73196411132812\n",
      "\n",
      "bert.encoder.layer.8.attention.self.key:\n",
      "old_total_weights = -25.741748809814453\n",
      "new_total_weights = -8.687017440795898\n",
      "\n",
      "bert.encoder.layer.8.attention.self.value:\n",
      "old_total_weights = 5.018766403198242\n",
      "new_total_weights = 5.230940818786621\n",
      "\n",
      "bert.encoder.layer.8.attention.output.dense:\n",
      "old_total_weights = 2.750354766845703\n",
      "new_total_weights = 8.816848754882812\n",
      "\n",
      "bert.encoder.layer.8.intermediate.dense:\n",
      "old_total_weights = 987.460205078125\n",
      "new_total_weights = 932.72216796875\n",
      "\n",
      "bert.encoder.layer.8.output.dense:\n",
      "old_total_weights = 81.67925262451172\n",
      "new_total_weights = 73.41251373291016\n",
      "\n",
      "bert.encoder.layer.9.attention.self.query:\n",
      "old_total_weights = -74.09059143066406\n",
      "new_total_weights = -77.53339385986328\n",
      "\n",
      "bert.encoder.layer.9.attention.self.key:\n",
      "old_total_weights = 12.520721435546875\n",
      "new_total_weights = 16.22161102294922\n",
      "\n",
      "bert.encoder.layer.9.attention.self.value:\n",
      "old_total_weights = 16.117881774902344\n",
      "new_total_weights = 8.086181640625\n",
      "\n",
      "bert.encoder.layer.9.attention.output.dense:\n",
      "old_total_weights = 6.51149845123291\n",
      "new_total_weights = 14.523763656616211\n",
      "\n",
      "bert.encoder.layer.9.intermediate.dense:\n",
      "old_total_weights = 1177.708984375\n",
      "new_total_weights = 1084.3167724609375\n",
      "\n",
      "bert.encoder.layer.9.output.dense:\n",
      "old_total_weights = -55.69377136230469\n",
      "new_total_weights = -61.409149169921875\n",
      "\n",
      "bert.encoder.layer.10.attention.self.query:\n",
      "old_total_weights = 92.87875366210938\n",
      "new_total_weights = 74.40454864501953\n",
      "\n",
      "bert.encoder.layer.10.attention.self.key:\n",
      "old_total_weights = -5.905996322631836\n",
      "new_total_weights = -21.057708740234375\n",
      "\n",
      "bert.encoder.layer.10.attention.self.value:\n",
      "old_total_weights = -22.011404037475586\n",
      "new_total_weights = -23.042478561401367\n",
      "\n",
      "bert.encoder.layer.10.attention.output.dense:\n",
      "old_total_weights = 6.191995620727539\n",
      "new_total_weights = 8.460063934326172\n",
      "\n",
      "bert.encoder.layer.10.intermediate.dense:\n",
      "old_total_weights = 124.51962280273438\n",
      "new_total_weights = 93.8774185180664\n",
      "\n",
      "bert.encoder.layer.10.output.dense:\n",
      "old_total_weights = -16.71365737915039\n",
      "new_total_weights = -26.39118766784668\n",
      "\n",
      "bert.encoder.layer.11.attention.self.query:\n",
      "old_total_weights = -86.7364501953125\n",
      "new_total_weights = -82.98149871826172\n",
      "\n",
      "bert.encoder.layer.11.attention.self.key:\n",
      "old_total_weights = 146.94430541992188\n",
      "new_total_weights = 134.37852478027344\n",
      "\n",
      "bert.encoder.layer.11.attention.self.value:\n",
      "old_total_weights = -9.398944854736328\n",
      "new_total_weights = -11.852212905883789\n",
      "\n",
      "bert.encoder.layer.11.attention.output.dense:\n",
      "old_total_weights = -2.0948753356933594\n",
      "new_total_weights = -5.165273666381836\n",
      "\n",
      "bert.encoder.layer.11.intermediate.dense:\n",
      "old_total_weights = 1081.45654296875\n",
      "new_total_weights = 980.502197265625\n",
      "\n",
      "bert.encoder.layer.11.output.dense:\n",
      "old_total_weights = -142.27239990234375\n",
      "new_total_weights = -121.6112060546875\n",
      "\n",
      "bert.pooler.dense:\n",
      "old_total_weights = 19.525592803955078\n",
      "new_total_weights = 13.470413208007812\n",
      "\n",
      "classifier:\n",
      "old_total_weights = -1.4085718393325806\n",
      "new_total_weights = -1.9221343994140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48596' max='48596' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48596/48596 1:00:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.738300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.751900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.788100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.818300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.799200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.787100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.796400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.805900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.798800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.795400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.786200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.760200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.778500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.794100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.782300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.786800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.781200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.772400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.768500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.763300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.766600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.766400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.763500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.758100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.751300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.757600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.736200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.738700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.711700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.705600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.727900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.719500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.731300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.728300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.705400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.724700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.718600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.680400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.651400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.681700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.670900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.630900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.667200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.663200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.660500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.625400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.624900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.658300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.636800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.644200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.668100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model was saved\n",
      "------------------------------ pruned bert-base-uncased weights ------------------------------\n",
      "+---------------------------------------------------------+------------+---------------+\n",
      "|                         Modules                         | Parameters | Sum of Tensor |\n",
      "+---------------------------------------------------------+------------+---------------+\n",
      "|          bert.embeddings.word_embeddings.weight         |  23440896  |   957280.625  |\n",
      "|        bert.embeddings.position_embeddings.weight       |   393216   |   4589.5972   |\n",
      "|       bert.embeddings.token_type_embeddings.weight      |    1536    |    14.4349    |\n",
      "|             bert.embeddings.LayerNorm.weight            |    768     |    652.8584   |\n",
      "|              bert.embeddings.LayerNorm.bias             |    768     |    31.8757    |\n",
      "|      bert.encoder.layer.0.attention.self.query.bias     |    768     |    167.1329   |\n",
      "|     bert.encoder.layer.0.attention.self.query.weight    |   589824   |   17511.2344  |\n",
      "|       bert.encoder.layer.0.attention.self.key.bias      |    768     |     1.8731    |\n",
      "|      bert.encoder.layer.0.attention.self.key.weight     |   589824   |   17277.8477  |\n",
      "|      bert.encoder.layer.0.attention.self.value.bias     |    768     |    19.4836    |\n",
      "|     bert.encoder.layer.0.attention.self.value.weight    |   589824   |   12072.2246  |\n",
      "|     bert.encoder.layer.0.attention.output.dense.bias    |    768     |    19.2756    |\n",
      "|    bert.encoder.layer.0.attention.output.dense.weight   |   589824   |    11741.0    |\n",
      "|  bert.encoder.layer.0.attention.output.LayerNorm.weight |    768     |    737.9766   |\n",
      "|   bert.encoder.layer.0.attention.output.LayerNorm.bias  |    768     |    155.1195   |\n",
      "|       bert.encoder.layer.0.intermediate.dense.bias      |    3072    |    348.6585   |\n",
      "|      bert.encoder.layer.0.intermediate.dense.weight     |  2359296   |   62135.5312  |\n",
      "|          bert.encoder.layer.0.output.dense.bias         |    768     |     44.028    |\n",
      "|         bert.encoder.layer.0.output.dense.weight        |  2359296   |   58629.8242  |\n",
      "|       bert.encoder.layer.0.output.LayerNorm.weight      |    768     |    581.6738   |\n",
      "|        bert.encoder.layer.0.output.LayerNorm.bias       |    768     |    56.3082    |\n",
      "|      bert.encoder.layer.1.attention.self.query.bias     |    768     |    87.1796    |\n",
      "|     bert.encoder.layer.1.attention.self.query.weight    |   589824   |   17328.1504  |\n",
      "|       bert.encoder.layer.1.attention.self.key.bias      |    768     |     2.7956    |\n",
      "|      bert.encoder.layer.1.attention.self.key.weight     |   589824   |   17289.9941  |\n",
      "|      bert.encoder.layer.1.attention.self.value.bias     |    768     |    17.9017    |\n",
      "|     bert.encoder.layer.1.attention.self.value.weight    |   589824   |   11865.4727  |\n",
      "|     bert.encoder.layer.1.attention.output.dense.bias    |    768     |    34.8241    |\n",
      "|    bert.encoder.layer.1.attention.output.dense.weight   |   589824   |   11503.123   |\n",
      "|  bert.encoder.layer.1.attention.output.LayerNorm.weight |    768     |    676.0329   |\n",
      "|   bert.encoder.layer.1.attention.output.LayerNorm.bias  |    768     |    96.8254    |\n",
      "|       bert.encoder.layer.1.intermediate.dense.bias      |    3072    |    313.5627   |\n",
      "|      bert.encoder.layer.1.intermediate.dense.weight     |  2359296   |   64778.2305  |\n",
      "|          bert.encoder.layer.1.output.dense.bias         |    768     |     35.322    |\n",
      "|         bert.encoder.layer.1.output.dense.weight        |  2359296   |   61220.1484  |\n",
      "|       bert.encoder.layer.1.output.LayerNorm.weight      |    768     |    668.7255   |\n",
      "|        bert.encoder.layer.1.output.LayerNorm.bias       |    768     |    54.1958    |\n",
      "|      bert.encoder.layer.2.attention.self.query.bias     |    768     |    72.1321    |\n",
      "|     bert.encoder.layer.2.attention.self.query.weight    |   589824   |   18932.6641  |\n",
      "|       bert.encoder.layer.2.attention.self.key.bias      |    768     |     2.3018    |\n",
      "|      bert.encoder.layer.2.attention.self.key.weight     |   589824   |   18534.4688  |\n",
      "|      bert.encoder.layer.2.attention.self.value.bias     |    768     |    24.1137    |\n",
      "|     bert.encoder.layer.2.attention.self.value.weight    |   589824   |   11725.8408  |\n",
      "|     bert.encoder.layer.2.attention.output.dense.bias    |    768     |    49.9984    |\n",
      "|    bert.encoder.layer.2.attention.output.dense.weight   |   589824   |   11319.6855  |\n",
      "|  bert.encoder.layer.2.attention.output.LayerNorm.weight |    768     |    669.8455   |\n",
      "|   bert.encoder.layer.2.attention.output.LayerNorm.bias  |    768     |    96.8183    |\n",
      "|       bert.encoder.layer.2.intermediate.dense.bias      |    3072    |    325.9747   |\n",
      "|      bert.encoder.layer.2.intermediate.dense.weight     |  2359296   |   65641.0312  |\n",
      "|          bert.encoder.layer.2.output.dense.bias         |    768     |    41.5878    |\n",
      "|         bert.encoder.layer.2.output.dense.weight        |  2359296   |   61969.4961  |\n",
      "|       bert.encoder.layer.2.output.LayerNorm.weight      |    768     |    654.7488   |\n",
      "|        bert.encoder.layer.2.output.LayerNorm.bias       |    768     |    48.6952    |\n",
      "|      bert.encoder.layer.3.attention.self.query.bias     |    768     |    69.9179    |\n",
      "|     bert.encoder.layer.3.attention.self.query.weight    |   589824   |   17493.4102  |\n",
      "|       bert.encoder.layer.3.attention.self.key.bias      |    768     |     2.7586    |\n",
      "|      bert.encoder.layer.3.attention.self.key.weight     |   589824   |   17417.4102  |\n",
      "|      bert.encoder.layer.3.attention.self.value.bias     |    768     |    16.0003    |\n",
      "|     bert.encoder.layer.3.attention.self.value.weight    |   589824   |   12911.0068  |\n",
      "|     bert.encoder.layer.3.attention.output.dense.bias    |    768     |    41.9733    |\n",
      "|    bert.encoder.layer.3.attention.output.dense.weight   |   589824   |   12059.1387  |\n",
      "|  bert.encoder.layer.3.attention.output.LayerNorm.weight |    768     |    666.4349   |\n",
      "|   bert.encoder.layer.3.attention.output.LayerNorm.bias  |    768     |    89.4055    |\n",
      "|       bert.encoder.layer.3.intermediate.dense.bias      |    3072    |    316.7501   |\n",
      "|      bert.encoder.layer.3.intermediate.dense.weight     |  2359296   |   66480.3047  |\n",
      "|          bert.encoder.layer.3.output.dense.bias         |    768     |    41.3013    |\n",
      "|         bert.encoder.layer.3.output.dense.weight        |  2359296   |   62848.9688  |\n",
      "|       bert.encoder.layer.3.output.LayerNorm.weight      |    768     |    623.7829   |\n",
      "|        bert.encoder.layer.3.output.LayerNorm.bias       |    768     |     36.76     |\n",
      "|      bert.encoder.layer.4.attention.self.query.bias     |    768     |    79.2117    |\n",
      "|     bert.encoder.layer.4.attention.self.query.weight    |   589824   |   17209.3242  |\n",
      "|       bert.encoder.layer.4.attention.self.key.bias      |    768     |     2.7922    |\n",
      "|      bert.encoder.layer.4.attention.self.key.weight     |   589824   |   17111.8887  |\n",
      "|      bert.encoder.layer.4.attention.self.value.bias     |    768     |    14.1984    |\n",
      "|     bert.encoder.layer.4.attention.self.value.weight    |   589824   |   14336.4023  |\n",
      "|     bert.encoder.layer.4.attention.output.dense.bias    |    768     |    21.7963    |\n",
      "|    bert.encoder.layer.4.attention.output.dense.weight   |   589824   |   13375.2832  |\n",
      "|  bert.encoder.layer.4.attention.output.LayerNorm.weight |    768     |    648.8423   |\n",
      "|   bert.encoder.layer.4.attention.output.LayerNorm.bias  |    768     |    78.7687    |\n",
      "|       bert.encoder.layer.4.intermediate.dense.bias      |    3072    |    329.6061   |\n",
      "|      bert.encoder.layer.4.intermediate.dense.weight     |  2359296   |   66935.3281  |\n",
      "|          bert.encoder.layer.4.output.dense.bias         |    768     |    31.5361    |\n",
      "|         bert.encoder.layer.4.output.dense.weight        |  2359296   |   63602.8516  |\n",
      "|       bert.encoder.layer.4.output.LayerNorm.weight      |    768     |    645.7525   |\n",
      "|        bert.encoder.layer.4.output.LayerNorm.bias       |    768     |    35.1187    |\n",
      "|      bert.encoder.layer.5.attention.self.query.bias     |    768     |    63.3617    |\n",
      "|     bert.encoder.layer.5.attention.self.query.weight    |   589824   |   17673.0488  |\n",
      "|       bert.encoder.layer.5.attention.self.key.bias      |    768     |     3.4999    |\n",
      "|      bert.encoder.layer.5.attention.self.key.weight     |   589824   |   17649.668   |\n",
      "|      bert.encoder.layer.5.attention.self.value.bias     |    768     |    15.3085    |\n",
      "|     bert.encoder.layer.5.attention.self.value.weight    |   589824   |   14342.5898  |\n",
      "|     bert.encoder.layer.5.attention.output.dense.bias    |    768     |     19.798    |\n",
      "|    bert.encoder.layer.5.attention.output.dense.weight   |   589824   |   13615.3438  |\n",
      "|  bert.encoder.layer.5.attention.output.LayerNorm.weight |    768     |    655.1617   |\n",
      "|   bert.encoder.layer.5.attention.output.LayerNorm.bias  |    768     |    76.3774    |\n",
      "|       bert.encoder.layer.5.intermediate.dense.bias      |    3072    |    324.3033   |\n",
      "|      bert.encoder.layer.5.intermediate.dense.weight     |  2359296   |   66675.9141  |\n",
      "|          bert.encoder.layer.5.output.dense.bias         |    768     |    31.9867    |\n",
      "|         bert.encoder.layer.5.output.dense.weight        |  2359296   |   62827.6094  |\n",
      "|       bert.encoder.layer.5.output.LayerNorm.weight      |    768     |    640.3774   |\n",
      "|        bert.encoder.layer.5.output.LayerNorm.bias       |    768     |    38.8588    |\n",
      "|      bert.encoder.layer.6.attention.self.query.bias     |    768     |    67.0804    |\n",
      "|     bert.encoder.layer.6.attention.self.query.weight    |   589824   |   17559.6074  |\n",
      "|       bert.encoder.layer.6.attention.self.key.bias      |    768     |     3.1424    |\n",
      "|      bert.encoder.layer.6.attention.self.key.weight     |   589824   |   17534.9883  |\n",
      "|      bert.encoder.layer.6.attention.self.value.bias     |    768     |    18.8147    |\n",
      "|     bert.encoder.layer.6.attention.self.value.weight    |   589824   |   14018.8223  |\n",
      "|     bert.encoder.layer.6.attention.output.dense.bias    |    768     |    23.5092    |\n",
      "|    bert.encoder.layer.6.attention.output.dense.weight   |   589824   |   13266.8945  |\n",
      "|  bert.encoder.layer.6.attention.output.LayerNorm.weight |    768     |    641.0266   |\n",
      "|   bert.encoder.layer.6.attention.output.LayerNorm.bias  |    768     |    74.4319    |\n",
      "|       bert.encoder.layer.6.intermediate.dense.bias      |    3072    |    314.2466   |\n",
      "|      bert.encoder.layer.6.intermediate.dense.weight     |  2359296   |   67042.1641  |\n",
      "|          bert.encoder.layer.6.output.dense.bias         |    768     |     40.488    |\n",
      "|         bert.encoder.layer.6.output.dense.weight        |  2359296   |   62216.9688  |\n",
      "|       bert.encoder.layer.6.output.LayerNorm.weight      |    768     |    641.5121   |\n",
      "|        bert.encoder.layer.6.output.LayerNorm.bias       |    768     |    40.0761    |\n",
      "|      bert.encoder.layer.7.attention.self.query.bias     |    768     |    75.6601    |\n",
      "|     bert.encoder.layer.7.attention.self.query.weight    |   589824   |   17716.291   |\n",
      "|       bert.encoder.layer.7.attention.self.key.bias      |    768     |     3.1871    |\n",
      "|      bert.encoder.layer.7.attention.self.key.weight     |   589824   |   17758.0664  |\n",
      "|      bert.encoder.layer.7.attention.self.value.bias     |    768     |    22.6385    |\n",
      "|     bert.encoder.layer.7.attention.self.value.weight    |   589824   |   13633.3535  |\n",
      "|     bert.encoder.layer.7.attention.output.dense.bias    |    768     |    22.8285    |\n",
      "|    bert.encoder.layer.7.attention.output.dense.weight   |   589824   |   13095.9189  |\n",
      "|  bert.encoder.layer.7.attention.output.LayerNorm.weight |    768     |    636.8367   |\n",
      "|   bert.encoder.layer.7.attention.output.LayerNorm.bias  |    768     |    79.6682    |\n",
      "|       bert.encoder.layer.7.intermediate.dense.bias      |    3072    |    335.6939   |\n",
      "|      bert.encoder.layer.7.intermediate.dense.weight     |  2359296   |   64808.4375  |\n",
      "|          bert.encoder.layer.7.output.dense.bias         |    768     |    46.1159    |\n",
      "|         bert.encoder.layer.7.output.dense.weight        |  2359296   |   60354.7266  |\n",
      "|       bert.encoder.layer.7.output.LayerNorm.weight      |    768     |    622.9286   |\n",
      "|        bert.encoder.layer.7.output.LayerNorm.bias       |    768     |    38.9745    |\n",
      "|      bert.encoder.layer.8.attention.self.query.bias     |    768     |    96.9207    |\n",
      "|     bert.encoder.layer.8.attention.self.query.weight    |   589824   |    17874.75   |\n",
      "|       bert.encoder.layer.8.attention.self.key.bias      |    768     |     4.0099    |\n",
      "|      bert.encoder.layer.8.attention.self.key.weight     |   589824   |   17919.5234  |\n",
      "|      bert.encoder.layer.8.attention.self.value.bias     |    768     |    20.7304    |\n",
      "|     bert.encoder.layer.8.attention.self.value.weight    |   589824   |   14420.8281  |\n",
      "|     bert.encoder.layer.8.attention.output.dense.bias    |    768     |    27.7386    |\n",
      "|    bert.encoder.layer.8.attention.output.dense.weight   |   589824   |   13657.0996  |\n",
      "|  bert.encoder.layer.8.attention.output.LayerNorm.weight |    768     |    647.5828   |\n",
      "|   bert.encoder.layer.8.attention.output.LayerNorm.bias  |    768     |     74.784    |\n",
      "|       bert.encoder.layer.8.intermediate.dense.bias      |    3072    |    326.7636   |\n",
      "|      bert.encoder.layer.8.intermediate.dense.weight     |  2359296   |   64723.6367  |\n",
      "|          bert.encoder.layer.8.output.dense.bias         |    768     |    47.4886    |\n",
      "|         bert.encoder.layer.8.output.dense.weight        |  2359296   |   59955.1602  |\n",
      "|       bert.encoder.layer.8.output.LayerNorm.weight      |    768     |    637.0541   |\n",
      "|        bert.encoder.layer.8.output.LayerNorm.bias       |    768     |    41.4147    |\n",
      "|      bert.encoder.layer.9.attention.self.query.bias     |    768     |    108.5074   |\n",
      "|     bert.encoder.layer.9.attention.self.query.weight    |   589824   |   18518.2422  |\n",
      "|       bert.encoder.layer.9.attention.self.key.bias      |    768     |     3.9467    |\n",
      "|      bert.encoder.layer.9.attention.self.key.weight     |   589824   |   18484.3359  |\n",
      "|      bert.encoder.layer.9.attention.self.value.bias     |    768     |    20.0187    |\n",
      "|     bert.encoder.layer.9.attention.self.value.weight    |   589824   |   14006.6475  |\n",
      "|     bert.encoder.layer.9.attention.output.dense.bias    |    768     |    34.3847    |\n",
      "|    bert.encoder.layer.9.attention.output.dense.weight   |   589824   |   13299.998   |\n",
      "|  bert.encoder.layer.9.attention.output.LayerNorm.weight |    768     |    631.1705   |\n",
      "|   bert.encoder.layer.9.attention.output.LayerNorm.bias  |    768     |     74.962    |\n",
      "|       bert.encoder.layer.9.intermediate.dense.bias      |    3072    |    341.7172   |\n",
      "|      bert.encoder.layer.9.intermediate.dense.weight     |  2359296   |   65137.8203  |\n",
      "|          bert.encoder.layer.9.output.dense.bias         |    768     |    45.6526    |\n",
      "|         bert.encoder.layer.9.output.dense.weight        |  2359296   |   61772.5391  |\n",
      "|       bert.encoder.layer.9.output.LayerNorm.weight      |    768     |    612.5734   |\n",
      "|        bert.encoder.layer.9.output.LayerNorm.bias       |    768     |     35.957    |\n",
      "|     bert.encoder.layer.10.attention.self.query.bias     |    768     |    106.6192   |\n",
      "|    bert.encoder.layer.10.attention.self.query.weight    |   589824   |   18417.6016  |\n",
      "|      bert.encoder.layer.10.attention.self.key.bias      |    768     |     3.7735    |\n",
      "|     bert.encoder.layer.10.attention.self.key.weight     |   589824   |   18386.959   |\n",
      "|     bert.encoder.layer.10.attention.self.value.bias     |    768     |    14.1298    |\n",
      "|    bert.encoder.layer.10.attention.self.value.weight    |   589824   |   14273.4453  |\n",
      "|    bert.encoder.layer.10.attention.output.dense.bias    |    768     |    32.0185    |\n",
      "|   bert.encoder.layer.10.attention.output.dense.weight   |   589824   |   13496.0537  |\n",
      "| bert.encoder.layer.10.attention.output.LayerNorm.weight |    768     |    647.6993   |\n",
      "|  bert.encoder.layer.10.attention.output.LayerNorm.bias  |    768     |    61.6929    |\n",
      "|      bert.encoder.layer.10.intermediate.dense.bias      |    3072    |    342.9952   |\n",
      "|     bert.encoder.layer.10.intermediate.dense.weight     |  2359296   |   64307.0156  |\n",
      "|         bert.encoder.layer.10.output.dense.bias         |    768     |    57.2791    |\n",
      "|        bert.encoder.layer.10.output.dense.weight        |  2359296   |   60205.6953  |\n",
      "|      bert.encoder.layer.10.output.LayerNorm.weight      |    768     |    624.2518   |\n",
      "|       bert.encoder.layer.10.output.LayerNorm.bias       |    768     |    37.6911    |\n",
      "|     bert.encoder.layer.11.attention.self.query.bias     |    768     |    125.8799   |\n",
      "|    bert.encoder.layer.11.attention.self.query.weight    |   589824   |   18081.2227  |\n",
      "|      bert.encoder.layer.11.attention.self.key.bias      |    768     |     3.0221    |\n",
      "|     bert.encoder.layer.11.attention.self.key.weight     |   589824   |   17835.8828  |\n",
      "|     bert.encoder.layer.11.attention.self.value.bias     |    768     |    11.4999    |\n",
      "|    bert.encoder.layer.11.attention.self.value.weight    |   589824   |   15621.834   |\n",
      "|    bert.encoder.layer.11.attention.output.dense.bias    |    768     |    22.4221    |\n",
      "|   bert.encoder.layer.11.attention.output.dense.weight   |   589824   |   14727.2832  |\n",
      "| bert.encoder.layer.11.attention.output.LayerNorm.weight |    768     |    654.925    |\n",
      "|  bert.encoder.layer.11.attention.output.LayerNorm.bias  |    768     |    52.4466    |\n",
      "|      bert.encoder.layer.11.intermediate.dense.bias      |    3072    |    269.1954   |\n",
      "|     bert.encoder.layer.11.intermediate.dense.weight     |  2359296   |   64561.6875  |\n",
      "|         bert.encoder.layer.11.output.dense.bias         |    768     |    31.9988    |\n",
      "|        bert.encoder.layer.11.output.dense.weight        |  2359296   |   57197.8164  |\n",
      "|      bert.encoder.layer.11.output.LayerNorm.weight      |    768     |    479.0781   |\n",
      "|       bert.encoder.layer.11.output.LayerNorm.bias       |    768     |    31.9651    |\n",
      "|                  bert.pooler.dense.bias                 |    768     |    19.6556    |\n",
      "|                 bert.pooler.dense.weight                |   589824   |   10506.1787  |\n",
      "|                     classifier.bias                     |     6      |     0.0273    |\n",
      "|                    classifier.weight                    |    4608    |    59.1764    |\n",
      "|                  Total Trainable Params                 | 109486854  |   3258045.75  |\n",
      "+---------------------------------------------------------+------------+---------------+\n",
      "------------------------------ pruned bert-base-uncased metrics ------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 value: 0.7393312876568089\n",
      "accuracy value: 0.7400582358448827\n",
      "\n",
      "------------------------------ half bert-base-uncased ------------------------------\n",
      "model was saved\n",
      "------------------------------ half bert-base-uncased weights ------------------------------\n",
      "+---------------------------------------------------------+------------+---------------+\n",
      "|                         Modules                         | Parameters | Sum of Tensor |\n",
      "+---------------------------------------------------------+------------+---------------+\n",
      "|          bert.embeddings.word_embeddings.weight         |  23440896  |      inf      |\n",
      "|        bert.embeddings.position_embeddings.weight       |   393216   |     4576.0    |\n",
      "|       bert.embeddings.token_type_embeddings.weight      |    1536    |    13.9219    |\n",
      "|             bert.embeddings.LayerNorm.weight            |    768     |     652.5     |\n",
      "|              bert.embeddings.LayerNorm.bias             |    768     |    31.7344    |\n",
      "|     bert.encoder.layer.0.attention.self.query.weight    |   589824   |    20032.0    |\n",
      "|      bert.encoder.layer.0.attention.self.query.bias     |    768     |    167.125    |\n",
      "|      bert.encoder.layer.0.attention.self.key.weight     |   589824   |    19696.0    |\n",
      "|       bert.encoder.layer.0.attention.self.key.bias      |    768     |     1.873     |\n",
      "|     bert.encoder.layer.0.attention.self.value.weight    |   589824   |    13480.0    |\n",
      "|      bert.encoder.layer.0.attention.self.value.bias     |    768     |    19.8906    |\n",
      "|    bert.encoder.layer.0.attention.output.dense.weight   |   589824   |    13040.0    |\n",
      "|     bert.encoder.layer.0.attention.output.dense.bias    |    768     |    19.1094    |\n",
      "|  bert.encoder.layer.0.attention.output.LayerNorm.weight |    768     |     736.5     |\n",
      "|   bert.encoder.layer.0.attention.output.LayerNorm.bias  |    768     |     153.75    |\n",
      "|      bert.encoder.layer.0.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|       bert.encoder.layer.0.intermediate.dense.bias      |    3072    |     347.25    |\n",
      "|         bert.encoder.layer.0.output.dense.weight        |  2359296   |      inf      |\n",
      "|          bert.encoder.layer.0.output.dense.bias         |    768     |    44.2188    |\n",
      "|       bert.encoder.layer.0.output.LayerNorm.weight      |    768     |     581.0     |\n",
      "|        bert.encoder.layer.0.output.LayerNorm.bias       |    768     |    56.6875    |\n",
      "|     bert.encoder.layer.1.attention.self.query.weight    |   589824   |    19872.0    |\n",
      "|      bert.encoder.layer.1.attention.self.query.bias     |    768     |     86.625    |\n",
      "|      bert.encoder.layer.1.attention.self.key.weight     |   589824   |    19776.0    |\n",
      "|       bert.encoder.layer.1.attention.self.key.bias      |    768     |     2.7949    |\n",
      "|     bert.encoder.layer.1.attention.self.value.weight    |   589824   |    13288.0    |\n",
      "|      bert.encoder.layer.1.attention.self.value.bias     |    768     |    18.3281    |\n",
      "|    bert.encoder.layer.1.attention.output.dense.weight   |   589824   |    12784.0    |\n",
      "|     bert.encoder.layer.1.attention.output.dense.bias    |    768     |    35.0312    |\n",
      "|  bert.encoder.layer.1.attention.output.LayerNorm.weight |    768     |     674.0     |\n",
      "|   bert.encoder.layer.1.attention.output.LayerNorm.bias  |    768     |    95.9375    |\n",
      "|      bert.encoder.layer.1.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|       bert.encoder.layer.1.intermediate.dense.bias      |    3072    |     312.75    |\n",
      "|         bert.encoder.layer.1.output.dense.weight        |  2359296   |      inf      |\n",
      "|          bert.encoder.layer.1.output.dense.bias         |    768     |    35.4688    |\n",
      "|       bert.encoder.layer.1.output.LayerNorm.weight      |    768     |     668.5     |\n",
      "|        bert.encoder.layer.1.output.LayerNorm.bias       |    768     |    54.3438    |\n",
      "|     bert.encoder.layer.2.attention.self.query.weight    |   589824   |    21744.0    |\n",
      "|      bert.encoder.layer.2.attention.self.query.bias     |    768     |     72.375    |\n",
      "|      bert.encoder.layer.2.attention.self.key.weight     |   589824   |    21248.0    |\n",
      "|       bert.encoder.layer.2.attention.self.key.bias      |    768     |     2.3008    |\n",
      "|     bert.encoder.layer.2.attention.self.value.weight    |   589824   |    13072.0    |\n",
      "|      bert.encoder.layer.2.attention.self.value.bias     |    768     |    24.4531    |\n",
      "|    bert.encoder.layer.2.attention.output.dense.weight   |   589824   |    12552.0    |\n",
      "|     bert.encoder.layer.2.attention.output.dense.bias    |    768     |    50.5312    |\n",
      "|  bert.encoder.layer.2.attention.output.LayerNorm.weight |    768     |     668.0     |\n",
      "|   bert.encoder.layer.2.attention.output.LayerNorm.bias  |    768     |    95.6875    |\n",
      "|      bert.encoder.layer.2.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|       bert.encoder.layer.2.intermediate.dense.bias      |    3072    |     325.0     |\n",
      "|         bert.encoder.layer.2.output.dense.weight        |  2359296   |      inf      |\n",
      "|          bert.encoder.layer.2.output.dense.bias         |    768     |    41.7812    |\n",
      "|       bert.encoder.layer.2.output.LayerNorm.weight      |    768     |     654.5     |\n",
      "|        bert.encoder.layer.2.output.LayerNorm.bias       |    768     |    49.2188    |\n",
      "|     bert.encoder.layer.3.attention.self.query.weight    |   589824   |    20144.0    |\n",
      "|      bert.encoder.layer.3.attention.self.query.bias     |    768     |     69.875    |\n",
      "|      bert.encoder.layer.3.attention.self.key.weight     |   589824   |    20000.0    |\n",
      "|       bert.encoder.layer.3.attention.self.key.bias      |    768     |     2.7578    |\n",
      "|     bert.encoder.layer.3.attention.self.value.weight    |   589824   |    14552.0    |\n",
      "|      bert.encoder.layer.3.attention.self.value.bias     |    768     |    16.1406    |\n",
      "|    bert.encoder.layer.3.attention.output.dense.weight   |   589824   |    13480.0    |\n",
      "|     bert.encoder.layer.3.attention.output.dense.bias    |    768     |    42.4375    |\n",
      "|  bert.encoder.layer.3.attention.output.LayerNorm.weight |    768     |     664.5     |\n",
      "|   bert.encoder.layer.3.attention.output.LayerNorm.bias  |    768     |    88.5625    |\n",
      "|      bert.encoder.layer.3.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|       bert.encoder.layer.3.intermediate.dense.bias      |    3072    |     316.0     |\n",
      "|         bert.encoder.layer.3.output.dense.weight        |  2359296   |      inf      |\n",
      "|          bert.encoder.layer.3.output.dense.bias         |    768     |    41.4375    |\n",
      "|       bert.encoder.layer.3.output.LayerNorm.weight      |    768     |     624.0     |\n",
      "|        bert.encoder.layer.3.output.LayerNorm.bias       |    768     |    37.5625    |\n",
      "|     bert.encoder.layer.4.attention.self.query.weight    |   589824   |    19776.0    |\n",
      "|      bert.encoder.layer.4.attention.self.query.bias     |    768     |     79.625    |\n",
      "|      bert.encoder.layer.4.attention.self.key.weight     |   589824   |    19664.0    |\n",
      "|       bert.encoder.layer.4.attention.self.key.bias      |    768     |     2.793     |\n",
      "|     bert.encoder.layer.4.attention.self.value.weight    |   589824   |    16328.0    |\n",
      "|      bert.encoder.layer.4.attention.self.value.bias     |    768     |    14.3984    |\n",
      "|    bert.encoder.layer.4.attention.output.dense.weight   |   589824   |    15088.0    |\n",
      "|     bert.encoder.layer.4.attention.output.dense.bias    |    768     |     22.25     |\n",
      "|  bert.encoder.layer.4.attention.output.LayerNorm.weight |    768     |     646.5     |\n",
      "|   bert.encoder.layer.4.attention.output.LayerNorm.bias  |    768     |     77.75     |\n",
      "|      bert.encoder.layer.4.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|       bert.encoder.layer.4.intermediate.dense.bias      |    3072    |     328.5     |\n",
      "|         bert.encoder.layer.4.output.dense.weight        |  2359296   |      inf      |\n",
      "|          bert.encoder.layer.4.output.dense.bias         |    768     |    31.5781    |\n",
      "|       bert.encoder.layer.4.output.LayerNorm.weight      |    768     |     646.0     |\n",
      "|        bert.encoder.layer.4.output.LayerNorm.bias       |    768     |    35.7188    |\n",
      "|     bert.encoder.layer.5.attention.self.query.weight    |   589824   |    20384.0    |\n",
      "|      bert.encoder.layer.5.attention.self.query.bias     |    768     |    63.5312    |\n",
      "|      bert.encoder.layer.5.attention.self.key.weight     |   589824   |    20352.0    |\n",
      "|       bert.encoder.layer.5.attention.self.key.bias      |    768     |      3.5      |\n",
      "|     bert.encoder.layer.5.attention.self.value.weight    |   589824   |    16280.0    |\n",
      "|      bert.encoder.layer.5.attention.self.value.bias     |    768     |    15.7266    |\n",
      "|    bert.encoder.layer.5.attention.output.dense.weight   |   589824   |    15384.0    |\n",
      "|     bert.encoder.layer.5.attention.output.dense.bias    |    768     |    20.2812    |\n",
      "|  bert.encoder.layer.5.attention.output.LayerNorm.weight |    768     |     653.0     |\n",
      "|   bert.encoder.layer.5.attention.output.LayerNorm.bias  |    768     |    75.3125    |\n",
      "|      bert.encoder.layer.5.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|       bert.encoder.layer.5.intermediate.dense.bias      |    3072    |     322.75    |\n",
      "|         bert.encoder.layer.5.output.dense.weight        |  2359296   |      inf      |\n",
      "|          bert.encoder.layer.5.output.dense.bias         |    768     |    31.9844    |\n",
      "|       bert.encoder.layer.5.output.LayerNorm.weight      |    768     |     640.5     |\n",
      "|        bert.encoder.layer.5.output.LayerNorm.bias       |    768     |    39.6562    |\n",
      "|     bert.encoder.layer.6.attention.self.query.weight    |   589824   |    20256.0    |\n",
      "|      bert.encoder.layer.6.attention.self.query.bias     |    768     |    67.0625    |\n",
      "|      bert.encoder.layer.6.attention.self.key.weight     |   589824   |    20224.0    |\n",
      "|       bert.encoder.layer.6.attention.self.key.bias      |    768     |     3.1426    |\n",
      "|     bert.encoder.layer.6.attention.self.value.weight    |   589824   |    15904.0    |\n",
      "|      bert.encoder.layer.6.attention.self.value.bias     |    768     |    19.1875    |\n",
      "|    bert.encoder.layer.6.attention.output.dense.weight   |   589824   |    14976.0    |\n",
      "|     bert.encoder.layer.6.attention.output.dense.bias    |    768     |    24.2656    |\n",
      "|  bert.encoder.layer.6.attention.output.LayerNorm.weight |    768     |     638.5     |\n",
      "|   bert.encoder.layer.6.attention.output.LayerNorm.bias  |    768     |    73.0625    |\n",
      "|      bert.encoder.layer.6.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|       bert.encoder.layer.6.intermediate.dense.bias      |    3072    |     312.25    |\n",
      "|         bert.encoder.layer.6.output.dense.weight        |  2359296   |      inf      |\n",
      "|          bert.encoder.layer.6.output.dense.bias         |    768     |      40.5     |\n",
      "|       bert.encoder.layer.6.output.LayerNorm.weight      |    768     |     641.5     |\n",
      "|        bert.encoder.layer.6.output.LayerNorm.bias       |    768     |    40.7188    |\n",
      "|     bert.encoder.layer.7.attention.self.query.weight    |   589824   |    20432.0    |\n",
      "|      bert.encoder.layer.7.attention.self.query.bias     |    768     |    75.4375    |\n",
      "|      bert.encoder.layer.7.attention.self.key.weight     |   589824   |    20480.0    |\n",
      "|       bert.encoder.layer.7.attention.self.key.bias      |    768     |     3.1875    |\n",
      "|     bert.encoder.layer.7.attention.self.value.weight    |   589824   |    15456.0    |\n",
      "|      bert.encoder.layer.7.attention.self.value.bias     |    768     |      23.0     |\n",
      "|    bert.encoder.layer.7.attention.output.dense.weight   |   589824   |    14816.0    |\n",
      "|     bert.encoder.layer.7.attention.output.dense.bias    |    768     |    23.4688    |\n",
      "|  bert.encoder.layer.7.attention.output.LayerNorm.weight |    768     |     634.5     |\n",
      "|   bert.encoder.layer.7.attention.output.LayerNorm.bias  |    768     |     78.125    |\n",
      "|      bert.encoder.layer.7.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|       bert.encoder.layer.7.intermediate.dense.bias      |    3072    |     331.5     |\n",
      "|         bert.encoder.layer.7.output.dense.weight        |  2359296   |      inf      |\n",
      "|          bert.encoder.layer.7.output.dense.bias         |    768     |    46.0312    |\n",
      "|       bert.encoder.layer.7.output.LayerNorm.weight      |    768     |     623.5     |\n",
      "|        bert.encoder.layer.7.output.LayerNorm.bias       |    768     |     39.375    |\n",
      "|     bert.encoder.layer.8.attention.self.query.weight    |   589824   |    20720.0    |\n",
      "|      bert.encoder.layer.8.attention.self.query.bias     |    768     |     96.375    |\n",
      "|      bert.encoder.layer.8.attention.self.key.weight     |   589824   |    20768.0    |\n",
      "|       bert.encoder.layer.8.attention.self.key.bias      |    768     |     4.0117    |\n",
      "|     bert.encoder.layer.8.attention.self.value.weight    |   589824   |    16640.0    |\n",
      "|      bert.encoder.layer.8.attention.self.value.bias     |    768     |    21.0156    |\n",
      "|    bert.encoder.layer.8.attention.output.dense.weight   |   589824   |    15688.0    |\n",
      "|     bert.encoder.layer.8.attention.output.dense.bias    |    768     |      28.5     |\n",
      "|  bert.encoder.layer.8.attention.output.LayerNorm.weight |    768     |     646.0     |\n",
      "|   bert.encoder.layer.8.attention.output.LayerNorm.bias  |    768     |      73.0     |\n",
      "|      bert.encoder.layer.8.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|       bert.encoder.layer.8.intermediate.dense.bias      |    3072    |     320.0     |\n",
      "|         bert.encoder.layer.8.output.dense.weight        |  2359296   |      inf      |\n",
      "|          bert.encoder.layer.8.output.dense.bias         |    768     |    47.3438    |\n",
      "|       bert.encoder.layer.8.output.LayerNorm.weight      |    768     |     638.5     |\n",
      "|        bert.encoder.layer.8.output.LayerNorm.bias       |    768     |    41.5312    |\n",
      "|     bert.encoder.layer.9.attention.self.query.weight    |   589824   |    21632.0    |\n",
      "|      bert.encoder.layer.9.attention.self.query.bias     |    768     |    108.3125   |\n",
      "|      bert.encoder.layer.9.attention.self.key.weight     |   589824   |    21568.0    |\n",
      "|       bert.encoder.layer.9.attention.self.key.bias      |    768     |     3.9473    |\n",
      "|     bert.encoder.layer.9.attention.self.value.weight    |   589824   |    16224.0    |\n",
      "|      bert.encoder.layer.9.attention.self.value.bias     |    768     |    20.1094    |\n",
      "|    bert.encoder.layer.9.attention.output.dense.weight   |   589824   |    15376.0    |\n",
      "|     bert.encoder.layer.9.attention.output.dense.bias    |    768     |    35.0625    |\n",
      "|  bert.encoder.layer.9.attention.output.LayerNorm.weight |    768     |     630.0     |\n",
      "|   bert.encoder.layer.9.attention.output.LayerNorm.bias  |    768     |     72.875    |\n",
      "|      bert.encoder.layer.9.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|       bert.encoder.layer.9.intermediate.dense.bias      |    3072    |     335.25    |\n",
      "|         bert.encoder.layer.9.output.dense.weight        |  2359296   |      inf      |\n",
      "|          bert.encoder.layer.9.output.dense.bias         |    768     |    45.3438    |\n",
      "|       bert.encoder.layer.9.output.LayerNorm.weight      |    768     |     615.0     |\n",
      "|        bert.encoder.layer.9.output.LayerNorm.bias       |    768     |    36.4375    |\n",
      "|    bert.encoder.layer.10.attention.self.query.weight    |   589824   |    21584.0    |\n",
      "|     bert.encoder.layer.10.attention.self.query.bias     |    768     |    106.375    |\n",
      "|     bert.encoder.layer.10.attention.self.key.weight     |   589824   |    21504.0    |\n",
      "|      bert.encoder.layer.10.attention.self.key.bias      |    768     |     3.7734    |\n",
      "|    bert.encoder.layer.10.attention.self.value.weight    |   589824   |    16528.0    |\n",
      "|     bert.encoder.layer.10.attention.self.value.bias     |    768     |    13.7578    |\n",
      "|   bert.encoder.layer.10.attention.output.dense.weight   |   589824   |    15600.0    |\n",
      "|    bert.encoder.layer.10.attention.output.dense.bias    |    768     |     32.25     |\n",
      "| bert.encoder.layer.10.attention.output.LayerNorm.weight |    768     |     647.5     |\n",
      "|  bert.encoder.layer.10.attention.output.LayerNorm.bias  |    768     |     59.25     |\n",
      "|     bert.encoder.layer.10.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|      bert.encoder.layer.10.intermediate.dense.bias      |    3072    |     332.0     |\n",
      "|        bert.encoder.layer.10.output.dense.weight        |  2359296   |      inf      |\n",
      "|         bert.encoder.layer.10.output.dense.bias         |    768     |    57.3438    |\n",
      "|      bert.encoder.layer.10.output.LayerNorm.weight      |    768     |     627.5     |\n",
      "|       bert.encoder.layer.10.output.LayerNorm.bias       |    768     |     38.25     |\n",
      "|    bert.encoder.layer.11.attention.self.query.weight    |   589824   |    21280.0    |\n",
      "|     bert.encoder.layer.11.attention.self.query.bias     |    768     |     126.0     |\n",
      "|     bert.encoder.layer.11.attention.self.key.weight     |   589824   |    20928.0    |\n",
      "|      bert.encoder.layer.11.attention.self.key.bias      |    768     |     3.0215    |\n",
      "|    bert.encoder.layer.11.attention.self.value.weight    |   589824   |    18336.0    |\n",
      "|     bert.encoder.layer.11.attention.self.value.bias     |    768     |    11.3984    |\n",
      "|   bert.encoder.layer.11.attention.output.dense.weight   |   589824   |    17296.0    |\n",
      "|    bert.encoder.layer.11.attention.output.dense.bias    |    768     |    22.9688    |\n",
      "| bert.encoder.layer.11.attention.output.LayerNorm.weight |    768     |     655.5     |\n",
      "|  bert.encoder.layer.11.attention.output.LayerNorm.bias  |    768     |    50.4062    |\n",
      "|     bert.encoder.layer.11.intermediate.dense.weight     |  2359296   |      inf      |\n",
      "|      bert.encoder.layer.11.intermediate.dense.bias      |    3072    |     257.0     |\n",
      "|        bert.encoder.layer.11.output.dense.weight        |  2359296   |      inf      |\n",
      "|         bert.encoder.layer.11.output.dense.bias         |    768     |     32.875    |\n",
      "|      bert.encoder.layer.11.output.LayerNorm.weight      |    768     |     486.0     |\n",
      "|       bert.encoder.layer.11.output.LayerNorm.bias       |    768     |    32.0938    |\n",
      "|                 bert.pooler.dense.weight                |   589824   |    11560.0    |\n",
      "|                  bert.pooler.dense.bias                 |    768     |    19.6406    |\n",
      "|                    classifier.weight                    |    4608    |    78.3125    |\n",
      "|                     classifier.bias                     |     6      |     0.0217    |\n",
      "|                  Total Trainable Params                 | 109486854  |      inf      |\n",
      "+---------------------------------------------------------+------------+---------------+\n",
      "------------------------------ half bert-base-uncased metrics ------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 value: 0.733601302532763\n",
      "accuracy value: 0.7348519924684385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"xlnet-base-cased\",\"bert-base-uncased\"]\n",
    "models={}\n",
    "\n",
    "for model_name in model_names:\n",
    "    #Initializing the model\n",
    "    print('=============================================================================')\n",
    "    print(f'============================== {model_name} ==============================')\n",
    "    print('=============================================================================')\n",
    "    this_model=OurAwesomeModel(model_name,dataset)\n",
    "    print(f'------------------------------ regular {model_name} ------------------------------')\n",
    "    # load the best parameters of the model\n",
    "    this_model.load_trained_model()\n",
    "    print(f'------------------------------ {model_name} weights ------------------------------')\n",
    "    this_model.show_parameters()\n",
    "    print(f'------------------------------ {model_name} metrics ------------------------------')\n",
    "    this_model.evaluate(args)\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    #pruning the model\n",
    "    print(f'------------------------------ pruned {model_name} ------------------------------')\n",
    "    pruning_model=copy.deepcopy(this_model)\n",
    "    pruning_model.pruning(0.5)\n",
    "    pruning_model.train({'output_dir':OUT_PATH, 'num_train_epochs':1})\n",
    "    pruning_model.save_model('pruned')\n",
    "    print(f'------------------------------ pruned {model_name} weights ------------------------------')\n",
    "    pruning_model.show_parameters()\n",
    "    print(f'------------------------------ pruned {model_name} metrics ------------------------------')\n",
    "    pruning_model.evaluate(args)\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "    # quantize of the model\n",
    "    print(f'------------------------------ half {model_name} ------------------------------')\n",
    "    half_model=copy.deepcopy(this_model)\n",
    "    half_model.half()\n",
    "    half_model.save_model('half')\n",
    "    print(f'------------------------------ half {model_name} weights ------------------------------')\n",
    "    half_model.show_parameters()\n",
    "    print(f'------------------------------ half {model_name} metrics ------------------------------')\n",
    "    half_model.evaluate(args)\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "#     print(f'------------------------------ distiled {model_name} ------------------------------')\n",
    "#     teacher_model = copy.deepcopy(this_model)\n",
    "#     teacher_model.model.to(device).eval()\n",
    "#     student_model = OurAwesomeModel('distilbert-base-uncased',dataset)    \n",
    "#     train_dataset = teacher_model.dataset['train']\n",
    "#     tokenizer = student_model.tokenizer\n",
    "#     # Tokenize and encode the text data\n",
    "#     train_dataset = train_dataset.map(lambda example: tokenizer(example['text'], truncation=True, padding='max_length'), batched=True)\n",
    "#     # Convert the labels to integers\n",
    "#     train_dataset = train_dataset.map(lambda example: {'labels': example['labels']}, batched=True)\n",
    "#     student_model.model = knowladge_distilation_training(student_model.model.to(device), teacher_model.model.to(device), train_dataset)\n",
    "#     student_model.save_model('student')\n",
    "#     print(f'------------------------------ student {model_name} weights ------------------------------')\n",
    "#     student_model.show_parameters()\n",
    "#     print(f'------------------------------ student {model_name} metrics ------------------------------')\n",
    "#     student_model.evaluate(args)\n",
    "    \n",
    "    \n",
    "        \n",
    "    models[model_name]= {'trained model': this_model, 'pruned_model':pruning_model,'half_model':half_model, 'student_model':student_model} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a703662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
